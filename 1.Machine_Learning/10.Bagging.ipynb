{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7e3d71",
   "metadata": {},
   "source": [
    "# Bootstrap Aggregating (Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c1782",
   "metadata": {},
   "source": [
    "## Recall Bias Variance Decomposition\n",
    "\n",
    "\n",
    "$$ \\begin{equation*}\n",
    "    \\underbrace{\\mathbb{E}[(h_D(x) - y)^2]}_\\mathrm{Error} = \\underbrace{\\mathbb{E}[(h_D(x)-\\bar{h}(x))^2]}_\\mathrm{Variance} + \\underbrace{\\mathbb{E}[(\\bar{h}(x)-\\bar{y}(x))^2]}_\\mathrm{Bias} + \\underbrace{\\mathbb{E}[(\\bar{y}(x)-y(x))^2]}_\\mathrm{Noise}\n",
    "\\end{equation*}$$\n",
    "\n",
    "\n",
    "We know that we can't do anything about the generalization error inccurred by the noise. However, we can reduce the variance via the bagging technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4e63e",
   "metadata": {},
   "source": [
    "# Central Limit Theory\n",
    "\n",
    "$$ \\text{Weak Law of Large Number} $$\n",
    "\n",
    "\n",
    "For independent and identically distributed random variables, CLT roughly state that the emperical mean approximates the mean. \n",
    "\n",
    "We can applying this law to reduce the generalization performance. More specificaly,\n",
    "\n",
    "\n",
    "$$ \\hat{h} = \\frac{1}{m}\\sum_{i = 1}^m h_{D_i} \\to \\bar{h} \\qquad as\\  m \\to \\infty $$\n",
    "\n",
    "However, we need to find $m$ datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17bbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pDL",
   "language": "python",
   "name": "pdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
