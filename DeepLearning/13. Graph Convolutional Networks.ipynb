{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph convolutional Network\n",
    "\n",
    "This tutorial based on [Theoretical Foundations of Graph Neural Networks](https://www.youtube.com/watch?v=uF53xsT7mjc) and [Graph Neural Networks](https://www.youtube.com/playlist?list=PLSgGvve8UweGx4_6hhrF3n4wpHf_RV76_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  1. Permutation Invariance Property\n",
    "\n",
    "We require that our algorithms to satisfiy the permutation invariance property. This requirement stems from the fact that graph structured data does not involve ordering. Hence, the order of nodes should not matter for us. \n",
    "\n",
    "\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times d}$ represent node features and $P$ be a permutation. A function $f$ permutation invariant if\n",
    "\n",
    "## $$ f(\\textbf{P} \\textbf{X}) = f(\\textbf{X})$$\n",
    "\n",
    "Let's take a look at some permutation invariant functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets by Zaheer et al. NeurIPS 2017\n",
    "\n",
    "A generic form:\n",
    "## $$ f(\\textbf{X}) = f(\\textbf{P}\\textbf{X})$$\n",
    "## $$ f(\\textbf{X}) = \\phi \\Bigg(\\Sigma_i \\psi(x_i) \\Bigg),$$\n",
    "\n",
    "where $\\phi$ and $ \\psi $ are learnable functions. $\\phi$ is a __permutation invariant aggregation function__ (e.g. summation, averagining, maximization).\n",
    "\n",
    "##  2. Permutation Equivariance Property\n",
    "\n",
    "\n",
    "## $$ f(\\textbf{P}\\textbf{X}) = \\textbf{P}f(\\textbf{X})$$\n",
    "\n",
    "## $$ f(\\textbf{X}) = \\phi \\Bigg( \\Phi_i \\psi(x_i) \\Bigg),$$\n",
    "\n",
    "where $\\Phi$ is a permutation-invariant aggregator (such as sum, avg or max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[ 0.86749028 -0.47076494]\n",
      " [ 0.72849109  0.49842976]\n",
      " [ 1.30273722 -0.50826614]\n",
      " [ 2.29124875 -0.29035236]]\n",
      "\n",
      "PX:\n",
      "[[ 1.30273722 -0.50826614]\n",
      " [ 0.86749028 -0.47076494]\n",
      " [ 2.29124875 -0.29035236]\n",
      " [ 0.72849109  0.49842976]]\n",
      "\t\t\t\t Only order is changed.\n",
      "Σ_i Σ_j X_ij=> True, 4.419\n",
      "Σ_i X_ij=> True, [ 5.18996734 -0.77095369]\n",
      "Σ_j X_ij=> True, [0.39672534 1.22692085 0.79447107 2.00089639]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Number of nodes and size of features\n",
    "n,d=4,2\n",
    "\n",
    "# (1) Node Representation\n",
    "X=np.random.randn(n,d)\n",
    "\n",
    "# (2) Permutation Matrix\n",
    "P=np.identity(n)[np.random.permutation(n)]\n",
    "\n",
    "print(f'X:\\n{X}\\n')\n",
    "\n",
    "print(f'PX:\\n{P@X}')\n",
    "print('\\t\\t\\t\\t Only order is changed.')\n",
    "\n",
    "\n",
    "# Sum\n",
    "sumX=np.einsum(\"ij->\",X)\n",
    "sumPX=np.einsum(\"ij->\",P@X)\n",
    "print(f'Σ_i Σ_j X_ij=> {np.allclose(sumX,sumPX,atol=1e4)}, {sumX:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "# Column Sum\n",
    "colsumX=np.einsum(\"ij-> j \",X)\n",
    "colsumPX=np.einsum(\"ij-> j \",P@X)\n",
    "print(f'Σ_i X_ij=> {np.allclose(colsumX,colsumPX,atol=1e4)}, {colsumX}')\n",
    "\n",
    "\n",
    "# Row Sum\n",
    "rowsumX=np.einsum(\"ij-> i \",X)\n",
    "rowsumPX=np.einsum(\"ij-> i \",P@X)\n",
    "print(f'Σ_j X_ij=> {np.allclose(rowsumX,rowsumPX,atol=1e4)}, {rowsumX}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning on Graphs\n",
    "\n",
    "We can represent edges with a __A__ be a binary adjacency matrix.\n",
    "\n",
    "## Invariance: $$ f(\\textbf{P}\\textbf{X},\\textbf{P} \\textbf{A} \\textbf{P}^T) = f(\\textbf{X},\\textbf{A})$$\n",
    "\n",
    "\n",
    "## Equivariance: $$ f(\\textbf{P}\\textbf{X},\\textbf{P} \\textbf{A} \\textbf{P}^T) = \\textbf{P}f(\\textbf{X},\\textbf{A})$$\n",
    "\n",
    "\n",
    "\n",
    "## Neighbourhood:\n",
    "$$ \\mathcal{N}_i = \\{ j, (i,j) \\in \\mathcal{E} \\vee (j,i) \\in \\mathcal{E} \\} $$\n",
    "\n",
    "\n",
    "$$ \n",
    "f(\\textbf{X},\\textbf{A})=\\begin{bmatrix} \n",
    "- \\quad g( \\textbf{x}_1, \\textbf{X}_{\\mathcal{N}_1} ) \\quad-\\\\\n",
    "- \\quad g( \\textbf{x}_2, \\textbf{X}_{\\mathcal{N}_2} ) \\quad-\\\\\n",
    "\\cdots \\\\\n",
    "- \\quad g( \\textbf{x}_n, \\textbf{X}_{\\mathcal{N}_n} ) \\quad-\\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "g should be also permutation invariant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Neural Network\n",
    "\n",
    "\n",
    "### $$ h_i = \\phi \\, \\Big( x_i , \\Phi_{j \\in \\mathcal{N}_i} c_{ij} \\, \\psi(x_j) \\Big)$$\n",
    "where features of neighbours aggregated with fixed wieght $c_{ij}$. Useful for homophilous graphs and scaling up: when edges encode label similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Graph Neural Network\n",
    "\n",
    "### $$ h_i = \\phi \\, \\big( x_i , \\Phi_{j \\in \\mathcal{N}_i} a(x_i,x_j) \\, \\psi(x_j) \\big)$$\n",
    "where features of neighbours aggregated with implicit weights (via attention)$. Usefull: Edges need not encode homophily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesage-passing Graph Neural Network\n",
    "\n",
    "### $$ h_i = \\phi \\, \\big( x_i , \\Phi_{j \\in \\mathcal{N}_i} \\psi(x_i,x_j) \\big)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Why does matrix product of A and X imply ?\n",
    "\n",
    "Matrix product of an adjecency matrix $A$ with node representations $X$ \"updates\" each node representations $X_i$ with its neighbours $A_i$. \n",
    "\n",
    "Let's understand this computation in detail. Assume that $A$ is a idendity matrix, hence, the impact of neighbors of $i$th node is __zero__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "X:\n",
      "[[-0.69339122 -0.59290391]\n",
      " [ 1.16204193  0.99112958]\n",
      " [ 0.62776671 -0.20528424]\n",
      " [-1.89022329  0.11236841]]\n",
      "\n",
      "AX:\n",
      "[[-0.69339122 -0.59290391]\n",
      " [ 1.16204193  0.99112958]\n",
      " [ 0.62776671 -0.20528424]\n",
      " [-1.89022329  0.11236841]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n,d=4,2\n",
    "A=np.eye(n)\n",
    "X=np.random.randn(n,d)\n",
    "AX=np.einsum('ik,kj->ij', A, X)\n",
    "print(f'A:\\n{A}\\n')\n",
    "print(f'X:\\n{X}\\n')\n",
    "print(f'AX:\\n{AX}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Why don't we extend the neighbourhood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's connect \n",
    "A[0][1]=1 # n_0 => n_1\n",
    "A[1][0]=1 # n_1 => n_0\n",
    "\n",
    "A[2][1]=1 # n_2 => n_1\n",
    "A[1][2]=1 # n_1 => n_2\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 0 and Node 2 are not connected\n",
    "assert A[0][2]==A[2][0]==0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[1.  1.  0.5 0. ]\n",
      " [1.  1.  1.  0. ]\n",
      " [0.5 1.  1.  0. ]\n",
      " [0.  0.  0.  1. ]]\n",
      "\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Neigbours of neighbours\n",
    "neighbours_of_neighbours_scalar=.5\n",
    "A+= ((A+A@A ==1) *1.0) * neighbours_of_neighbours_scalar \n",
    "print(f'A:\\n{A}\\n')\n",
    "print(A[0][2])\n",
    "print(A[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[1.  1.  0.5 0. ]\n",
      " [1.  1.  1.  0. ]\n",
      " [0.5 1.  1.  0. ]\n",
      " [0.  0.  0.  1. ]]\n",
      "\n",
      "AX:\n",
      "[[ 0.78253406  0.29558355]\n",
      " [ 1.09641742  0.19294143]\n",
      " [ 1.44311303  0.48939339]\n",
      " [-1.89022329  0.11236841]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AX=np.einsum('ik,kj->ij', A, X)\n",
    "print(f'A:\\n{A}\\n')\n",
    "print(f'AX:\\n{AX}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset\n",
      "Pre-processing node features\n"
     ]
    }
   ],
   "source": [
    "A, X, labels, train_mask, val_mask, test_mask = spektral.datasets.citation.load_data(dataset_name='cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2708, 1433)\n",
      "A: (2708, 2708)\n",
      "Y: (2708, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = X.todense()\n",
    "\n",
    "A = A.todense()\n",
    "A = A + np.eye(A.shape[0])\n",
    "X = X.astype('float32')\n",
    "A = A.astype('float32')\n",
    "\n",
    "print('X:',X.shape)\n",
    "print('A:',A.shape)\n",
    "# Labels\n",
    "print('Y:',labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]]\n",
      "[0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(A[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(2708,)\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "print(train_mask[0])\n",
    "print(train_mask.shape)\n",
    "print(np.sum(train_mask)) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(2708,)\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(val_mask[0])\n",
    "print(val_mask.shape)\n",
    "print(np.sum(val_mask)) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(2708,)\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(test_mask[0])\n",
    "print(test_mask.shape)\n",
    "print(np.sum(test_mask)) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(logits, labels, masks):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "    masks = tf.cast(masks, dtype=tf.float32)\n",
    "    masks /= tf.reduce_mean(masks)\n",
    "    loss *= masks\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(logits, labels, masks):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    masks = tf.cast(masks, dtype=tf.float32)\n",
    "    masks /= tf.reduce_mean(masks)\n",
    "    accuracy_all *= masks\n",
    "    return tf.reduce_mean(accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn(fts, adj, transform, activation):\n",
    "    seq_fts = transform(fts)\n",
    "    ret_fts = tf.matmul(adj, seq_fts)\n",
    "    return activation(ret_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cora(fts, adj, gnn_fn, units=32, epochs=200, lr=.01):\n",
    "    lyr_1 = tf.keras.layers.Dense(units)\n",
    "    lyr_2 = tf.keras.layers.Dense(7)  # for 7 classes\n",
    "\n",
    "    def cora_gnn(fts, adj):\n",
    "        hidden = gnn_fn(fts, adj, lyr_1, tf.nn.relu)\n",
    "        logits = gnn_fn(hidden, adj, lyr_2, tf.identity)\n",
    "        return logits\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for ep in range(epochs + 1):\n",
    "        with tf.GradientTape() as t:\n",
    "            logits = cora_gnn(fts, adj)\n",
    "            loss = masked_softmax_cross_entropy(logits, labels, train_mask)\n",
    "        variables = t.watched_variables()\n",
    "        grad = t.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grad, variables))\n",
    "\n",
    "        logits = cora_gnn(fts, adj)\n",
    "        val_accuracy = masked_accuracy(logits, labels, val_mask)\n",
    "        test_accuracy = masked_accuracy(logits, labels, test_mask)\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            print(f'Epoch {ep} | Traninng Loss: {loss.numpy():.3f} | Val acc: {val_accuracy.numpy():.3f} Test Acc: {test_accuracy.numpy():.3f}| ')\n",
    "    print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum Pooling of neighbours\n",
      "Epoch 0 | Traninng Loss: 2.020 | Val acc: 0.326 Test Acc: 0.310| \n",
      "Epoch 2 | Traninng Loss: 1.723 | Val acc: 0.540 Test Acc: 0.549| \n",
      "Epoch 3 | Traninng Loss: 1.391 | Val acc: 0.568 Test Acc: 0.603| \n",
      "Epoch 4 | Traninng Loss: 1.294 | Val acc: 0.610 Test Acc: 0.638| \n",
      "Epoch 5 | Traninng Loss: 1.146 | Val acc: 0.654 Test Acc: 0.662| \n",
      "Epoch 6 | Traninng Loss: 1.030 | Val acc: 0.708 Test Acc: 0.739| \n",
      "Epoch 7 | Traninng Loss: 0.927 | Val acc: 0.726 Test Acc: 0.751| \n",
      "Epoch 8 | Traninng Loss: 0.851 | Val acc: 0.728 Test Acc: 0.753| \n",
      "Epoch 9 | Traninng Loss: 0.784 | Val acc: 0.732 Test Acc: 0.746| \n",
      "Epoch 10 | Traninng Loss: 0.716 | Val acc: 0.736 Test Acc: 0.750| \n",
      "Epoch 11 | Traninng Loss: 0.653 | Val acc: 0.740 Test Acc: 0.744| \n",
      "Epoch 18 | Traninng Loss: 0.345 | Val acc: 0.744 Test Acc: 0.760| \n",
      "Epoch 24 | Traninng Loss: 0.207 | Val acc: 0.746 Test Acc: 0.764| \n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "print('Sum Pooling of neighbours')\n",
    "train_cora(X, A, gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighbours of neighbours\n",
    "\n",
    "neighbours_of_neighbours_scalar=.1\n",
    "A_neg= A+((A+A@A ==1) *1.0) * neighbours_of_neighbours_scalar \n",
    "A_neg = A_neg.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum Pooling of neighbours\n",
      "Epoch 0 | Traninng Loss: 2.047 | Val acc: 0.174 Test Acc: 0.164| \n",
      "Epoch 2 | Traninng Loss: 3.564 | Val acc: 0.232 Test Acc: 0.259| \n",
      "Epoch 3 | Traninng Loss: 2.360 | Val acc: 0.476 Test Acc: 0.469| \n",
      "Epoch 4 | Traninng Loss: 1.518 | Val acc: 0.534 Test Acc: 0.520| \n",
      "Epoch 5 | Traninng Loss: 1.552 | Val acc: 0.552 Test Acc: 0.503| \n",
      "Epoch 6 | Traninng Loss: 1.629 | Val acc: 0.618 Test Acc: 0.587| \n",
      "Epoch 7 | Traninng Loss: 1.428 | Val acc: 0.666 Test Acc: 0.632| \n",
      "Epoch 8 | Traninng Loss: 1.224 | Val acc: 0.708 Test Acc: 0.694| \n",
      "Epoch 9 | Traninng Loss: 1.088 | Val acc: 0.728 Test Acc: 0.740| \n",
      "Epoch 27 | Traninng Loss: 0.441 | Val acc: 0.734 Test Acc: 0.754| \n",
      "Epoch 28 | Traninng Loss: 0.421 | Val acc: 0.750 Test Acc: 0.767| \n",
      "Epoch 46 | Traninng Loss: 0.210 | Val acc: 0.752 Test Acc: 0.749| \n",
      "Epoch 48 | Traninng Loss: 0.194 | Val acc: 0.754 Test Acc: 0.752| \n",
      "Epoch 50 | Traninng Loss: 0.181 | Val acc: 0.758 Test Acc: 0.756| \n",
      "Epoch 60 | Traninng Loss: 0.126 | Val acc: 0.760 Test Acc: 0.757| \n",
      "Epoch 61 | Traninng Loss: 0.121 | Val acc: 0.762 Test Acc: 0.758| \n",
      "Epoch 63 | Traninng Loss: 0.113 | Val acc: 0.764 Test Acc: 0.761| \n",
      "Epoch 64 | Traninng Loss: 0.110 | Val acc: 0.766 Test Acc: 0.760| \n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "print('Sum Pooling of neighbours')\n",
    "train_cora(X, A_neg, gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only itself\n",
      "Epoch 0 | Traninng Loss: 1.946 | Val acc: 0.236 Test Acc: 0.294| \n",
      "Epoch 1 | Traninng Loss: 1.933 | Val acc: 0.360 Test Acc: 0.379| \n",
      "Epoch 2 | Traninng Loss: 1.914 | Val acc: 0.402 Test Acc: 0.431| \n",
      "Epoch 8 | Traninng Loss: 1.716 | Val acc: 0.418 Test Acc: 0.438| \n",
      "Epoch 9 | Traninng Loss: 1.672 | Val acc: 0.428 Test Acc: 0.452| \n",
      "Epoch 10 | Traninng Loss: 1.626 | Val acc: 0.442 Test Acc: 0.468| \n",
      "Epoch 11 | Traninng Loss: 1.577 | Val acc: 0.444 Test Acc: 0.480| \n",
      "Epoch 12 | Traninng Loss: 1.525 | Val acc: 0.446 Test Acc: 0.486| \n",
      "Epoch 16 | Traninng Loss: 1.297 | Val acc: 0.448 Test Acc: 0.488| \n",
      "Epoch 17 | Traninng Loss: 1.236 | Val acc: 0.450 Test Acc: 0.488| \n",
      "Epoch 19 | Traninng Loss: 1.111 | Val acc: 0.456 Test Acc: 0.495| \n",
      "Epoch 20 | Traninng Loss: 1.048 | Val acc: 0.458 Test Acc: 0.497| \n",
      "Epoch 21 | Traninng Loss: 0.984 | Val acc: 0.464 Test Acc: 0.498| \n",
      "Epoch 22 | Traninng Loss: 0.921 | Val acc: 0.470 Test Acc: 0.501| \n",
      "Epoch 23 | Traninng Loss: 0.859 | Val acc: 0.482 Test Acc: 0.503| \n",
      "Epoch 24 | Traninng Loss: 0.798 | Val acc: 0.488 Test Acc: 0.503| \n",
      "Epoch 27 | Traninng Loss: 0.624 | Val acc: 0.492 Test Acc: 0.509| \n",
      "Epoch 28 | Traninng Loss: 0.570 | Val acc: 0.494 Test Acc: 0.510| \n",
      "Epoch 29 | Traninng Loss: 0.520 | Val acc: 0.498 Test Acc: 0.514| \n",
      "Epoch 30 | Traninng Loss: 0.472 | Val acc: 0.500 Test Acc: 0.519| \n",
      "Epoch 31 | Traninng Loss: 0.427 | Val acc: 0.502 Test Acc: 0.521| \n",
      "Epoch 34 | Traninng Loss: 0.311 | Val acc: 0.504 Test Acc: 0.516| \n",
      "Epoch 37 | Traninng Loss: 0.223 | Val acc: 0.506 Test Acc: 0.513| \n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# (2) Rely on yourself; Do not make use of your first neighbours: Node feature + Identity  matrix + GNN\n",
    "print('Only itself')\n",
    "train_cora(X, tf.eye(A.shape[0]), gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Pooling\n",
      "Epoch 0 | Traninng Loss: 1.946 | Val acc: 0.124 Test Acc: 0.142| \n",
      "Epoch 2 | Traninng Loss: 1.918 | Val acc: 0.132 Test Acc: 0.151| \n",
      "Epoch 3 | Traninng Loss: 1.900 | Val acc: 0.134 Test Acc: 0.160| \n",
      "Epoch 4 | Traninng Loss: 1.880 | Val acc: 0.136 Test Acc: 0.161| \n",
      "Epoch 5 | Traninng Loss: 1.858 | Val acc: 0.144 Test Acc: 0.171| \n",
      "Epoch 6 | Traninng Loss: 1.835 | Val acc: 0.166 Test Acc: 0.188| \n",
      "Epoch 7 | Traninng Loss: 1.811 | Val acc: 0.208 Test Acc: 0.218| \n",
      "Epoch 8 | Traninng Loss: 1.784 | Val acc: 0.240 Test Acc: 0.248| \n",
      "Epoch 9 | Traninng Loss: 1.757 | Val acc: 0.268 Test Acc: 0.284| \n",
      "Epoch 10 | Traninng Loss: 1.727 | Val acc: 0.292 Test Acc: 0.326| \n",
      "Epoch 11 | Traninng Loss: 1.696 | Val acc: 0.324 Test Acc: 0.365| \n",
      "Epoch 12 | Traninng Loss: 1.663 | Val acc: 0.376 Test Acc: 0.392| \n",
      "Epoch 13 | Traninng Loss: 1.629 | Val acc: 0.426 Test Acc: 0.428| \n",
      "Epoch 14 | Traninng Loss: 1.592 | Val acc: 0.488 Test Acc: 0.474| \n",
      "Epoch 15 | Traninng Loss: 1.554 | Val acc: 0.528 Test Acc: 0.515| \n",
      "Epoch 16 | Traninng Loss: 1.515 | Val acc: 0.570 Test Acc: 0.561| \n",
      "Epoch 17 | Traninng Loss: 1.473 | Val acc: 0.618 Test Acc: 0.606| \n",
      "Epoch 18 | Traninng Loss: 1.431 | Val acc: 0.646 Test Acc: 0.638| \n",
      "Epoch 19 | Traninng Loss: 1.387 | Val acc: 0.664 Test Acc: 0.661| \n",
      "Epoch 20 | Traninng Loss: 1.342 | Val acc: 0.692 Test Acc: 0.684| \n",
      "Epoch 21 | Traninng Loss: 1.296 | Val acc: 0.708 Test Acc: 0.707| \n",
      "Epoch 22 | Traninng Loss: 1.249 | Val acc: 0.722 Test Acc: 0.722| \n",
      "Epoch 23 | Traninng Loss: 1.202 | Val acc: 0.742 Test Acc: 0.733| \n",
      "Epoch 24 | Traninng Loss: 1.154 | Val acc: 0.748 Test Acc: 0.752| \n",
      "Epoch 25 | Traninng Loss: 1.106 | Val acc: 0.766 Test Acc: 0.768| \n",
      "Epoch 26 | Traninng Loss: 1.059 | Val acc: 0.772 Test Acc: 0.776| \n",
      "Epoch 27 | Traninng Loss: 1.012 | Val acc: 0.780 Test Acc: 0.784| \n",
      "Epoch 28 | Traninng Loss: 0.965 | Val acc: 0.784 Test Acc: 0.786| \n",
      "Epoch 29 | Traninng Loss: 0.919 | Val acc: 0.786 Test Acc: 0.789| \n",
      "Epoch 30 | Traninng Loss: 0.874 | Val acc: 0.790 Test Acc: 0.792| \n",
      "Epoch 36 | Traninng Loss: 0.630 | Val acc: 0.792 Test Acc: 0.799| \n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# (3) Make use of average of  your first neighbours (1) Node feature + Degree normalized ADJ matrix + GNN\n",
    "print('Average Pooling')\n",
    "degree_matrix = tf.reduce_sum(A, axis=-1)\n",
    "train_cora(X, A / degree_matrix, gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Traninng Loss: 1.946 | Val acc: 0.388 Test Acc: 0.389| \n",
      "Epoch 6 | Traninng Loss: 1.868 | Val acc: 0.404 Test Acc: 0.429| \n",
      "Epoch 7 | Traninng Loss: 1.852 | Val acc: 0.472 Test Acc: 0.475| \n",
      "Epoch 8 | Traninng Loss: 1.834 | Val acc: 0.530 Test Acc: 0.525| \n",
      "Epoch 9 | Traninng Loss: 1.814 | Val acc: 0.564 Test Acc: 0.563| \n",
      "Epoch 10 | Traninng Loss: 1.793 | Val acc: 0.596 Test Acc: 0.604| \n",
      "Epoch 11 | Traninng Loss: 1.771 | Val acc: 0.622 Test Acc: 0.630| \n",
      "Epoch 12 | Traninng Loss: 1.749 | Val acc: 0.634 Test Acc: 0.653| \n",
      "Epoch 13 | Traninng Loss: 1.725 | Val acc: 0.656 Test Acc: 0.673| \n",
      "Epoch 14 | Traninng Loss: 1.699 | Val acc: 0.688 Test Acc: 0.695| \n",
      "Epoch 15 | Traninng Loss: 1.671 | Val acc: 0.702 Test Acc: 0.712| \n",
      "Epoch 16 | Traninng Loss: 1.643 | Val acc: 0.718 Test Acc: 0.726| \n",
      "Epoch 17 | Traninng Loss: 1.613 | Val acc: 0.728 Test Acc: 0.741| \n",
      "Epoch 18 | Traninng Loss: 1.582 | Val acc: 0.740 Test Acc: 0.751| \n",
      "Epoch 19 | Traninng Loss: 1.551 | Val acc: 0.746 Test Acc: 0.748| \n",
      "Epoch 20 | Traninng Loss: 1.518 | Val acc: 0.750 Test Acc: 0.754| \n",
      "Epoch 21 | Traninng Loss: 1.485 | Val acc: 0.760 Test Acc: 0.755| \n",
      "Epoch 27 | Traninng Loss: 1.273 | Val acc: 0.762 Test Acc: 0.770| \n",
      "Epoch 28 | Traninng Loss: 1.236 | Val acc: 0.766 Test Acc: 0.774| \n",
      "Epoch 30 | Traninng Loss: 1.164 | Val acc: 0.768 Test Acc: 0.775| \n",
      "Epoch 36 | Traninng Loss: 0.957 | Val acc: 0.772 Test Acc: 0.776| \n",
      "Epoch 39 | Traninng Loss: 0.863 | Val acc: 0.774 Test Acc: 0.770| \n",
      "Epoch 42 | Traninng Loss: 0.777 | Val acc: 0.778 Test Acc: 0.773| \n",
      "Epoch 43 | Traninng Loss: 0.751 | Val acc: 0.780 Test Acc: 0.775| \n",
      "Epoch 45 | Traninng Loss: 0.701 | Val acc: 0.784 Test Acc: 0.773| \n",
      "Epoch 54 | Traninng Loss: 0.517 | Val acc: 0.786 Test Acc: 0.780| \n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# neighbours of neighbours\n",
    "\n",
    "neighbours_of_neighbours_scalar=1.0\n",
    "A_neg= A+((A+A@A ==1) *1.0) * neighbours_of_neighbours_scalar \n",
    "A_neg = A_neg.astype('float32')\n",
    "\n",
    "degree_matrix_A_neg = tf.reduce_sum(A_neg, axis=-1)\n",
    "train_cora(X, A_neg / degree_matrix_A_neg, gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Pooling\n",
      "Epoch 0 | Traninng Loss: 1.946 | Val acc: 0.170 Test Acc: 0.201| \n",
      "Epoch 1 | Traninng Loss: 1.938 | Val acc: 0.208 Test Acc: 0.223| \n",
      "Epoch 2 | Traninng Loss: 1.928 | Val acc: 0.212 Test Acc: 0.232| \n",
      "Epoch 3 | Traninng Loss: 1.914 | Val acc: 0.220 Test Acc: 0.260| \n",
      "Epoch 4 | Traninng Loss: 1.899 | Val acc: 0.276 Test Acc: 0.305| \n",
      "Epoch 5 | Traninng Loss: 1.882 | Val acc: 0.368 Test Acc: 0.388| \n",
      "Epoch 6 | Traninng Loss: 1.864 | Val acc: 0.450 Test Acc: 0.464| \n",
      "Epoch 7 | Traninng Loss: 1.845 | Val acc: 0.500 Test Acc: 0.511| \n",
      "Epoch 8 | Traninng Loss: 1.825 | Val acc: 0.522 Test Acc: 0.546| \n",
      "Epoch 9 | Traninng Loss: 1.803 | Val acc: 0.558 Test Acc: 0.561| \n",
      "Epoch 10 | Traninng Loss: 1.779 | Val acc: 0.572 Test Acc: 0.589| \n",
      "Epoch 11 | Traninng Loss: 1.752 | Val acc: 0.586 Test Acc: 0.616| \n",
      "Epoch 12 | Traninng Loss: 1.724 | Val acc: 0.602 Test Acc: 0.636| \n",
      "Epoch 13 | Traninng Loss: 1.695 | Val acc: 0.610 Test Acc: 0.643| \n",
      "Epoch 14 | Traninng Loss: 1.664 | Val acc: 0.612 Test Acc: 0.655| \n",
      "Epoch 15 | Traninng Loss: 1.632 | Val acc: 0.620 Test Acc: 0.667| \n",
      "Epoch 16 | Traninng Loss: 1.598 | Val acc: 0.640 Test Acc: 0.686| \n",
      "Epoch 17 | Traninng Loss: 1.563 | Val acc: 0.650 Test Acc: 0.701| \n",
      "Epoch 18 | Traninng Loss: 1.525 | Val acc: 0.666 Test Acc: 0.711| \n",
      "Epoch 19 | Traninng Loss: 1.487 | Val acc: 0.680 Test Acc: 0.725| \n",
      "Epoch 20 | Traninng Loss: 1.446 | Val acc: 0.696 Test Acc: 0.737| \n",
      "Epoch 21 | Traninng Loss: 1.405 | Val acc: 0.708 Test Acc: 0.744| \n",
      "Epoch 22 | Traninng Loss: 1.363 | Val acc: 0.714 Test Acc: 0.759| \n",
      "Epoch 23 | Traninng Loss: 1.320 | Val acc: 0.724 Test Acc: 0.770| \n",
      "Epoch 24 | Traninng Loss: 1.276 | Val acc: 0.740 Test Acc: 0.775| \n",
      "Epoch 25 | Traninng Loss: 1.232 | Val acc: 0.746 Test Acc: 0.787| \n",
      "Epoch 26 | Traninng Loss: 1.186 | Val acc: 0.754 Test Acc: 0.797| \n",
      "Epoch 27 | Traninng Loss: 1.139 | Val acc: 0.762 Test Acc: 0.802| \n",
      "Epoch 28 | Traninng Loss: 1.093 | Val acc: 0.764 Test Acc: 0.805| \n",
      "Epoch 29 | Traninng Loss: 1.046 | Val acc: 0.768 Test Acc: 0.803| \n",
      "Epoch 30 | Traninng Loss: 1.000 | Val acc: 0.772 Test Acc: 0.802| \n",
      "Epoch 31 | Traninng Loss: 0.954 | Val acc: 0.774 Test Acc: 0.800| \n",
      "Epoch 33 | Traninng Loss: 0.863 | Val acc: 0.780 Test Acc: 0.805| \n",
      "Epoch 35 | Traninng Loss: 0.776 | Val acc: 0.784 Test Acc: 0.801| \n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (4) It is a good idea to normalize adjesonsy matrix\n",
    "norm_degr = tf.linalg.diag(1.0 / tf.sqrt(degree_matrix))\n",
    "norm_A = tf.linalg.matmul(norm_degr, tf.matmul(A, norm_degr))\n",
    "print('Normalized Pooling')\n",
    "train_cora(X, norm_A, gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Graph Neural Network in Detail\n",
    "\n",
    "\n",
    "Let \n",
    "+ $X \\in \\mathbb{R}^{N \\times d}$ be a matrix of node feature vectors.\n",
    "+ $A \\in \\mathbb{R}^{N \\times N}$ be an adjacency matrix\n",
    "+ $D$ be a degree matrix ,i.e., $D_{ii}= \\sum_j A_{ij}$\n",
    "\n",
    "$$ H^{l+1} = \\sigma \\,( \\tilde{D}^{-\\frac{1}{2}} \\, \\tilde{A} \\, \\tilde{D}^{-\\frac{1}{2}} \\, H^l \\, W^l)$$\n",
    "\n",
    "\n",
    "+ $\\tilde{A} = A + I$ is the adjacency matrix of the undirected graph G with added self-connections,i.e., $I$ is the indendity matrix.\n",
    "\n",
    "\n",
    "+ $\\tilde{D}_{ii}=\\sum_j \\tilde{A}_{ij}$\n",
    "\n",
    "\n",
    "+ $H^l \\in \\mathbb{R}^{N \\times D}$ is the matrix in the l.th layer. Hence $H^0$= X is a matrix of node feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \n",
    "A=\\begin{bmatrix} \n",
    "0 \\quad 1 \\quad 0 \\quad 0 \\quad 0 \\\\\n",
    "1 \\quad 0 \\quad 1 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 1 \\quad 0 \\quad 1 \\quad 1 \\\\\n",
    "0 \\quad 0 \\quad 1 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad 1 \\quad 0 \\quad 0 \\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "$$\n",
    "D=\\begin{bmatrix} \n",
    "1 \\quad 0 \\quad 0 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 2 \\quad 0 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad 3 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad 0 \\quad 1 \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad 0 \\quad 0 \\quad 1 \\\\\n",
    "\\end{bmatrix} \n",
    "$$ and $D^{-1}$ denotes the inverse of $D$.\n",
    "$$\n",
    "D^{-1}=\\begin{bmatrix} \n",
    "1.0 \\quad 0 \\quad 0 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad .5 \\quad 0 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad .3 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad 0 \\quad 1. \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad 0 \\quad 0 \\quad 1. \\\\\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "$$ D^{-1} \\cdot A \\cdot x=y,$$\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} \n",
    "1.0 \\quad 0 \\quad 0 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad .5 \\quad 0 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad .3 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad 0 \\quad 1. \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad 0 \\quad 0 \\quad 1. \\\\\n",
    "\\end{bmatrix} \n",
    "\\cdot\n",
    "\\begin{bmatrix} \n",
    "0 \\quad 1 \\quad 0 \\quad 0 \\quad 0 \\\\\n",
    "1 \\quad 0 \\quad 1 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 1 \\quad 0 \\quad 1 \\quad 1 \\\\\n",
    "0 \\quad 0 \\quad 1 \\quad 0 \\quad 0 \\\\\n",
    "0 \\quad 0 \\quad 1 \\quad 0 \\quad 0 \\\\\n",
    "\\end{bmatrix} \n",
    "\\cdot\n",
    "\\begin{bmatrix} 1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "2. \\\\\n",
    "2. \\\\\n",
    "3.3 \\\\\n",
    "3.0 \\\\\n",
    "3.0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "+ The vector representation of the $i-$th node is represented with $x_i$.\n",
    "+ $A_{i:} \\cdot x=y_i$ update the vector representation of the $i-$th node.\n",
    "+ This means that $y_i$ is a sum of the features of the neighbouring nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm \n",
    "\n",
    "import networkx as nx\n",
    "from scipy.linalg import sqrtm \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "from IPython.display import HTML\n",
    "\n",
    "# (1) A is given\n",
    "A = np.array(\n",
    "    [[0, 1, 0, 0, 0], \n",
    "     [1, 0, 1, 0, 0], \n",
    "     [0, 1, 0, 1, 1], \n",
    "     [0, 0, 1, 0, 0], \n",
    "     [0, 0, 1, 0, 0]])\n",
    "\n",
    "# (2) Compute A tilde.\n",
    "A_tilda = A + np.identity(len(A))\n",
    "\n",
    "# (3) Compute D and D tilde.\n",
    "D = np.zeros(A.shape)\n",
    "np.fill_diagonal(D, A.sum(axis=0))\n",
    "D_tilda = D + np.identity(len(D))\n",
    "D_tilda_inv_sqrt=sqrtm(np.linalg.inv(D_tilda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\tilde A = A + I$$\n",
    "$$ \\tilde D = D + I$$\n",
    "\n",
    "\n",
    "$$ \\hat A = \\tilde D ^{-\\frac{1}{2}} \\tilde A \\tilde D ^{-\\frac{1}{2}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_tilda_inv_sqrt @ A_tilda @ D_tilda_inv_sqrt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma \\,( \\tilde{D}^{-\\frac{1}{2}} \\, \\tilde{A} \\, \\tilde{D}^{-\\frac{1}{2}} \\, X \\, W)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.randn(5,5)\n",
    "W=np.random.randn(5,1)\n",
    "\n",
    "# New Representation of input.\n",
    "D_tilda_inv_sqrt @ A_tilda @ D_tilda_inv_sqrt @ X @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Create nx Network\n",
    "g = nx.from_numpy_array(A)\n",
    "# (2) Compute tilda\n",
    "A_mod = A + np.eye(g.number_of_nodes())\n",
    "\n",
    "# D for A_mod:\n",
    "D_mod = np.zeros_like(A_mod)\n",
    "np.fill_diagonal(D_mod, A_mod.sum(axis=1).flatten())\n",
    "\n",
    "# Inverse square root of D:\n",
    "D_mod_invroot = np.linalg.inv(sqrtm(D_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_labels = {i: i+1 for i in range(g.number_of_nodes())}\n",
    "pos = nx.planar_layout(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "nx.draw(\n",
    "    g, pos, with_labels=True, \n",
    "    labels=node_labels, \n",
    "    node_color='#83C167', edge_color='gray', node_size=1500, font_size=30, font_family='serif'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hat = D_mod_invroot @ A_mod @ D_mod_invroot\n",
    "A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.zeros((g.number_of_nodes(), 1))\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H[0,0] = 1 # the \"water drop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "iters = 20\n",
    "results = [H.flatten()]\n",
    "for i in range(iters):\n",
    "    H = A_hat @ H\n",
    "    results.append(H.flatten())\n",
    "print(f\"Initial signal input: {results[0]}\")\n",
    "print(f\"Final signal output after running {iters} steps of message-passing:  {results[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "kwargs = {'cmap': 'hot', 'node_size': 1500, 'edge_color': 'gray', \n",
    "          'vmin': np.array(results).min(), 'vmax': np.array(results).max()*1.1}\n",
    "\n",
    "def update(idx):\n",
    "    ax.clear()\n",
    "    colors = results[idx]\n",
    "    nx.draw(g, pos, node_color=colors, ax=ax, **kwargs)\n",
    "    ax.set_title(f\"Iter={idx}\", fontsize=20)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update, frames=len(results), interval=1000, repeat=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim.save('water_drop_example.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolution Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from networkx.algorithms.community.modularity_max import greedy_modularity_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_kkl(nx_G, label_map, node_color, pos=None, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    if pos is None:\n",
    "        pos = nx.spring_layout(nx_G, k=5/np.sqrt(nx_G.number_of_nodes()))\n",
    "\n",
    "    nx.draw(\n",
    "        nx_G, pos, with_labels=label_map is not None, \n",
    "        labels=label_map, \n",
    "        node_color=node_color, \n",
    "        ax=ax, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = nx.karate_club_graph()\n",
    "g.number_of_nodes(), g.number_of_edges()\n",
    "communities = greedy_modularity_communities(g)\n",
    "colors = np.zeros(g.number_of_nodes())\n",
    "for i, com in enumerate(communities):\n",
    "    colors[list(com)] = i\n",
    "\n",
    "n_classes = np.unique(colors).shape[0]\n",
    "labels = np.eye(n_classes)[colors.astype(int)]\n",
    "\n",
    "club_labels = nx.get_node_attributes(g,'club')\n",
    "\n",
    "_ = draw_kkl(g, None, colors, cmap='gist_rainbow', edge_color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "pos = nx.spring_layout(g, k=5/np.sqrt(g.number_of_nodes()))\n",
    "kwargs = {\"cmap\": 'gist_rainbow', \"edge_color\":'gray'}\n",
    "nx.draw(\n",
    "    g, pos, with_labels=False, \n",
    "    node_color=colors, \n",
    "    ax=ax, **kwargs)\n",
    "#plt.savefig('karate_club_graph.png', bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Compute adjacency matrix\n",
    "A = nx.to_numpy_matrix(g)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Add self connections\n",
    "A_mod = A + np.eye(g.number_of_nodes())\n",
    "\n",
    "# (3) Compute D\n",
    "D_mod = np.zeros_like(A_mod)\n",
    "np.fill_diagonal(D_mod, np.asarray(A_mod.sum(axis=1)).flatten())\n",
    "\n",
    "D_mod_invroot = np.linalg.inv(sqrtm(D_mod))\n",
    "\n",
    "A_hat = D_mod_invroot @ A_mod @ D_mod_invroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.eye(g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(nin, nout):\n",
    "    sd = np.sqrt(6.0 / (nin + nout))\n",
    "    return np.random.uniform(-sd, sd, size=(nin, nout))\n",
    "\n",
    "\n",
    "def xent(pred, labels):\n",
    "    return -np.log(pred)[np.arange(pred.shape[0]), np.argmax(labels, axis=1)]\n",
    "\n",
    "\n",
    "def norm_diff(dW, dW_approx):\n",
    "    return np.linalg.norm(dW - dW_approx) / (np.linalg.norm(dW) + np.linalg.norm(dW_approx))\n",
    "\n",
    "\n",
    "class GradDescentOptim():\n",
    "    def __init__(self, lr, wd):\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self._y_pred = None\n",
    "        self._y_true = None\n",
    "        self._out = None\n",
    "        self.bs = None\n",
    "        self.train_nodes = None\n",
    "        \n",
    "    def __call__(self, y_pred, y_true, train_nodes=None):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        \n",
    "        if train_nodes is None:\n",
    "            self.train_nodes = np.arange(y_pred.shape[0])\n",
    "        else:\n",
    "            self.train_nodes = train_nodes\n",
    "            \n",
    "        self.bs = self.train_nodes.shape[0]\n",
    "        \n",
    "    @property\n",
    "    def out(self):\n",
    "        return self._out\n",
    "    \n",
    "    @out.setter\n",
    "    def out(self, y):\n",
    "        self._out = y\n",
    "    \n",
    "\n",
    "class GCNLayer():\n",
    "    def __init__(self, n_inputs, n_outputs, activation=None, name=''):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.W = glorot_init(self.n_outputs, self.n_inputs)\n",
    "        self.activation = activation\n",
    "        self.name = name\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"GCN: W{'_'+self.name if self.name else ''} ({self.n_inputs}, {self.n_outputs})\"\n",
    "        \n",
    "    def forward(self, A, X, W=None):\n",
    "        \"\"\"\n",
    "        Assumes A is (bs, bs) adjacency matrix and X is (bs, D), \n",
    "            where bs = \"batch size\" and D = input feature length\n",
    "        \"\"\"\n",
    "        self._X = (A @ X).T # for calculating gradients.  (D, bs)\n",
    "        \n",
    "        if W is None:\n",
    "            W = self.W\n",
    "        \n",
    "        H = W @ self._X # (h, D)*(D, bs) -> (h, bs)\n",
    "        if self.activation is not None:\n",
    "            H = self.activation(H)\n",
    "        self._H = H # (h, bs)\n",
    "        return self._H.T # (bs, h)\n",
    "    \n",
    "    def backward(self, optim, update=True):\n",
    "        dtanh = 1 - np.asarray(self._H.T)**2 # (bs, out_dim)\n",
    "        d2 = np.multiply(optim.out, dtanh)  # (bs, out_dim) *element_wise* (bs, out_dim)\n",
    "        \n",
    "        optim.out = d2 @ self.W # (bs, out_dim)*(out_dim, in_dim) = (bs, in_dim)\n",
    "        \n",
    "        dW = np.asarray(d2.T @ self._X.T) / optim.bs  # (out_dim, bs)*(bs, D) -> (out_dim, D)\n",
    "        dW_wd = self.W * optim.wd / optim.bs # weight decay update\n",
    "        \n",
    "        if update:\n",
    "            self.W -= (dW + dW_wd) * optim.lr \n",
    "        \n",
    "        return dW + dW_wd\n",
    "\n",
    "    \n",
    "class SoftmaxLayer():\n",
    "    def __init__(self, n_inputs, n_outputs, name=''):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.W = glorot_init(self.n_outputs, self.n_inputs)\n",
    "        self.b = np.zeros((self.n_outputs, 1))\n",
    "        self.name = name\n",
    "        self._X = None # Used to calculate gradients\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Softmax: W{'_'+self.name if self.name else ''} ({self.n_inputs}, {self.n_outputs})\"\n",
    "    \n",
    "    def shift(self, proj):\n",
    "        shiftx = proj - np.max(proj, axis=0, keepdims=True)\n",
    "        exps = np.exp(shiftx)\n",
    "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
    "        \n",
    "    def forward(self, X, W=None, b=None):\n",
    "        \"\"\"Compute the softmax of vector x in a numerically stable way.\n",
    "        \n",
    "        X is assumed to be (bs, h)\n",
    "        \"\"\"\n",
    "        self._X = X.T\n",
    "        if W is None:\n",
    "            W = self.W\n",
    "        if b is None:\n",
    "            b = self.b\n",
    "\n",
    "        proj = np.asarray(W @ self._X) + b # (out, h)*(h, bs) = (out, bs)\n",
    "        return self.shift(proj).T # (bs, out)\n",
    "    \n",
    "    def backward(self, optim, update=True):\n",
    "        # should take in optimizer, update its own parameters and update the optimizer's \"out\"\n",
    "        # Build mask on loss\n",
    "        train_mask = np.zeros(optim.y_pred.shape[0])\n",
    "        train_mask[optim.train_nodes] = 1\n",
    "        train_mask = train_mask.reshape((-1, 1))\n",
    "        \n",
    "        # derivative of loss w.r.t. activation (pre-softmax)\n",
    "        d1 = np.asarray((optim.y_pred - optim.y_true)) # (bs, out_dim)\n",
    "        d1 = np.multiply(d1, train_mask) # (bs, out_dim) with loss of non-train nodes set to zero\n",
    "        \n",
    "        optim.out = d1 @ self.W # (bs, out_dim)*(out_dim, in_dim) = (bs, in_dim)\n",
    "        \n",
    "        dW = (d1.T @ self._X.T) / optim.bs  # (out_dim, bs)*(bs, in_dim) -> (out_dim, in_dim)\n",
    "        db = d1.T.sum(axis=1, keepdims=True) / optim.bs # (out_dim, 1)\n",
    "                \n",
    "        dW_wd = self.W * optim.wd / optim.bs # weight decay update\n",
    "        \n",
    "        if update:   \n",
    "            self.W -= (dW + dW_wd) * optim.lr\n",
    "            self.b -= db.reshape(self.b.shape) * optim.lr\n",
    "        \n",
    "        return dW + dW_wd, db.reshape(self.b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn1 = GCNLayer(g.number_of_nodes(), 2, activation=np.tanh, name='1')\n",
    "sm1 = SoftmaxLayer(2, n_classes, \"SM\")\n",
    "opt = GradDescentOptim(lr=0, wd=1.)\n",
    "gcn1_out = gcn1.forward(A_hat, X)\n",
    "opt(sm1.forward(gcn1_out), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grads(inputs, layer, argname, labels, eps=1e-4, wd=0):\n",
    "    cp = getattr(layer, argname).copy()\n",
    "    cp_flat = np.asarray(cp).flatten()\n",
    "    grads = np.zeros_like(cp_flat)\n",
    "    n_parms = cp_flat.shape[0]\n",
    "    for i, theta in enumerate(cp_flat):\n",
    "        #print(f\"Parm {argname}_{i}\")\n",
    "        theta_cp = theta\n",
    "        \n",
    "        # J(theta + eps)\n",
    "        cp_flat[i] = theta + eps\n",
    "        cp_tmp = cp_flat.reshape(cp.shape)\n",
    "        predp = layer.forward(*inputs, **{argname: cp_tmp})\n",
    "        wd_term = wd/2*(cp_flat**2).sum() / labels.shape[0]\n",
    "        #print(wd_term)\n",
    "        Jp = xent(predp, labels).mean() + wd_term\n",
    "        \n",
    "        # J(theta - eps)\n",
    "        cp_flat[i] = theta - eps\n",
    "        cp_tmp = cp_flat.reshape(cp.shape)\n",
    "        predm = layer.forward(*inputs, **{argname: cp_tmp})\n",
    "        wd_term = wd/2*(cp_flat**2).sum() / labels.shape[0]\n",
    "        #print(wd_term)\n",
    "        Jm = xent(predm, labels).mean() + wd_term\n",
    "        \n",
    "        # grad\n",
    "        grads[i] = ((Jp - Jm) / (2*eps))\n",
    "        \n",
    "        # Back to normal\n",
    "        cp_flat[i] = theta\n",
    "\n",
    "    return grads.reshape(cp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW_approx = get_grads((gcn1_out,), sm1, \"W\", labels, eps=1e-4, wd=opt.wd)\n",
    "db_approx = get_grads((gcn1_out,), sm1, \"b\", labels, eps=1e-4, wd=opt.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW, db = sm1.backward(opt, update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert norm_diff(dW, dW_approx) < 1e-7\n",
    "assert norm_diff(db, db_approx) < 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gcn_grads(inputs, gcn, sm_layer, labels, eps=1e-4, wd=0):\n",
    "    cp = gcn.W.copy()\n",
    "    cp_flat = np.asarray(cp).flatten()\n",
    "    grads = np.zeros_like(cp_flat)\n",
    "    n_parms = cp_flat.shape[0]\n",
    "    for i, theta in enumerate(cp_flat):\n",
    "        theta_cp = theta\n",
    "        \n",
    "        # J(theta + eps)\n",
    "        cp_flat[i] = theta + eps\n",
    "        cp_tmp = cp_flat.reshape(cp.shape)\n",
    "        pred = sm_layer.forward(gcn.forward(*inputs, W=cp_tmp))\n",
    "        w2 = (cp_flat**2).sum()+(sm_layer.W.flatten()**2).sum()\n",
    "        Jp = xent(pred, labels).mean() + wd/(2*labels.shape[0])*w2\n",
    "        \n",
    "        # J(theta - eps)\n",
    "        cp_flat[i] = theta - eps\n",
    "        cp_tmp = cp_flat.reshape(cp.shape)\n",
    "        pred = sm_layer.forward(gcn.forward(*inputs, W=cp_tmp))\n",
    "        w2 = (cp_flat**2).sum()+(sm_layer.W.flatten()**2).sum()\n",
    "        Jm = xent(pred, labels).mean() + wd/(2*labels.shape[0])*w2\n",
    "        \n",
    "        # grad\n",
    "        grads[i] = ((Jp - Jm) / (2*eps))\n",
    "        \n",
    "        # Back to normal\n",
    "        cp_flat[i] = theta\n",
    "\n",
    "    return grads.reshape(cp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = gcn1.backward(opt, update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2_approx = get_gcn_grads((A_hat, X), gcn1, sm1, labels, eps=1e-4, wd=opt.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert norm_diff(dW2, dW2_approx) < 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCNNetwork():\n",
    "    def __init__(self, n_inputs, n_outputs, n_layers, hidden_sizes, activation, seed=0):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.layers = list()\n",
    "        # Input layer\n",
    "        gcn_in = GCNLayer(n_inputs, hidden_sizes[0], activation, name='in')\n",
    "        self.layers.append(gcn_in)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for layer in range(n_layers):\n",
    "            gcn = GCNLayer(self.layers[-1].W.shape[0], hidden_sizes[layer], activation, name=f'h{layer}')\n",
    "            self.layers.append(gcn)\n",
    "            \n",
    "        # Output layer\n",
    "        sm_out = SoftmaxLayer(hidden_sizes[-1], n_outputs, name='sm')\n",
    "        self.layers.append(sm_out)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([str(l) for l in self.layers])\n",
    "    \n",
    "    def embedding(self, A, X):\n",
    "        # Loop through all GCN layers\n",
    "        H = X\n",
    "        for layer in self.layers[:-1]:\n",
    "            H = layer.forward(A, H)\n",
    "        return np.asarray(H)\n",
    "    \n",
    "    def forward(self, A, X):\n",
    "        # GCN layers\n",
    "        H = self.embedding(A, X)\n",
    "        \n",
    "        # Softmax\n",
    "        p = self.layers[-1].forward(H)\n",
    "        \n",
    "        return np.asarray(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gcn_model = GCNNetwork(\n",
    "    n_inputs=g.number_of_nodes(), \n",
    "    n_outputs=n_classes, \n",
    "    n_layers=2,\n",
    "    hidden_sizes=[16, 2], \n",
    "    activation=np.tanh,\n",
    "    seed=100,\n",
    ")\n",
    "gcn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gcn_model.forward(A_hat, X)\n",
    "embed = gcn_model.embedding(A_hat, X)\n",
    "xent(y_pred, labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = {i: embed[i,:] for i in range(embed.shape[0])}\n",
    "_ = draw_kkl(g, None, colors, pos=pos, cmap='gist_rainbow', edge_color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes = np.array([0, 1, 8])\n",
    "test_nodes = np.array([i for i in range(labels.shape[0]) if i not in train_nodes])\n",
    "opt2 = GradDescentOptim(lr=2e-2, wd=2.5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeds = list()\n",
    "accs = list()\n",
    "train_losses = list()\n",
    "test_losses = list()\n",
    "\n",
    "loss_min = 1e6\n",
    "es_iters = 0\n",
    "es_steps = 50\n",
    "# lr_rate_ramp = 0 #-0.05\n",
    "# lr_ramp_steps = 1000\n",
    "\n",
    "for epoch in range(15000):\n",
    "    \n",
    "    y_pred = gcn_model.forward(A_hat, X)\n",
    "\n",
    "    opt2(y_pred, labels, train_nodes)\n",
    "    \n",
    "#     if ((epoch+1) % lr_ramp_steps) == 0:\n",
    "#         opt2.lr *= 1+lr_rate_ramp\n",
    "#         print(f\"LR set to {opt2.lr:.4f}\")\n",
    "\n",
    "    for layer in reversed(gcn_model.layers):\n",
    "        layer.backward(opt2, update=True)\n",
    "        \n",
    "    embeds.append(gcn_model.embedding(A_hat, X))\n",
    "    # Accuracy for non-training nodes\n",
    "    acc = (np.argmax(y_pred, axis=1) == np.argmax(labels, axis=1))[\n",
    "        [i for i in range(labels.shape[0]) if i not in train_nodes]\n",
    "    ]\n",
    "    accs.append(acc.mean())\n",
    "    \n",
    "    loss = xent(y_pred, labels)\n",
    "    loss_train = loss[train_nodes].mean()\n",
    "    loss_test = loss[test_nodes].mean()\n",
    "    \n",
    "    train_losses.append(loss_train)\n",
    "    test_losses.append(loss_test)\n",
    "    \n",
    "    if loss_test < loss_min:\n",
    "        loss_min = loss_test\n",
    "        es_iters = 0\n",
    "    else:\n",
    "        es_iters += 1\n",
    "        \n",
    "    if es_iters > es_steps:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch+1}, Train Loss: {loss_train:.3f}, Test Loss: {loss_test:.3f}\")\n",
    "        \n",
    "train_losses = np.array(train_losses)\n",
    "test_losses = np.array(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.log10(train_losses), label='Train')\n",
    "ax.plot(np.log10(test_losses), label='Test')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = {i: embeds[-1][i,:] for i in range(embeds[-1].shape[0])}\n",
    "_ = draw_kkl(g, None, colors, pos=pos, cmap='gist_rainbow', edge_color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = ax.plot(accs, marker='o')\n",
    "ax.grid()\n",
    "_ = ax.set(ylim=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "snapshots = np.linspace(0, len(embeds)-1, N).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "kwargs = {'cmap': 'gist_rainbow', 'edge_color': 'gray', }#'node_size': 55}\n",
    "\n",
    "def update(idx):\n",
    "    ax.clear()\n",
    "    embed = embeds[snapshots[idx]]\n",
    "    pos = {i: embed[i,:] for i in range(embed.shape[0])}\n",
    "    nx.draw(g, pos, node_color=colors, ax=ax, **kwargs)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update, frames=snapshots.shape[0], interval=10, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim.save('embed_anim.mp4', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pDL",
   "language": "python",
   "name": "pdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
