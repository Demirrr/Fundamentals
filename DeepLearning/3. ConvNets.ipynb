{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convolutions\n",
    "\n",
    "\n",
    "Convolutional neural networks are my favs. In this tutorial, I will show you how to implement forward and bacward pass in convolutions.\n",
    "\n",
    "\n",
    "This tutorial is based on [CS231n Winter 2016: Lecture 6-7: Neural Networks, Convolutional Neural Networks](https://cs231n.github.io/convolutional-networks/), [video](https://www.youtube.com/watch?v=i94OvYb6noo&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&index=4).\n",
    "\n",
    "### TL,DR\n",
    "\n",
    "+ Implement forward and backward computation flows of convolutions with numpy.\n",
    "+ Sanity check with pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_checker(a,b):\n",
    "    return a.shape==b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self,in_channels=1, out_channels=1,kernel_size=(2, 2), stride=1, padding=0):\n",
    "        \n",
    "        self.kernel_h,self.kernel_w=kernel_size\n",
    "        self.weight=np.random.randn(out_channels,\n",
    "                               in_channels,\n",
    "                               self.kernel_h,\n",
    "                               self.kernel_w) /np.sqrt(in_channels/2)\n",
    "        self.bias=np.zeros(out_channels)    \n",
    "\n",
    "        \n",
    "        self.stride=stride\n",
    "        self.padding=padding\n",
    "\n",
    "        # Gradients.\n",
    "        self.dweight, self.dbias=None, None\n",
    "        self.cache=dict()\n",
    "\n",
    "    def set_params(self,weights,bias=None):\n",
    "        self.weight,self.bias=weights, bias\n",
    "        n,d,self.kernel_h,self.kernel_w=self.weight.shape        \n",
    "    \n",
    "    def compute_dim(self,X):\n",
    "        # parameter check\n",
    "        xN, xD, xH, xW = X.shape\n",
    "        wN, wD, wH, wW = self.weight.shape\n",
    "        assert wH == wW\n",
    "        assert (xH - wH) % self.stride == 0\n",
    "        assert (xW - wW) % self.stride == 0\n",
    "        self.cache['X']=X\n",
    "        \n",
    "        zH, zW = (xH - wH) // self.stride + 1, (xW - wW) // self.stride + 1\n",
    "        zD,zN = wN,xN\n",
    "        return np.zeros((zN, zD, zH, zW))\n",
    "    \n",
    "    def get_region(self,hight,width):\n",
    "        h1=hight*self.stride\n",
    "        h2=h1+self.kernel_h\n",
    "        w1=width*self.stride\n",
    "        w2=w1+self.kernel_w\n",
    "        return h1,h2,w1,w2\n",
    "    \n",
    "    def convolve_forward_step(self,X_n):\n",
    "        xD, xH, xW = X_n.shape\n",
    "        hZ=int((xH-self.kernel_h)/self.stride+1)\n",
    "        wZ=int((xW-self.kernel_w)/self.stride+1)\n",
    "        Z = np.zeros((len(self.weight),hZ, wZ))\n",
    "        \n",
    "        for d in range(len(Z)):\n",
    "            for i in range(hZ):\n",
    "                for j in range(wZ):\n",
    "                    h1,h2,w1,w2=self.get_region(i,j)\n",
    "                    x_loc = X_n[:, \n",
    "                              h1: h2,\n",
    "                              w1: w2]\n",
    "                    Z[d,i,j]=np.sum(x_loc*self.weight[d])+ self.bias[d]\n",
    "        return Z\n",
    "    \n",
    "    def forward(self,X):\n",
    "        Z=self.compute_dim(X)\n",
    "        for n in range(len(Z)):\n",
    "            Z[n,:,:,:]=self.convolve_forward_step(X[n])\n",
    "        self.cache['Z']=Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):        \n",
    "        assert dim_checker(dZ,self.cache['Z'])\n",
    "        \n",
    "        dX, self.dweight, self.dbias=np.zeros(self.cache['X'].shape), np.zeros(self.weight.shape),np.zeros(self.bias.shape)\n",
    "        (N, depth, hight, width) = dZ.shape\n",
    "         \n",
    "        for n in range(N):\n",
    "            for h in range(hight):        \n",
    "                for w in range(width):      \n",
    "                    for d in range(depth): # correcponds to d.th kernel\n",
    "                        h1,h2,w1,w2=self.get_region(h,w)\n",
    "                        dX[n,:,h1:h2,w1:w2]+= self.weight[d,:,:,:] * dZ[n, d, h, w]\n",
    "                        self.dweight[d,:,:,:] += self.cache['X'][n, :, h1:h2, w1:w2] * dZ[n, d, h, w]            \n",
    "                        self.dbias[d] +=dZ[n, d, h, w]\n",
    "                    \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data and determine the hyperparameters of convolution.\n",
    "xN, xD, xH, xW =3, 3, 4, 4\n",
    "X = np.random.randn(xN, xD, xH, xW)\n",
    "#kernel init\n",
    "nW, k, stride = 3, 2, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution with forward and backward computaton with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "conv = nn.Conv1d(in_channels=xD, out_channels=nW,kernel_size=(k, k), stride=stride)\n",
    "\n",
    "weights=conv.weight.data.detach().numpy()\n",
    "bias=conv.bias.data.detach().numpy()\n",
    "\n",
    "x_torch = torch.from_numpy(X).float() \n",
    "x_torch = Variable(x_torch, requires_grad=True)\n",
    "# Compute Conv\n",
    "res=conv(x_torch)\n",
    "# Sum the res\n",
    "out=res.sum()\n",
    "out.backward() # compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0425, -0.5480],\n",
       "          [ 0.0891, -0.0269]],\n",
       "\n",
       "         [[-0.6784, -0.5389],\n",
       "          [-0.3247, -0.0879]],\n",
       "\n",
       "         [[-0.0854,  1.0464],\n",
       "          [ 0.0661,  0.7227]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6650, -0.2599],\n",
       "          [-0.8087, -0.3958]],\n",
       "\n",
       "         [[ 0.5801, -0.7693],\n",
       "          [-0.2034, -1.0046]],\n",
       "\n",
       "         [[-0.1822,  0.0312],\n",
       "          [-0.2092, -0.6925]]],\n",
       "\n",
       "\n",
       "        [[[-0.0535, -0.3817],\n",
       "          [ 0.4270,  0.4379]],\n",
       "\n",
       "         [[-0.2526,  0.0221],\n",
       "          [ 0.4212,  1.1195]],\n",
       "\n",
       "         [[-0.3280,  1.5954],\n",
       "          [ 0.1795,  0.5587]]]], grad_fn=<MkldnnConvolutionBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output of conv\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution with forward and backward computaton with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our framework\n",
    "our_conv = Conv(in_channels=xD, out_channels=nW, kernel_size=(k, k), stride=stride)\n",
    "# Use the same weights and bias.\n",
    "our_conv.set_params(weights=weights,\n",
    "                    bias=bias)\n",
    "# Compute Conv\n",
    "Z=our_conv.forward(X)\n",
    "# Compute gradients. Note that gradient of addition is 1.\n",
    "dX=our_conv.backward(np.ones(Z.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.04249447, -0.54797479],\n",
       "         [ 0.08912934, -0.02693765]],\n",
       "\n",
       "        [[-0.67843511, -0.53892653],\n",
       "         [-0.32466128, -0.08789815]],\n",
       "\n",
       "        [[-0.08541131,  1.04639815],\n",
       "         [ 0.06606271,  0.72273387]]],\n",
       "\n",
       "\n",
       "       [[[ 0.66503736, -0.25988651],\n",
       "         [-0.80873865, -0.39577682]],\n",
       "\n",
       "        [[ 0.58013828, -0.76932111],\n",
       "         [-0.20342898, -1.00464547]],\n",
       "\n",
       "        [[-0.1821961 ,  0.03121532],\n",
       "         [-0.20915804, -0.69251473]]],\n",
       "\n",
       "\n",
       "       [[[-0.05345719, -0.38167461],\n",
       "         [ 0.42704866,  0.43793867]],\n",
       "\n",
       "        [[-0.25256085,  0.02212139],\n",
       "         [ 0.42118294,  1.11945092]],\n",
       "\n",
       "        [[-0.32799742,  1.59537328],\n",
       "         [ 0.1795361 ,  0.55865354]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.allclose(res.data.detach().numpy(),Z,atol=1e6))\n",
    "assert np.all(np.allclose(x_torch.grad.data.detach().numpy(),dX))\n",
    "assert np.all(np.allclose(conv.bias.grad.data.detach().numpy(),our_conv.dbias))\n",
    "assert np.all(np.allclose(conv.weight.grad.data.detach().numpy(),our_conv.dweight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We show how to implement forward and backward computation in convolutions\n",
    "# However, let's take a look at it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 1.0], \n",
    "                  [0.0, 1.0, 0.0], \n",
    "                  [0.0, 0.0, 0.0]])\n",
    "K = torch.tensor([[1.0, 0.0], \n",
    "                  [1.0, 0.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What corr2d(X, K) tells us ?\n",
    "\n",
    "Corr2d draws an image to us by looking at X from the eyes of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 50])\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batchsize=1\n",
    "channel_in=1\n",
    "height=4\n",
    "width=50\n",
    "num_kernels=32\n",
    "kernel_size=3\n",
    "x=torch.randn(batchsize, channel_in, height, width)\n",
    "print(x.shape)\n",
    "print(x.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num param in conv 288\n",
      "Num of output: 6400\n",
      "Num param in mlp 1280000\n",
      "Param ratio: 4444.444444444444\n"
     ]
    }
   ],
   "source": [
    "# 2D conv in an 2D tensor, a matrix\n",
    "conv = torch.nn.Conv2d(in_channels=channel_in, out_channels=num_kernels, kernel_size=kernel_size, stride=1,padding=1)\n",
    "print('Num param in conv',conv.weight.numel())\n",
    "print('Num of output:',conv(x).numel())\n",
    "\n",
    "mlp=torch.nn.Linear(channel_in*height*width, conv(x).numel()) \n",
    "\n",
    "print('Num param in mlp',mlp.weight.numel())\n",
    "# Same number of output\n",
    "assert mlp(torch.flatten(x)).numel() == conv(x).numel()\n",
    "print('Param ratio:',mlp.weight.numel()/conv.weight.numel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1, 50])\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Convert the representation\n",
    "channel_in=4\n",
    "height=1\n",
    "x=torch.randn(batchsize, channel_in, height, width)\n",
    "print(x.shape)\n",
    "print(x.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num param in conv 1152\n",
      "Num of output: 1600\n",
      "Num param in mlp 320000\n",
      "Param ratio: 277.77777777777777\n"
     ]
    }
   ],
   "source": [
    "# 2D conv in an 2D tensor, a matrix\n",
    "conv = torch.nn.Conv2d(in_channels=channel_in, out_channels=num_kernels, kernel_size=kernel_size, stride=1,padding=1)\n",
    "print('Num param in conv',conv.weight.numel())\n",
    "print('Num of output:',conv(x).numel())\n",
    "\n",
    "\n",
    "mlp=torch.nn.Linear(channel_in*height*width, conv(x).numel()) \n",
    "print('Num param in mlp',mlp.weight.numel())\n",
    "\n",
    "# Same number of output\n",
    "assert mlp(torch.flatten(x)).numel() == conv(x).numel()\n",
    "print('Param ratio:',mlp.weight.numel()/conv.weight.numel())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pDL",
   "language": "python",
   "name": "pdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
