{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of a function\n",
    "\n",
    "Let $f$ be a function of $f:\\mathbb{R}^n \\mapsto \\mathbb{R}^m$. The derivative/slope of $f$ on $x$ corresponds the rate of change of $f$ with respect to $x$ and defined as :\n",
    "\n",
    "$$ \\frac{d f(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time advances. Let $x$ be time in seconds and $f$ is a function indicating the position of the ball in the direction of the right. \n",
    "\n",
    "\n",
    "* The $f_1(x)$ does not change depending on the input $x$. Consequently, an infinitesimal change on $x+h$ does not have any effect on the rate of change in $f_1$,i.e.,$f_1(x)=f_1(x+h)$. In otherwords, **the ball does not move in any direction**.\n",
    "\n",
    "* The $f_2(x)$ changes depending on $x$. An infinitesimal change $x+h$ corresponds to an infinitesimal change on $f_2$,i.e.\n",
    "$(f_2(x+h)-f_2(x))/h=1$. Consequently, the rate of change is **1**. otherwords, **the ball is moving in the direction of the right with a constant velocity.** I\n",
    "\n",
    "* The output of $f_3$ changes depending on $x$. An infinitesimal change $x+h$ corresponds to an infinitesimal change on $f_2$,i.e.\n",
    "$(f_2(x+h)-f_2(x))/h=-1$. Consequently, the rate of change is **-1**. In otherwords, **the ball moving with a constant velocity in the direction of the left**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "f_1= lambda x: 0 # Gradient is always 0.\n",
    "f_2= lambda x: x # Gradient is always close to 1.\n",
    "f_3= lambda x: -x # Gradient is always close to -1.\n",
    "\n",
    "f_4= lambda x: x**2 # Gradient increases as the input increases\n",
    "f_5= lambda x: -x**2 # Gradient increases as the input increases\n",
    "f_6= lambda x: np.sqrt(x**2).sum() \n",
    "f_7= lambda x: x.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_derivative(f,x):\n",
    "    h=.00001\n",
    "    return (f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########f_1(x)=0##########\n",
      "f( 1.0 )=0\t df at x is 0.0\n",
      "f( 1.5 )=0\t df at x is 0.0\n",
      "f( 2.0 )=0\t df at x is 0.0\n",
      "f( 5.0 )=0\t df at x is 0.0\n",
      "f( 10.0 )=0\t df at x is 0.0\n",
      "#########f_2(x)=x##########\n",
      "f( 1.0 )=1.0\t df at x is 1.0000000000065512\n",
      "f( 1.5 )=1.5\t df at x is 1.0000000000065512\n",
      "f( 2.0 )=2.0\t df at x is 1.0000000000065512\n",
      "f( 5.0 )=5.0\t df at x is 0.9999999999621422\n",
      "f( 10.0 )=10.0\t df at x is 0.9999999999621422\n",
      "#########f_3(x)=-x##########\n",
      "f( 1.0 )=-1.0\t df at x is -1.0000000000065512\n",
      "f( 1.5 )=-1.5\t df at x is -1.0000000000065512\n",
      "f( 2.0 )=-2.0\t df at x is -1.0000000000065512\n",
      "f( 5.0 )=-5.0\t df at x is -0.9999999999621422\n",
      "f( 10.0 )=-10.0\t df at x is -0.9999999999621422\n",
      "#########f_4(x)=x**2##########\n",
      "f( 1.0 )=1.0\t df at x is 2.00001000001393\n",
      "f( 1.5 )=2.25\t df at x is 3.0000100000204806\n",
      "f( 2.0 )=4.0\t df at x is 4.000010000027032\n",
      "f( 5.0 )=25.0\t df at x is 10.000009999444615\n",
      "f( 10.0 )=100.0\t df at x is 20.00000999942131\n",
      "#########f_5(x)=x**-2##########\n",
      "f( 1.0 )=-1.0\t df at x is -2.00001000001393\n",
      "f( 1.5 )=-2.25\t df at x is -3.0000100000204806\n",
      "f( 2.0 )=-4.0\t df at x is -4.000010000027032\n",
      "f( 5.0 )=-25.0\t df at x is -10.000009999444615\n",
      "f( 10.0 )=-100.0\t df at x is -20.00000999942131\n"
     ]
    }
   ],
   "source": [
    "# Gradients always fixed in f_1 to f_3Let's calculate derivatives of 3 different function\n",
    "print('#########f_1(x)=0##########')\n",
    "for x in [1.0, 1.5, 2.0 ,5.0, 10.0]:\n",
    "    print('f( {0} )={1}\\t df at x is {2}'.format(x,f_1(x),calculate_derivative(f_1,x)))\n",
    "    \n",
    "print('#########f_2(x)=x##########')\n",
    "for x in [1.0, 1.5, 2.0 ,5.0, 10.0]:\n",
    "    print('f( {0} )={1}\\t df at x is {2}'.format(x,f_2(x),calculate_derivative(f_2,x)))\n",
    "    \n",
    "print('#########f_3(x)=-x##########')\n",
    "for x in [1.0, 1.5, 2.0 ,5.0, 10.0]:\n",
    "    print('f( {0} )={1}\\t df at x is {2}'.format(x,f_3(x),calculate_derivative(f_3,x)))\n",
    "    \n",
    "print('#########f_4(x)=x**2##########')\n",
    "for x in [1.0, 1.5, 2.0 ,5.0, 10.0]:\n",
    "    print('f( {0} )={1}\\t df at x is {2}'.format(x,f_4(x),calculate_derivative(f_4,x)))\n",
    "    \n",
    "print('#########f_5(x)=x**-2##########')\n",
    "for x in [1.0, 1.5, 2.0 ,5.0, 10.0]:\n",
    "    print('f( {0} )={1}\\t df at x is {2}'.format(x,f_5(x),calculate_derivative(f_5,x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The output of $f_4$ changes depending on $x$. Increasing $x$ increases the output of $f_3$ **significantly **. The rate of change is positively follows the the $x$. In otherwords, **the ball moving with an increasing velocity in the direction of the right**.\n",
    "\n",
    "\n",
    "* The output of $f_5$ changes depending on $x$. Increasing $x$ decreases the output of $f_3$ **significantly **. The rate of change is negatively follows the behaviour of the $x$. In otherwords, **the ball moving with an increasing velocity in the direction of the left**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytical and Numerical Gradient\n",
    "\n",
    "Up until now, We use $f:\\mathbb{R}^n \\mapsto \\mathbb{R}^m$ where $n=1$ and $m=1$ and we **numerically calculate** the the derivative of $f$. Such numerical calculation becomes **very slow** as the $n$ increases as shown in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_partial_derivatives(f,x):\n",
    "    \"\"\"\n",
    "    x is a numpy array (K,)\n",
    "    f is a function that takes x as input and generates R.\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros(len(x))\n",
    "    h = 0.00001\n",
    "\n",
    "    for ith, val in enumerate(x):\n",
    "        x[ith]+=h # increment by h\n",
    "        fxh = f(x) # evalute f(x + h)\n",
    "        # compute the partial derivative\n",
    "        grad[ith]= (fxh - fx) / h # the slope\n",
    "        x[ith]=val    \n",
    "    return grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Derivatives, Gradients, Jacobians\n",
    "\n",
    "\n",
    "## 2.1. Scalar valued functions\n",
    "Let $f$ be a scalar valued function and defined as\n",
    "(say $f:\\mathbb{R}^1 \\mapsto \\mathbb{R}^1$). The derivative of $f$ is computed as \n",
    "\n",
    "$$ \\frac{\\partial f(x)}{\\partial x} = lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$\n",
    "\n",
    "\n",
    "Let $f$ be a scalar valued function and defined as\n",
    "(say $f:\\mathbb{R}^m \\mapsto \\mathbb{R}^1$). The derivative of $f$ is a vector of partial derivatives. Similarly,\n",
    "$\\frac{\\partial f(x)}{\\partial x_i}$ reveals us how much $f(x)$ increases if $x_i$ increases. Strictly speaking, **gradients** are only defined for scalar functions. For vector valued functions we are dealing with vector of partial derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def drelu(x):\n",
    "    dr=np.ones((x.shape))\n",
    "    dr[x <= 0] = 0\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[-1.3  0.5  2.1]\n",
      "### Sigmoid ###\n",
      "S:\n",
      "[0.21416502 0.62245933 0.89090318]\n",
      "dS/X:\n",
      " [0.16829836 0.23500371 0.0971947 ]\n",
      "### RELU ###\n",
      "R:\n",
      "[0.  0.5 2.1]\n",
      "dR/X:\n",
      " [0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([-1.3, 0.5, 2.1])\n",
    "print('X:\\n{0}'.format(x))\n",
    "print('### Sigmoid ###')\n",
    "sx=sigmoid(x)\n",
    "print('S:\\n{0}'.format(sx))\n",
    "print('dS/X:\\n',dsigmoid(x))\n",
    "\n",
    "print('### RELU ###')\n",
    "rx=relu(x)\n",
    "print('R:\\n{0}'.format(rx))\n",
    "print('dR/X:\\n',drelu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Vector  valued functions\n",
    "Let $f$ be a vector valued function (say $f:\\mathbb{R}^n \\mapsto \\mathbb{R}^m$).\n",
    "\n",
    "$$ f(\\vec{x}):\\begin{bmatrix}\n",
    "\\vec{x}_1\\\\\n",
    "\\vec{x}_2\\\\\n",
    "\\cdots\\\\\n",
    "\\vec{x}_n\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "\\vec{y}_1\\\\\n",
    "\\vec{y}_2\\\\\n",
    "\\cdots\\\\\n",
    "\\vec{y}_m\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Then the gradient of $\\vec{y}=f(\\vec{x})$ with respect to $\\vec{x}$ is a Jacobian matrix:\n",
    "\n",
    "$$ \\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align} $$\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_func(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z), axis=0)).T\n",
    "    return sm\n",
    "def softmax_grad(s):\n",
    "    jacobian_m = np.diag(s)\n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m)):\n",
    "            if i == j:\n",
    "                jacobian_m[i][j] = s[i] * (1 - s[i])\n",
    "            else:\n",
    "                jacobian_m[i][j] = -s[i] * s[j]\n",
    "    return jacobian_m\n",
    "\n",
    "def softmax_grad_vec(softmax):\n",
    "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "    s = softmax.reshape(-1, 1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[-3.4 -1.6  0. ] \t \n",
      "S:\n",
      "[0.02701699 0.16344326 0.80953975]\n",
      "J:\n",
      " [[ 0.02628707 -0.00441574 -0.02187133]\n",
      " [-0.00441574  0.13672956 -0.13231381]\n",
      " [-0.02187133 -0.13231381  0.15418514]]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([-1.3, 0.5, 2.1])\n",
    "sx=softmax_func(x)\n",
    "print('X:\\n{0} \\t \\nS:\\n{1}'.format(x,sx))\n",
    "print('J:\\n',softmax_grad(sx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conex)",
   "language": "python",
   "name": "conex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
