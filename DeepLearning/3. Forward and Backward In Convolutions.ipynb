{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Forward and Backward with Convolutions\n",
    "\n",
    "\n",
    "\n",
    "This tutorial is based on [CS231n Winter 2016: Lecture 6-7: Neural Networks, Convolutional Neural Networks](https://cs231n.github.io/convolutional-networks/), [video](https://www.youtube.com/watch?v=i94OvYb6noo&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&index=4).\n",
    "\n",
    "### TL,DR\n",
    "\n",
    "+ Implement forward and backward computation flows of convolutions with numpy.\n",
    "+ Sanity check with pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_checker(a,b):\n",
    "    return a.shape==b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self,in_channels=1, out_channels=1,kernel_size=(2, 2), stride=1, padding=0):\n",
    "        \n",
    "        self.kernel_h,self.kernel_w=kernel_size\n",
    "        self.weight=np.random.randn(out_channels,\n",
    "                               in_channels,\n",
    "                               self.kernel_h,\n",
    "                               self.kernel_w) /np.sqrt(in_channels/2)\n",
    "        self.bias=np.zeros(out_channels)    \n",
    "\n",
    "        \n",
    "        self.stride=stride\n",
    "        self.padding=padding\n",
    "\n",
    "        # Gradients.\n",
    "        self.dweight, self.dbias=None, None\n",
    "        self.cache=dict()\n",
    "\n",
    "    def set_params(self,weights,bias=None):\n",
    "        self.weight,self.bias=weights, bias\n",
    "        n,d,self.kernel_h,self.kernel_w=self.weight.shape        \n",
    "    \n",
    "    def compute_dim(self,X):\n",
    "        # parameter check\n",
    "        xN, xD, xH, xW = X.shape\n",
    "        wN, wD, wH, wW = self.weight.shape\n",
    "        assert wH == wW\n",
    "        assert (xH - wH) % self.stride == 0\n",
    "        assert (xW - wW) % self.stride == 0\n",
    "        self.cache['X']=X\n",
    "        \n",
    "        zH, zW = (xH - wH) // self.stride + 1, (xW - wW) // self.stride + 1\n",
    "        zD,zN = wN,xN\n",
    "        return np.zeros((zN, zD, zH, zW))\n",
    "    \n",
    "    def get_region(self,hight,width):\n",
    "        h1=hight*self.stride\n",
    "        h2=h1+self.kernel_h\n",
    "        w1=width*self.stride\n",
    "        w2=w1+self.kernel_w\n",
    "        return h1,h2,w1,w2\n",
    "    \n",
    "    def convolve_forward_step(self,X_n):\n",
    "        xD, xH, xW = X_n.shape\n",
    "        hZ=int((xH-self.kernel_h)/self.stride+1)\n",
    "        wZ=int((xW-self.kernel_w)/self.stride+1)\n",
    "        Z = np.zeros((len(self.weight),hZ, wZ))\n",
    "        \n",
    "        for d in range(len(Z)):\n",
    "            for i in range(hZ):\n",
    "                for j in range(wZ):\n",
    "                    h1,h2,w1,w2=self.get_region(i,j)\n",
    "                    x_loc = X_n[:, \n",
    "                              h1: h2,\n",
    "                              w1: w2]\n",
    "                    Z[d,i,j]=np.sum(x_loc*self.weight[d])+ self.bias[d]\n",
    "        return Z\n",
    "    \n",
    "    def forward(self,X):\n",
    "        Z=self.compute_dim(X)\n",
    "        for n in range(len(Z)):\n",
    "            Z[n,:,:,:]=self.convolve_forward_step(X[n])\n",
    "        self.cache['Z']=Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):        \n",
    "        assert dim_checker(dZ,self.cache['Z'])\n",
    "        \n",
    "        dX, self.dweight, self.dbias=np.zeros(self.cache['X'].shape), np.zeros(self.weight.shape),np.zeros(self.bias.shape)\n",
    "        (N, depth, hight, width) = dZ.shape\n",
    "         \n",
    "        for n in range(N):\n",
    "            for h in range(hight):        \n",
    "                for w in range(width):      \n",
    "                    for d in range(depth): # correcponds to d.th kernel\n",
    "                        h1,h2,w1,w2=self.get_region(h,w)\n",
    "                        dX[n,:,h1:h2,w1:w2]+= self.weight[d,:,:,:] * dZ[n, d, h, w]\n",
    "                        self.dweight[d,:,:,:] += self.cache['X'][n, :, h1:h2, w1:w2] * dZ[n, d, h, w]            \n",
    "                        self.dbias[d] +=dZ[n, d, h, w]\n",
    "                    \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data and determine the hyperparameters of convolution.\n",
    "xN, xD, xH, xW =3, 3, 4, 4\n",
    "X = np.random.randn(xN, xD, xH, xW)\n",
    "#kernel init\n",
    "nW, k, stride = 3, 2, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution with forward and backward computaton with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "conv = nn.Conv1d(in_channels=xD, out_channels=nW,kernel_size=(k, k), stride=stride)\n",
    "\n",
    "weights=conv.weight.data.detach().numpy()\n",
    "bias=conv.bias.data.detach().numpy()\n",
    "\n",
    "x_torch = torch.from_numpy(X).float() \n",
    "x_torch = Variable(x_torch, requires_grad=True)\n",
    "# Compute Conv\n",
    "res=conv(x_torch)\n",
    "# Sum the res\n",
    "out=res.sum()\n",
    "out.backward() # compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1736, -1.0017],\n",
       "          [ 0.3615,  0.6587]],\n",
       "\n",
       "         [[ 0.2726,  0.3989],\n",
       "          [-0.4202,  0.0123]],\n",
       "\n",
       "         [[-0.0096, -0.1182],\n",
       "          [ 0.0696, -0.5973]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5397,  0.9625],\n",
       "          [-0.0459,  0.0360]],\n",
       "\n",
       "         [[-0.4860, -1.0586],\n",
       "          [ 0.4743, -0.2946]],\n",
       "\n",
       "         [[ 0.3986,  0.0487],\n",
       "          [ 0.0093,  0.3697]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4263, -0.8408],\n",
       "          [ 0.5263, -0.1460]],\n",
       "\n",
       "         [[-0.0229,  0.4724],\n",
       "          [ 0.1137, -0.1686]],\n",
       "\n",
       "         [[ 0.4142,  0.5402],\n",
       "          [-0.5596,  0.7685]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output of conv\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution with forward and backward computaton with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our framework\n",
    "our_conv = Conv(in_channels=xD, out_channels=nW, kernel_size=(k, k), stride=stride)\n",
    "# Use the same weights and bias.\n",
    "our_conv.set_params(weights=weights,\n",
    "                    bias=bias)\n",
    "# Compute Conv\n",
    "Z=our_conv.forward(X)\n",
    "# Compute gradients. Note that gradient of addition is 1.\n",
    "dX=our_conv.backward(np.ones(Z.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.17356728, -1.00174533],\n",
       "         [ 0.36150299,  0.65872909]],\n",
       "\n",
       "        [[ 0.27264468,  0.39885802],\n",
       "         [-0.42024546,  0.01231507]],\n",
       "\n",
       "        [[-0.00960317, -0.11824357],\n",
       "         [ 0.06955349, -0.59728522]]],\n",
       "\n",
       "\n",
       "       [[[ 0.53967837,  0.96254086],\n",
       "         [-0.04587774,  0.03596001]],\n",
       "\n",
       "        [[-0.48599323, -1.05856395],\n",
       "         [ 0.47428459, -0.29457397]],\n",
       "\n",
       "        [[ 0.3985896 ,  0.04870414],\n",
       "         [ 0.00931856,  0.36972917]]],\n",
       "\n",
       "\n",
       "       [[[ 0.42632537, -0.84082647],\n",
       "         [ 0.52628804, -0.14603831]],\n",
       "\n",
       "        [[-0.02294473,  0.47241284],\n",
       "         [ 0.11368198, -0.16859461]],\n",
       "\n",
       "        [[ 0.41421841,  0.54022526],\n",
       "         [-0.55958669,  0.76850111]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.allclose(res.data.detach().numpy(),Z,atol=1e6))\n",
    "assert np.all(np.allclose(x_torch.grad.data.detach().numpy(),dX))\n",
    "assert np.all(np.allclose(conv.bias.grad.data.detach().numpy(),our_conv.dbias))\n",
    "assert np.all(np.allclose(conv.weight.grad.data.detach().numpy(),our_conv.dweight))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
