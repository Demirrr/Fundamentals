{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Embedding\n",
    "\n",
    "In this tutorial, I would like to briefly explain Knowledge Graph and implement [TransE](https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html) (Bordes et al. 2013 )\n",
    "from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Background of Learning Embeddings\n",
    "\n",
    "\n",
    "+ [Bengio et al. (2003)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) suggest that **learning a distributed representation for words** can be an effective means to tackle the curse of dimensionality at learning the join probability of sequence of words in a language. \n",
    "\n",
    "+ In this context, the impact of **The curse of dimsionsality** can be shown as follows. To compute the joint probability of **n** words in a language having the vocabulary **V**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute the joint probability of 3 items of a vocabulary of size 1: 1\n",
      "To compute the joint probability of 3 items of a vocabulary of size 10: 1000\n",
      "To compute the joint probability of 3 items of a vocabulary of size 100: 1000000\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "for n,size_of_vocabulary in [(3,1),(3,10), (3,100)]:\n",
    "    print(f'To compute the joint probability of {n} items of a vocabulary of size {size_of_vocabulary}: {len({i for i in product(range(size_of_vocabulary), repeat=n)})}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The number of parameters for learning the join prob. dist. does not increase linearly but exponentialy.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating Embeddings for Modeling Multi-relational Data\n",
    "[Bordes et al. 2013](https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html) propose to embed entities and relations of multi-relational data in low-dimensional vector space. In other words, **the goal is to model large knowledge graphs by learning a distributed representations for entities and relations based on translation operation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "1. Parse input knowledge graph via KG class\n",
    "2. Generate training dataset via DatasetTriple class\n",
    "3. Train TransE\n",
    "4. Report the filtered link prediction results of TransE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KG:\n",
    "    def __init__(self, data_dir=None):\n",
    "        \n",
    "        # 1. Parse the benchmark dataset\n",
    "        s = '------------------- Description of Dataset' + data_dir + '----------------------------'\n",
    "        print(f'\\n{s}')\n",
    "        self.train = self.load_data(data_dir + 'train.txt', add_reciprical=False)\n",
    "        self.valid = self.load_data(data_dir + 'valid.txt', add_reciprical=False)\n",
    "        self.test = self.load_data(data_dir + 'test.txt', add_reciprical=False)\n",
    "        \n",
    "        self.all_triples = self.train + self.valid + self.test\n",
    "        self.entities = self.get_entities(self.all_triples)\n",
    "        self.relations = self.get_relations(self.all_triples)\n",
    "\n",
    "        # 2. Index entities and relations\n",
    "        self.entity_idxs = {self.entities[i]: i for i in range(len(self.entities))}\n",
    "        self.relation_idxs = {self.relations[i]: i for i in range(len(self.relations))}\n",
    "\n",
    "        print(f'Number of triples: {len(self.all_triples)}')\n",
    "        print(f'Number of entities: {len(self.entities)}')\n",
    "        print(f'Number of relations: {len(self.relations)}')\n",
    "        print(f'Number of triples on train set: {len(self.train)}')\n",
    "        print(f'Number of triples on valid set: {len(self.valid)}')\n",
    "        print(f'Number of triples on test set: {len(self.test)}')\n",
    "        s = len(s) * '-'\n",
    "        print(f'{s}\\n')\n",
    "\n",
    "        # 3. Index train, validation and test sets \n",
    "        self.train_idx = [(self.entity_idxs[s], self.relation_idxs[p], self.entity_idxs[o]) for s, p, o in\n",
    "                          self.train]\n",
    "        self.valid_idx = [(self.entity_idxs[s], self.relation_idxs[p], self.entity_idxs[o]) for s, p, o in\n",
    "                          self.valid]\n",
    "        self.test_idx = [(self.entity_idxs[s], self.relation_idxs[p], self.entity_idxs[o]) for s, p, o in\n",
    "                         self.test]\n",
    "\n",
    "        # 4. Create mappings for the filtered link prediction\n",
    "        self.sp_vocab = dict()\n",
    "        self.po_vocab = dict()\n",
    "        self.so_vocab = dict()\n",
    "\n",
    "        for i in self.all_triples:\n",
    "            s, p, o = i[0], i[1], i[2]\n",
    "            s_idx, p_idx, o_idx = self.entity_idxs[s], self.relation_idxs[p], self.entity_idxs[o]\n",
    "            self.sp_vocab.setdefault((s_idx, p_idx), []).append(o_idx)\n",
    "            self.so_vocab.setdefault((s_idx, o_idx), []).append(p_idx)\n",
    "            self.po_vocab.setdefault((p_idx, o_idx), []).append(s_idx)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(data_dir, add_reciprical=True):\n",
    "        with open(data_dir, \"r\") as f:\n",
    "            data = f.read().strip().split(\"\\n\")\n",
    "            data = [i.split() for i in data]\n",
    "            if add_reciprical:\n",
    "                data += [[i[2], i[1] + \"_reverse\", i[0]] for i in data]\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def get_relations(data):\n",
    "        relations = sorted(list(set([d[1] for d in data])))\n",
    "        return relations\n",
    "\n",
    "    @staticmethod\n",
    "    def get_entities(data):\n",
    "        entities = sorted(list(set([d[0] for d in data] + [d[2] for d in data])))\n",
    "        return entities\n",
    "\n",
    "    @property\n",
    "    def num_entities(self):\n",
    "        return len(self.entities)\n",
    "    @property\n",
    "    def num_relations(self):\n",
    "        return len(self.relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------- Description of DatasetKGs/UMLS/----------------------------\n",
      "Number of triples: 6529\n",
      "Number of entities: 135\n",
      "Number of relations: 46\n",
      "Number of triples on train set: 5216\n",
      "Number of triples on valid set: 652\n",
      "Number of triples on test set: 661\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kg=KG(data_dir='KGs/UMLS/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset from Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTriple(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, num_entities=None, nneg=1,**kwargs):\n",
    "        data = torch.Tensor(data).long()\n",
    "        self.head_idx = data[:, 0]\n",
    "        self.rel_idx = data[:, 1]\n",
    "        self.tail_idx = data[:, 2]\n",
    "        self.num_entities = num_entities\n",
    "        self.nneg = nneg\n",
    "        assert self.head_idx.shape == self.rel_idx.shape == self.tail_idx.shape\n",
    "\n",
    "        self.length = len(self.head_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        h = self.head_idx[idx]\n",
    "        r = self.rel_idx[idx]\n",
    "        t = self.tail_idx[idx]\n",
    "        return h, r, t\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\" Generate Negative Triples\"\"\"\n",
    "        batch = torch.LongTensor(batch)\n",
    "        h, r, t = batch[:, 0], batch[:, 1], batch[:, 2]\n",
    "        size_of_batch, _ = batch.shape\n",
    "        assert size_of_batch > 0\n",
    "        label = torch.ones((size_of_batch,))\n",
    "        # Generate Negative Triples\n",
    "        corr = torch.randint(0, self.num_entities, (size_of_batch * self.nneg, 1))\n",
    "        \n",
    "        if torch.rand(1).item()>.5:\n",
    "            # 2.1 Head Corrupt:\n",
    "            h_corr = corr[:, 0]\n",
    "            r_corr = r.repeat(self.nneg, )\n",
    "            t_corr = t.repeat(self.nneg, )\n",
    "            label_corr = -torch.ones(len(t_corr), )\n",
    "        else:\n",
    "            # 2.2. Tail Corrupt\n",
    "            h_corr = h.repeat(self.nneg, )\n",
    "            r_corr = r.repeat(self.nneg, )\n",
    "            t_corr = corr[:, 0]\n",
    "            label_corr = -torch.ones(len(t_corr), )\n",
    "\n",
    "        # 3. Stack True and Corrupted Triples\n",
    "        h = torch.cat((h, h_corr), 0)\n",
    "        r = torch.cat((r, r_corr), 0)\n",
    "        t = torch.cat((t, t_corr), 0)\n",
    "        label = torch.cat((label, label_corr), 0)\n",
    "        return h, r, t, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define TransE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim,num_entities,num_relations,**kwargs):\n",
    "        super(TransE, self).__init__()\n",
    "        self.name = 'TransE'\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_entities = num_entities\n",
    "        self.num_relations = num_relations\n",
    "\n",
    "        self.emb_ent = nn.Embedding(self.num_entities, self.embedding_dim)\n",
    "        self.emb_rel = nn.Embedding(self.num_relations, self.embedding_dim)\n",
    "        \n",
    "        low,high=-6/torch.sqrt(torch.Tensor([self.embedding_dim])).item(),6/torch.sqrt(torch.Tensor([self.embedding_dim])).item()\n",
    "        self.emb_ent.weight.data.uniform_(low, high)\n",
    "        self.emb_rel.weight.data.uniform_(low, high)\n",
    "        \n",
    "        \n",
    "    def forward(self, e1_idx, rel_idx, e2_idx ):\n",
    "        # (1) Embeddings of head, relation and tail\n",
    "        emb_head, emb_rel, emb_tail = self.emb_ent(e1_idx),self.emb_rel(rel_idx), self.emb_ent(e2_idx)\n",
    "        # (2) Normalize head and tail entities\n",
    "        emb_head = F.normalize(emb_head, p=2,dim=1)\n",
    "        emb_tail = F.normalize(emb_tail, p=2,dim=1)\n",
    "        # (3) Compute Distance\n",
    "        distance = torch.norm((emb_head + emb_rel) - emb_tail, p=2,dim=1)\n",
    "        return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hparam={'embedding_dim':25,\n",
    "       'num_entities':kg.num_entities,\n",
    "       'num_relations':kg.num_relations,\n",
    "       'gamma':1.0, # margin for loss\n",
    "       'lr':.01,# learning rate for optimizer\n",
    "       'batch_size':256,\n",
    "       'num_epochs':100\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetTriple(data=kg.train_idx, **hparam)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=hparam['batch_size'], num_workers=4, shuffle=True,drop_last=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "\n",
    "model = TransE(**hparam)\n",
    "gamma = nn.Parameter( torch.Tensor([ hparam['gamma'] ]),requires_grad=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparam['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.th epoch sum of loss: 3351.2478790283203\n",
      "20.th epoch sum of loss: 2201.5757446289062\n",
      "30.th epoch sum of loss: 1927.5072174072266\n",
      "40.th epoch sum of loss: 1828.472312927246\n",
      "50.th epoch sum of loss: 1835.4241943359375\n",
      "60.th epoch sum of loss: 1812.6691436767578\n",
      "70.th epoch sum of loss: 1829.0188751220703\n",
      "80.th epoch sum of loss: 1798.6147384643555\n",
      "90.th epoch sum of loss: 1772.907299041748\n"
     ]
    }
   ],
   "source": [
    "for e in range(1,hparam['num_epochs']):\n",
    "    epoch_loss=.0\n",
    "    for h, r, t, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute Distance based on translation,i.e. h + r \\approx t provided that h,r,t \\in G.\n",
    "        distance=model.forward(h,r,t)    \n",
    "        \n",
    "        pos_distance=distance[labels == 1]\n",
    "        neg_distance=distance[labels == -1]\n",
    "\n",
    "        loss= (F.relu(gamma + pos_distance - neg_distance)).sum()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if e%10==0:\n",
    "        print(f'{e}.th epoch sum of loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The filtered Link Prediction Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = dict()\n",
    "reciprocal_ranks = []\n",
    "\n",
    "for test_triple_idx in kg.test_idx:\n",
    "    s_idx,p_idx,o_idx= test_triple_idx\n",
    "\n",
    "    all_entities = torch.arange(0, dataset.num_entities).long()\n",
    "    all_entities = all_entities.reshape(len(all_entities), )\n",
    "\n",
    "    # 2. Compute tail distances \\forall x \\in Entities: TransE(s,p,x)\n",
    "    predictions_tails = model.forward(e1_idx=torch.tensor(s_idx).repeat(dataset.num_entities, ),\n",
    "                                              rel_idx=torch.tensor(p_idx).repeat(dataset.num_entities, ),\n",
    "                                              e2_idx=all_entities)\n",
    "    \n",
    "    # 3. Compute head distances \\forall x \\in Entities: TransE(x,p,o)\n",
    "    predictions_heads = model.forward(e1_idx=all_entities,\n",
    "                                              rel_idx=torch.tensor(p_idx).repeat(dataset.num_entities, ),\n",
    "                                              e2_idx=torch.tensor(o_idx).repeat(dataset.num_entities, ))\n",
    "    \n",
    "    # 3. Computed filtered ranks for missing head and tail entities\n",
    "    # 3.1. Filtered ranks for tail entities\n",
    "    filt_tails = kg.sp_vocab[(s_idx, p_idx)]\n",
    "    target_value = predictions_tails[o_idx].item()\n",
    "    predictions_tails[filt_tails] = +np.Inf\n",
    "    predictions_tails[o_idx] = target_value    \n",
    "    _, sort_idxs = torch.sort(predictions_tails, descending=False)\n",
    "    sort_idxs = sort_idxs.cpu().numpy()\n",
    "    filt_tail_entity_rank = np.where(sort_idxs == o_idx)[0][0]\n",
    "\n",
    "\n",
    "    \n",
    "    # 3.2. Filtered ranks for head entities\n",
    "    filt_heads = kg.po_vocab[(p_idx, o_idx)]\n",
    "    target_value = predictions_heads[s_idx].item()\n",
    "    predictions_heads[filt_heads] = +np.Inf\n",
    "    predictions_heads[s_idx] = target_value\n",
    "    _, sort_idxs = torch.sort(predictions_heads, descending=False)\n",
    "    sort_idxs = sort_idxs.cpu().numpy()\n",
    "    filt_head_entity_rank = np.where(sort_idxs == s_idx)[0][0]\n",
    "\n",
    "    # 4. Add 1 to ranks as numpy array first item has the index of 0.\n",
    "    filt_head_entity_rank += 1\n",
    "    filt_tail_entity_rank += 1\n",
    "    \n",
    "    # 5. Store reciprocal ranks.\n",
    "    reciprocal_ranks.append(1.0 / filt_head_entity_rank + (1.0 / filt_tail_entity_rank))\n",
    "\n",
    "    # 4. Compute Hit@N\n",
    "    for hits_level in range(1, 11):\n",
    "        I = 1 if filt_head_entity_rank <= hits_level else 0\n",
    "        I += 1 if filt_tail_entity_rank <= hits_level else 0\n",
    "        hits.setdefault(hits_level, []).append(I)\n",
    "        \n",
    "    mean_reciprocal_rank = sum(reciprocal_ranks) / (float(len(kg.test_idx) * 2))\n",
    "    hit_1 = sum(hits[1]) / (float(len(kg.test_idx) * 2))\n",
    "    hit_3 = sum(hits[3]) / (float(len(kg.test_idx) * 2))\n",
    "    hit_10 = sum(hits[10]) / (float(len(kg.test_idx) * 2))\n",
    "    \n",
    "    results = {'H@1': hit_1, 'H@3': hit_3, 'H@10': hit_10,'MRR': mean_reciprocal_rank}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H@1': 0.4296520423600605,\n",
       " 'H@3': 0.6565809379727685,\n",
       " 'H@10': 0.8381240544629349,\n",
       " 'MRR': 0.5741972594591919}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conex)",
   "language": "python",
   "name": "conex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
