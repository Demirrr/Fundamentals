{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b864ea44",
   "metadata": {},
   "source": [
    "# Label Smoothing\n",
    "\n",
    "\n",
    "\n",
    "## $$ p_k = \\frac{e^{x^T w_k} }{ \\sum_l ^L e^{x^T w_l}}$$\n",
    "\n",
    "where $p_k$ is the likelihood the model assigns to the $k$-th class, $w_k$ represents the weights and bias of the last layer, x is the input vector of the last layer.\n",
    "\n",
    "\n",
    "We minimize the expected value of the cross-entropy between hard targets $y_k$ and $p_k$ as in\n",
    "\n",
    "\n",
    "## $$H(\\mathbf{y},\\mathbf{p}) = \\sum_k ^K -y_k log(p_k),$$\n",
    "\n",
    "where $y_k$ is 1 of the correct class and 0 for the rest.\n",
    "\n",
    "\n",
    "\n",
    "Targets via label smoothing\n",
    "\n",
    "\n",
    "## $$ y_k = y_k (1- \\alpha) + \\frac{\\alpha}{K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f94253",
   "metadata": {},
   "source": [
    "# Multi-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce6f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0e6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLossCanonical(nn.Module):\n",
    "    def __init__(self, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLossCanonical, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.dim = dim\n",
    "    def forward(self, pred, target):\n",
    "        # Log softmax is used for numerical stability\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "            true_dist += self.smoothing / pred.size(self.dim)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44bbde",
   "metadata": {},
   "source": [
    "# Label Relaxation\n",
    "\n",
    ": TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91000771",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LabelRelaxationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.1, dim=-1, logits_provided=True, one_hot_encode_trgts=True, num_classes=-1):\n",
    "        super(LabelRelaxationLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "\n",
    "        # Greater zero threshold\n",
    "        self.gz_threshold = 0.1\n",
    "\n",
    "        self.logits_provided = logits_provided\n",
    "        self.one_hot_encode_trgts = one_hot_encode_trgts\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        if self.logits_provided:\n",
    "            pred = pred.softmax(dim=self.dim)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        # Apply one-hot encoding to targets\n",
    "        if self.one_hot_encode_trgts:\n",
    "            target = F.one_hot(target, num_classes=self.num_classes)\n",
    "\n",
    "        sum_y_hat_prime = torch.sum((torch.ones_like(target) - target) * pred, dim=-1)\n",
    "        pred_hat = self.alpha * pred / torch.unsqueeze(sum_y_hat_prime, dim=-1)\n",
    "        target_credal = torch.where(target > self.gz_threshold, torch.ones_like(target) - self.alpha, pred_hat)\n",
    "        divergence = nn.functional.kl_div(pred.log(), target_credal, log_target=False)\n",
    "\n",
    "        pred = torch.sum(pred * target, dim=-1)\n",
    "\n",
    "        result = torch.where(torch.gt(pred, 1. - self.alpha), torch.zeros_like(divergence), divergence)\n",
    "        return torch.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f0b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f271551a",
   "metadata": {},
   "source": [
    "# Dummy Multi-label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dda4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, Y = make_classification()\n",
    "X\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abecc299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67, 20), (67,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cf04d",
   "metadata": {},
   "source": [
    "# Affine Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694fdc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model,X,y):\n",
    "    X = torch.FloatTensor(X)\n",
    "    with torch.no_grad():\n",
    "        preds=torch.sigmoid(model(X))\n",
    "        return accuracy_score(torch.argmax(preds,axis=1).numpy(), y)\n",
    "def train(X_train, X_test, y_train, y_test,label_smoothing=0.0):\n",
    "    X_train_torch = torch.FloatTensor(X_train)\n",
    "    y_train_torch = torch.tensor(y_train,dtype=torch.long)\n",
    "    model = nn.Linear(20, 2) \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        output = torch.sigmoid(model(X_train_torch))\n",
    "        loss = criterion(output, y_train_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 ==0:\n",
    "            print('Loss: {:.3f}'.format(loss.item()))\n",
    "    print(f'Train Accuracy:{eval_model(model,X_test,y_test):.3f}')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed7822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.714\n",
      "Loss: 0.478\n",
      "Loss: 0.436\n",
      "Loss: 0.415\n",
      "Loss: 0.402\n",
      "Train Accuracy:0.667\n"
     ]
    }
   ],
   "source": [
    "train(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0850addc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.708\n",
      "Loss: 0.505\n",
      "Loss: 0.471\n",
      "Loss: 0.454\n",
      "Loss: 0.443\n",
      "Train Accuracy:0.667\n"
     ]
    }
   ],
   "source": [
    "train(X_train, X_test, y_train, y_test,label_smoothing=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a6c9acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.696\n",
      "Loss: 0.539\n",
      "Loss: 0.508\n",
      "Loss: 0.493\n",
      "Loss: 0.484\n",
      "Train Accuracy:0.606\n"
     ]
    }
   ],
   "source": [
    "train(X_train, X_test, y_train, y_test,label_smoothing=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93572797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.716\n",
      "Loss: 0.631\n",
      "Loss: 0.612\n",
      "Loss: 0.603\n",
      "Loss: 0.599\n",
      "Train Accuracy:0.667\n"
     ]
    }
   ],
   "source": [
    "train(X_train, X_test, y_train, y_test,label_smoothing=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46df8d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.355\n",
      "Loss: 0.267\n",
      "Loss: 0.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demir/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/nn/functional.py:2747: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.224\n",
      "Loss: 0.216\n",
      "Train Accuracy:0.667\n"
     ]
    }
   ],
   "source": [
    "def train_lr(X_train, X_test, y_train, y_test,alpha=0.0):\n",
    "    X_train_torch = torch.FloatTensor(X_train)\n",
    "    y_train_torch = torch.tensor(y_train,dtype=torch.long)\n",
    "    model = nn.Linear(20, 2) \n",
    "    criterion = LabelRelaxationLoss(alpha=alpha)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        output = torch.sigmoid(model(X_train_torch))\n",
    "        loss = criterion(output, y_train_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 ==0:\n",
    "            print('Loss: {:.3f}'.format(loss.item()))\n",
    "    print(f'Train Accuracy:{eval_model(model,X_test,y_test):.3f}')\n",
    "    \n",
    "    \n",
    "train_lr(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88bd12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.195\n",
      "Loss: 0.123\n",
      "Loss: 0.103\n",
      "Loss: 0.095\n",
      "Loss: 0.089\n",
      "Train Accuracy:0.606\n"
     ]
    }
   ],
   "source": [
    "train_lr(X_train, X_test, y_train, y_test,alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd23ef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.101\n",
      "Loss: 0.064\n",
      "Loss: 0.050\n",
      "Loss: 0.044\n",
      "Loss: 0.040\n",
      "Train Accuracy:0.576\n"
     ]
    }
   ],
   "source": [
    "train_lr(X_train, X_test, y_train, y_test,alpha=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e34acd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.001\n",
      "Loss: 0.001\n",
      "Loss: 0.001\n",
      "Loss: 0.001\n",
      "Loss: 0.001\n",
      "Train Accuracy:0.545\n"
     ]
    }
   ],
   "source": [
    "train_lr(X_train, X_test, y_train, y_test,alpha=.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pDL",
   "language": "python",
   "name": "pdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
