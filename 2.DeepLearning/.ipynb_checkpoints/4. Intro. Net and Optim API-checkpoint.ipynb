{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Intro. Net and Optim API\n",
    "\n",
    "\n",
    "In this tutorial, we implement neural network and optimization in object oriented fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(ABC):\n",
    "    def __init__(self):\n",
    "        self.num_param=0           \n",
    "        self.weight, self.bias=None, None\n",
    "        self.dweight, self.dbias=None, None\n",
    "    @abstractmethod\n",
    "    def forward(self, *args, **kwargs):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def backward(self, *args, **kwargs):\n",
    "        pass\n",
    "    @property\n",
    "    def param_size(self):\n",
    "        if self.weight is None and self.bias is None:\n",
    "            return 0\n",
    "        return self.weight.size+self.bias.size   \n",
    "    def dim_checker(self,a,b):\n",
    "        return a.shape==b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(Gate):\n",
    "    def __init__(self,in_channels=1, out_channels=1,kernel_size=(2, 2), stride=1, padding=0):        \n",
    "        super().__init__()\n",
    "        self.kernel_h,self.kernel_w=kernel_size\n",
    "        self.weight=np.random.randn(out_channels,\n",
    "                               in_channels,\n",
    "                               self.kernel_h,\n",
    "                               self.kernel_w) /np.sqrt(in_channels/2)\n",
    "        self.bias=np.zeros(out_channels)    \n",
    "        self.stride=stride\n",
    "        self.padding=padding\n",
    "        self.cache=dict()\n",
    "\n",
    "    def set_params(self,weights,bias=None):\n",
    "        self.weight,self.bias=weights, bias\n",
    "        n,d,self.kernel_h,self.kernel_w=self.weight.shape        \n",
    "\n",
    "    def compute_dim(self,X):\n",
    "        # parameter check\n",
    "        xN, xD, xH, xW = X.shape\n",
    "        wN, wD, wH, wW = self.weight.shape\n",
    "        assert wH == wW\n",
    "        assert (xH - wH) % self.stride == 0\n",
    "        assert (xW - wW) % self.stride == 0\n",
    "        self.cache['X']=X\n",
    "        \n",
    "        zH, zW = (xH - wH) // self.stride + 1, (xW - wW) // self.stride + 1\n",
    "        zD,zN = wN,xN\n",
    "        return np.zeros((zN, zD, zH, zW))\n",
    "    \n",
    "    def get_region(self,hight,width):\n",
    "        h1=hight*self.stride\n",
    "        h2=h1+self.kernel_h\n",
    "        w1=width*self.stride\n",
    "        w2=w1+self.kernel_w\n",
    "        return h1,h2,w1,w2\n",
    "    \n",
    "    def convolve_forward_step(self,X_n):\n",
    "        xD, xH, xW = X_n.shape\n",
    "        hZ=int((xH-self.kernel_h)/self.stride+1)\n",
    "        wZ=int((xW-self.kernel_w)/self.stride+1)\n",
    "        Z = np.zeros((len(self.weight),hZ, wZ))\n",
    "        \n",
    "        for d in range(len(Z)):\n",
    "            for i in range(hZ):\n",
    "                for j in range(wZ):\n",
    "                    h1,h2,w1,w2=self.get_region(i,j)\n",
    "                    x_loc = X_n[:, \n",
    "                              h1: h2,\n",
    "                              w1: w2]\n",
    "                    Z[d,i,j]=np.sum(x_loc*self.weight[d])+ self.bias[d]\n",
    "        return Z\n",
    "    \n",
    "    def forward(self,X):\n",
    "        Z=self.compute_dim(X)\n",
    "        for n in range(len(Z)):\n",
    "            Z[n,:,:,:]=self.convolve_forward_step(X[n])\n",
    "        self.cache['Z']=Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,dZ):        \n",
    "        assert self.dim_checker(dZ,self.cache['Z'])\n",
    "        \n",
    "        dX, self.dweight, self.dbias=np.zeros(self.cache['X'].shape), np.zeros(self.weight.shape),np.zeros(self.bias.shape)\n",
    "        (N, depth, hight, width) = dZ.shape\n",
    "         \n",
    "        for n in range(N):\n",
    "            for h in range(hight):        \n",
    "                for w in range(width):      \n",
    "                    for d in range(depth): # correcponds to d.th kernel\n",
    "                        h1,h2,w1,w2=self.get_region(h,w)\n",
    "                        dX[n,:,h1:h2,w1:w2]+= self.weight[d,:,:,:] * dZ[n, d, h, w]\n",
    "                        self.dweight[d,:,:,:] += self.cache['X'][n, :, h1:h2, w1:w2] * dZ[n, d, h, w]            \n",
    "                        self.dbias[d] +=dZ[n, d, h, w]\n",
    "                    \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(Gate):\n",
    "    def __init__(self,out_shape=None,flatten=False):\n",
    "        super().__init__()\n",
    "        self.flatten=flatten\n",
    "        self.out_shape=out_shape     \n",
    "        self.cache=dict()\n",
    "    def forward(self, X):\n",
    "        self.cache['X']=X\n",
    "        if self.flatten==True:\n",
    "            self.out_shape=(len(X), X.size//len(X))\n",
    "        Z= np.reshape(X,self.out_shape)       \n",
    "        return Z\n",
    "    def backward(self, dL_dZ):\n",
    "        dX= dL_dZ.reshape(self.cache['X'].shape)        \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Gate):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X,axis=1):\n",
    "        assert len(X.shape)==2\n",
    "        X-=np.max(X,axis=axis,keepdims=True)\n",
    "        exp_scores=np.exp(X)\n",
    "        Z=exp_scores/np.sum(exp_scores,axis=axis,keepdims=True)\n",
    "        return Z\n",
    "    def backward(self, dL_dZ):\n",
    "        return dL_dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Gate):\n",
    "    def __init__(self,in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight=np.random.rand(in_features,out_features)/in_features\n",
    "        self.bias=np.random.rand(out_features)\n",
    "        self.cache=dict()\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : shape=(N,in_features)\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        Z:shape(N,out_features)\n",
    "        \"\"\"\n",
    "        self.cache['X']=X\n",
    "        Z=X.dot(self.weight)+self.bias\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : shape=(N,out_features)\n",
    "        \n",
    "        ----------\n",
    "        dX: dZ (N,out_features) * weight (out_features,in_features) => (N,in_features)\n",
    "\n",
    "        dW: X.T (in_features,N,) * dZ (N,out_features) => (in_features,out_features)\n",
    "        \n",
    "        db: dZ= (out_features,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dX : shape (N,in_features)\n",
    "        \"\"\"\n",
    "        dX= dZ.dot(self.weight.T) # dL/dZ * dZ/dX\n",
    "        self.dweight= self.cache['X'].T.dot(dZ)        # dZ/dX* dL/dZ\n",
    "        self.dbias=dZ.sum(axis=0)\n",
    "        try:\n",
    "            assert self.dweight.shape==self.weight.shape\n",
    "            assert self.dbias.shape==self.bias.shape\n",
    "        except AssertionError as a:\n",
    "            print(self.weight.shape)\n",
    "            print(self.dweight.shape)\n",
    "\n",
    "            print(self.bias.shape)\n",
    "            print(self.dbias.shape)\n",
    "            \n",
    "            raise\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n",
      "(1797, 1, 8, 8) (1797,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Digit Recognition\n",
    "digits = datasets.load_digits()\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)\n",
    "    X=digits.images\n",
    "y=digits.target\n",
    "#Descritive stats; distribution of labels.\n",
    "#print(np.histogram(y, density=True))\n",
    "print(X.shape)\n",
    "\n",
    "X_images=np.expand_dims(X, axis=1)\n",
    "print(X_images.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class Optim(ABC):\n",
    "    def __init__(self,learning_rate=.001):\n",
    "        self.learning_rate=learning_rate\n",
    "    @abstractmethod\n",
    "    def update(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "class SGD(Optim):\n",
    "    def __init__(self,learning_rate=.001):\n",
    "        super().__init__()\n",
    "        self.learning_rate=learning_rate\n",
    "        \n",
    "    def update(self,gates):\n",
    "        for g in gates:\n",
    "            if g.param_size>0:\n",
    "                try:\n",
    "                    g.weight += - self.learning_rate * g.dweight\n",
    "                    g.bias   += - self.learning_rate * g.dbias\n",
    "                except TypeError as e:\n",
    "                    print(g)\n",
    "                    print(g.weight)\n",
    "                    print(g.bias)\n",
    "                    print(g.dweight)\n",
    "                    print(g.dbias)\n",
    "                    raise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self,optimizer,verbose=0):\n",
    "        self.gates = []\n",
    "        self.optimizer=optimizer\n",
    "        self.losses=[]\n",
    "        self.verbose=verbose\n",
    "        self.total_param=0\n",
    "    \n",
    "    def describe(self):\n",
    "        for th, g in enumerate(self.gates):\n",
    "            print(f'[{th+1}. layer] => {g}')  \n",
    "        print(f'Total param.:{self.total_param}')\n",
    "\n",
    "    def add(self, gate):\n",
    "        self.total_param+=gate.param_size\n",
    "        self.gates.append(gate)\n",
    "        \n",
    "    def add_from_iter(self, l):\n",
    "        for gate in l:\n",
    "            self.add(gate)\n",
    "            if gate.num_param>0:\n",
    "                self.param_gates.add(gate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if len(inputs)==1:\n",
    "            inputs=np.expand_dims(inputs, axis=0)\n",
    "        for g in self.gates:\n",
    "            inputs = g.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, dL):\n",
    "        for g in reversed(self.gates):\n",
    "            dL = g.backward(dL)\n",
    "    \n",
    "    def iterate_minibatches(self, inputs, targets, batchsize, shuffle_per_epoch=False):\n",
    "        assert inputs.shape[0] == targets.shape[0]\n",
    "        if shuffle_per_epoch:\n",
    "            indices = np.arange(inputs.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "        for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "            if shuffle_per_epoch:\n",
    "                excerpt = indices[start_idx:start_idx + batchsize]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "    def train(self,X,y,epoch=100,print_out_per_epoch=1,batchsize=256,shuffle_per_epoch=True):\n",
    "        \n",
    "        print('Training starts.')\n",
    "        for i in range(1, epoch+1):\n",
    "            loss,acc=0,0\n",
    "            for X_minibatch, y_minibatch in self.iterate_minibatches(X,y,batchsize,shuffle_per_epoch):\n",
    "\n",
    "                Z=self.forward(X_minibatch)\n",
    "                pred_prob_true_class=Z[range(len(Z)),y_minibatch]\n",
    "                loss += (-np.log(pred_prob_true_class)).sum()            \n",
    "                acc  += (np.argmax(Z,axis=1)==y_minibatch).sum()\n",
    "\n",
    "                # Compute Gradients of cross entropy loss w.r.t. predictions.\n",
    "                dL_dZ = Z\n",
    "                dL_dZ[range(len(Z)),y_minibatch] -= 1 \n",
    "                dL_dZ/=len(dL_dZ)\n",
    "\n",
    "                self.backward(dL_dZ)\n",
    "                self.optimizer.update(self.gates)    \n",
    "\n",
    "            avg_acc=acc/len(X)\n",
    "            avg_loss=loss/len(X)\n",
    "            \n",
    "            if i%print_out_per_epoch==0:\n",
    "                    print(f'[Epoch:{i}]-[Avg.Loss:{avg_loss:.3f}]-[Avg.Acc:{avg_acc:.3f}]')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts.\n",
      "[Epoch:1]-[Avg.Loss:3.563]-[Avg.Acc:0.102]\n",
      "[Epoch:2]-[Avg.Loss:11.021]-[Avg.Acc:0.519]\n",
      "[Epoch:3]-[Avg.Loss:15.959]-[Avg.Acc:0.191]\n",
      "[Epoch:4]-[Avg.Loss:22.871]-[Avg.Acc:0.368]\n",
      "[Epoch:5]-[Avg.Loss:27.121]-[Avg.Acc:0.202]\n",
      "[Epoch:6]-[Avg.Loss:28.514]-[Avg.Acc:0.350]\n",
      "[Epoch:7]-[Avg.Loss:25.606]-[Avg.Acc:0.275]\n",
      "[Epoch:8]-[Avg.Loss:23.514]-[Avg.Acc:0.264]\n",
      "[Epoch:9]-[Avg.Loss:23.330]-[Avg.Acc:0.348]\n",
      "[Epoch:10]-[Avg.Loss:22.057]-[Avg.Acc:0.441]\n"
     ]
    }
   ],
   "source": [
    "net=Net(optimizer=SGD())\n",
    "net.add_from_iter([Conv(in_channels=1,out_channels=2,kernel_size=(2,2),stride=1),\n",
    "                   Reshape(flatten=True),\n",
    "                   Linear(in_features=98,out_features=10),\n",
    "                   Softmax()])\n",
    "net.train(X_images,y,epoch=10,batchsize=len(X_images),shuffle_per_epoch=False) # Full, without shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts.\n",
      "[Epoch:1]-[Avg.Loss:2.172]-[Avg.Acc:0.277]\n",
      "[Epoch:2]-[Avg.Loss:1.262]-[Avg.Acc:0.650]\n"
     ]
    }
   ],
   "source": [
    "net=Net(optimizer=SGD())\n",
    "net.add_from_iter([Conv(in_channels=1,out_channels=2,kernel_size=(2,2),stride=1),\n",
    "                   Reshape(flatten=True),\n",
    "                   Linear(in_features=98,out_features=10),\n",
    "                   Softmax()])\n",
    "net.train(X_images,y,epoch=10,batchsize=len(X_images)//4,shuffle_per_epoch=True) # minibatch, with shuffle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
