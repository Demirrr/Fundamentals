{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib; matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from gym.wrappers.monitor import load_results\n",
    "from copy import deepcopy\n",
    "\n",
    "#REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        # read parameters from parents, and children can override the values.\n",
    "        parents = []\n",
    "        queue = [self.__class__]\n",
    "        while queue:\n",
    "            parent = queue.pop()\n",
    "            if issubclass(parent, Config) and parent is not Config:\n",
    "                parents.append(parent)\n",
    "                for p in reversed(parent.__bases__):\n",
    "                    queue.append(p)\n",
    "\n",
    "        params = {}\n",
    "        for cfg in reversed(parents):\n",
    "            params.update(cfg.__dict__)\n",
    "\n",
    "        # Set all instance variable based on kwargs and default class variables\n",
    "        for key, value in params.items():\n",
    "            if key.startswith('__'):\n",
    "                continue\n",
    "\n",
    "            if key in kwargs:\n",
    "                # override default with provided parameter\n",
    "                value = kwargs[key]\n",
    "            else:\n",
    "                # Need to make copies of class variables so that they aren't changed by instances\n",
    "                value = deepcopy(value)\n",
    "\n",
    "            self.__dict__[key] = value\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if name not in self.__dict__:\n",
    "            raise AttributeError(f\"{self.__class__.__name__} does not have attribute {name}\")\n",
    "        self.__dict__[name] = value\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # Raise error on assignment of missing variable\n",
    "        if name not in self.__dict__:\n",
    "            raise AttributeError(f\"{self.__class__.__name__} does not have attribute {name}\")\n",
    "        return self.__dict__[name]\n",
    "\n",
    "    def as_dict(self):\n",
    "        return deepcopy(self.__dict__)\n",
    "\n",
    "    def copy(self):\n",
    "        return self.__class__(**self.as_dict())\n",
    "\n",
    "    def get(self, name, default):\n",
    "        return self.as_dict().get(name, default)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return super().__repr__() + \"\\n\" + self.dumps()\n",
    "\n",
    "\n",
    "def plot_learning_curve(filename, value_dict, xlabel='step'):\n",
    "    # Plot step vs the mean(last 50 episodes' rewards)\n",
    "    fig = plt.figure(figsize=(12, 4 * len(value_dict)))\n",
    "\n",
    "    for i, (key, values) in enumerate(value_dict.items()):\n",
    "        ax = fig.add_subplot(len(value_dict), 1, i + 1)\n",
    "        ax.plot(range(len(values)), values)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid('k--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.join(REPO_ROOT, 'figs'), exist_ok=True)\n",
    "    plt.savefig(os.path.join(REPO_ROOT, 'figs', filename))\n",
    "\n",
    "\n",
    "def plot_from_monitor_results(monitor_dir, window=10):\n",
    "    assert os.path.exists(monitor_dir)\n",
    "    if monitor_dir.endswith('/'):\n",
    "        monitor_dir = monitor_dir[:-1]\n",
    "\n",
    "    data = load_results(monitor_dir)\n",
    "    n_episodes = len(data['episode_lengths'])\n",
    "    assert n_episodes > 0\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), tight_layout=True, sharex=True)\n",
    "\n",
    "    ax1.plot(range(n_episodes), pd.rolling_mean(np.array(data['episode_lengths']), window))\n",
    "    ax1.set_xlabel('episode')\n",
    "    ax1.set_ylabel('episode length')\n",
    "    ax1.grid('k--', alpha=0.6)\n",
    "\n",
    "    ax2.plot(range(n_episodes), pd.rolling_mean(np.array(data['episode_rewards']), window))\n",
    "    ax2.set_xlabel('episode')\n",
    "    ax2.set_ylabel('episode reward')\n",
    "    ax2.grid('k--', alpha=0.6)\n",
    "\n",
    "    os.makedirs(os.path.join(REPO_ROOT, 'figs'), exist_ok=True)\n",
    "    plt.savefig(os.path.join(REPO_ROOT, 'figs', os.path.basename(monitor_dir) + '-monitor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gym.spaces import Box, Discrete\n",
    "from gym.utils import colorize\n",
    "\n",
    "#from playground.utils.misc import Config\n",
    "#from playground.utils.misc import REPO_ROOT\n",
    "\n",
    "\n",
    "class TrainConfig(Config):\n",
    "    lr = 0.001\n",
    "    n_steps = 10000\n",
    "    warmup_steps = 5000\n",
    "    batch_size = 64\n",
    "    log_every_step = 1000\n",
    "\n",
    "    # give an extra bonus if done; only needed for certain tasks.\n",
    "    done_reward = None\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, env, name, training=True, gamma=0.99, deterministic=False):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.training = training\n",
    "        self.name = name\n",
    "\n",
    "        if deterministic:\n",
    "            np.random.seed(1)\n",
    "            tf.set_random_seed(1)\n",
    "\n",
    "    @property\n",
    "    def act_size(self):\n",
    "        # number of options of an action; this only makes sense for discrete actions.\n",
    "        if isinstance(self.env.action_space, Discrete):\n",
    "            return self.env.action_space.n\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def act_dim(self):\n",
    "        # dimension of an action; this only makes sense for continuous actions.\n",
    "        if isinstance(self.env.action_space, Box):\n",
    "            return list(self.env.action_space.shape)\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    @property\n",
    "    def state_dim(self):\n",
    "        # dimension of a state.\n",
    "        return list(self.env.observation_space.shape)\n",
    "\n",
    "    def obs_to_inputs(self, ob):\n",
    "        return ob.flatten()\n",
    "\n",
    "    def act(self, state, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def build(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, n_episodes):\n",
    "        reward_history = []\n",
    "        reward = 0.\n",
    "\n",
    "        for i in range(n_episodes):\n",
    "            ob = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                a = self.act(ob)\n",
    "                new_ob, r, done, _ = self.env.step(a)\n",
    "                self.env.render()\n",
    "                reward += r\n",
    "                ob = new_ob\n",
    "\n",
    "            reward_history.append(reward)\n",
    "            reward = 0.\n",
    "\n",
    "        print(\"Avg. reward over {} episodes: {:.4f}\".format(n_episodes, np.mean(reward_history)))\n",
    "\n",
    "\n",
    "class BaseModelMixin:\n",
    "    \"\"\"Abstract object representing an tensorflow model that can be easily saved/loaded.\n",
    "    Modified based on https://github.com/devsisters/DQN-tensorflow/blob/master/dqn/base.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, tf_sess_config=None):\n",
    "        self._saver = None\n",
    "        self._writer = None\n",
    "        self._model_name = model_name\n",
    "        self._sess = None\n",
    "\n",
    "        if tf_sess_config is None:\n",
    "            tf_sess_config = {\n",
    "                'allow_soft_placement': True,\n",
    "                'intra_op_parallelism_threads': 8,\n",
    "                'inter_op_parallelism_threads': 4,\n",
    "            }\n",
    "        self.tf_sess_config = tf_sess_config\n",
    "\n",
    "    def scope_vars(self, scope, only_trainable=True):\n",
    "        collection = tf.GraphKeys.TRAINABLE_VARIABLES if only_trainable else tf.GraphKeys.VARIABLES\n",
    "        variables = tf.get_collection(collection, scope=scope)\n",
    "        assert len(variables) > 0\n",
    "        print(f\"Variables in scope '{scope}':\")\n",
    "        for v in variables:\n",
    "            print(\"\\t\" + str(v))\n",
    "        return variables\n",
    "\n",
    "    def get_variable_values(self):\n",
    "        t_vars = tf.trainable_variables()\n",
    "        vals = self.sess.run(t_vars)\n",
    "        return {v.name: value for v, value in zip(t_vars, vals)}\n",
    "\n",
    "    def save_checkpoint(self, step=None):\n",
    "        print(colorize(\" [*] Saving checkpoints...\", \"green\"))\n",
    "        ckpt_file = os.path.join(self.checkpoint_dir, self.model_name)\n",
    "        self.saver.save(self.sess, ckpt_file, global_step=step)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print(colorize(\" [*] Loading checkpoints...\", \"green\"))\n",
    "        ckpt_path = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "        print(self.checkpoint_dir)\n",
    "        print(\"ckpt_path:\", ckpt_path)\n",
    "\n",
    "        if ckpt_path:\n",
    "            # self._saver = tf.train.import_meta_graph(ckpt_path + '.meta')\n",
    "            self.saver.restore(self.sess, ckpt_path)\n",
    "            print(colorize(\" [*] Load SUCCESS: %s\" % ckpt_path, \"green\"))\n",
    "            return True\n",
    "        else:\n",
    "            print(colorize(\" [!] Load FAILED: %s\" % self.checkpoint_dir, \"red\"))\n",
    "            return False\n",
    "\n",
    "    def _get_dir(self, dir_name):\n",
    "        path = os.path.join(REPO_ROOT, dir_name, self.model_name)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        return path\n",
    "\n",
    "    @property\n",
    "    def log_dir(self):\n",
    "        return self._get_dir('logs')\n",
    "\n",
    "    @property\n",
    "    def checkpoint_dir(self):\n",
    "        return self._get_dir('checkpoints')\n",
    "\n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return self._get_dir('models')\n",
    "\n",
    "    @property\n",
    "    def tb_dir(self):\n",
    "        # tensorboard\n",
    "        return self._get_dir('tb')\n",
    "\n",
    "    @property\n",
    "    def model_name(self):\n",
    "        assert self._model_name, \"Not a valid model name.\"\n",
    "        return self._model_name\n",
    "\n",
    "    @property\n",
    "    def saver(self):\n",
    "        if self._saver is None:\n",
    "            self._saver = tf.train.Saver(max_to_keep=5)\n",
    "        return self._saver\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        if self._writer is None:\n",
    "            self._writer = tf.summary.FileWriter(self.tb_dir, self.sess.graph)\n",
    "        return self._writer\n",
    "\n",
    "    @property\n",
    "    def sess(self):\n",
    "        if self._sess is None:\n",
    "            config = tf.ConfigProto(**self.tf_sess_config)\n",
    "            self._sess = tf.Session(config=config)\n",
    "\n",
    "        return self._sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction by the primary Q network for the actual actions.\n",
    "action_one_hot = tf.one_hot(actions, env.action_space.n, 1.0, 0.0, name='action_one_hot')\n",
    "pred = tf.reduce_sum(q * action_one_hot, reduction_indices=-1, name='q_acted')\n",
    "\n",
    "# The optimization target defined by the Bellman equation and the target network.\n",
    "max_q_next_by_target = tf.reduce_max(q_target, axis=-1)\n",
    "y = rewards + (1. - done_flags) * gamma * max_q_next_by_target\n",
    "\n",
    "# The loss measures the mean squared error between prediction and target.\n",
    "loss = tf.reduce_mean(tf.square(pred - tf.stop_gradient(y)), name=\"loss_mse_train\")\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss, name=\"adam_optim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-04ba7e1bb856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get all the variables in the Q target network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mq_target_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_VARIABLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Q_target\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_vars\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_target_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_target_q_net_hard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get all the variables in the Q primary network.\n",
    "q_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_primary\")\n",
    "# Get all the variables in the Q target network.\n",
    "q_target_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_target\")\n",
    "assert len(q_vars) == len(q_target_vars)\n",
    "\n",
    "def update_target_q_net_hard():\n",
    "    # Hard update\n",
    "    sess.run([v_t.assign(v) for v_t, v in zip(q_target_vars, q_vars)])\n",
    "\n",
    "def update_target_q_net_soft(tau=0.05):\n",
    "    # Soft update: polyak averaging.\n",
    "    sess.run([v_t.assign(v_t * (1. - tau) + v * tau) for v_t, v in zip(q_target_vars, q_vars)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (temp)",
   "language": "python",
   "name": "temp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
