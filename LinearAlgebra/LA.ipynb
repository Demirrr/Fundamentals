{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f87d04e",
   "metadata": {},
   "source": [
    "# Linear Algebra \n",
    "\n",
    "\n",
    "Inspred by \n",
    "\n",
    "1- Linear Review and Reference Zico Kolter (updated by Chuong Do)  \n",
    "2- Linear Algebra Review by Jing Xiang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f87fc6",
   "metadata": {},
   "source": [
    "## 1. Linear System of Equations\n",
    "$$ \n",
    "\\begin{array}\n",
    "4x_1 - 5 x_2 = -13 \\\\\n",
    "-2x_1 + 3x_2 = 9.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$ Ax = b$$\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "4 & - 5 \\\\\n",
    "-2 & 3\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} = \\; b=\\begin{bmatrix}\n",
    "-13 \\\\\n",
    "9\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where \n",
    "1. $ A \\in \\mathbb{R}^{m \\times n} $\n",
    "2. $ x \\in \\mathbb{R}^{n} $ a column vector (n rows and 1 column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c732e7ac",
   "metadata": {},
   "source": [
    "## 2. Matrix Multiplication\n",
    "\n",
    "#### Vector-Vector Multiplication $$ \\text{Let } x,y \\in \\mathbb R ^n $$\n",
    "#### $$ \n",
    "\\begin{bmatrix}\n",
    "x_{1} & x_{1} & x_{1}\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "y_{3}\n",
    "\\end{bmatrix}\n",
    "= \\sum_{i=1} ^n x_{i} \\, y_{i} = \\mid\\mid x \\mid\\mid \\mid\\mid y \\mid\\mid \\text{cost} \\Theta,\n",
    "$$\n",
    "where the operands are column vectors. $\\Theta$ is the angle between two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dff960",
   "metadata": {},
   "source": [
    "\n",
    "## Matrix-Vector Multiplication\n",
    "\n",
    "### $$ \\text{Let } A \\in \\mathbb R ^{n \\times l} \\, , b \\in \\mathbb R ^{l} \\\\ Ab = \\Big( \\sum_{k=1} ^l a_{i,k} \\, b_{k} \\Big)^{n}$$\n",
    "\n",
    "#### $$ \n",
    "\\begin{bmatrix}\n",
    "\\color{red}{a_{1,1}} & \\color{red}{a_{1,2}} & \\color{red}{a_{1,3}}\\\\\n",
    "a_{2,1} & a_{2,2} & a_{2,3}\\\\\n",
    "a_{3,1} & a_{3,2} & a_{3,3}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\color{green}{b_1} \\\\\n",
    "\\color{green}{b_2} \\\\\n",
    "\\color{green}{b_3}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\color{blue}{c_{1,1}}\\\\\n",
    "c_{2,1}\\\\\n",
    "c_{3,1}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "\\color{blue}{a_{1,1} b_1} + \\color{blue}{a_{1,2} b_2} + \\color{blue}{a_{1,3} b_3} \\\\\n",
    "a_{2,1} b_1 + a_{2,2} b_2 + a_{2,3} b_3 \\\\\n",
    "a_{3,1} b_1 + a_{3,2} b_2 + a_{3,3} b_3 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Matrix-Matrix Multiplication\n",
    "\n",
    "### $$ \\text{Let } A \\in \\mathbb R ^{n \\times l} \\, , B \\in \\mathbb R ^{l \\times m} \\\\ AB = \\Big( \\sum_{k=1} ^l a_{i,k} \\, b_{k,j} \\Big)^{n,m}$$\n",
    "#### $$ \n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} & a_{1,3}\\\\\n",
    "\\color{red}{a_{2,1}} & \\color{red}{a_{2,2}} & \\color{red}{a_{2,3}}\\\\\n",
    "a_{3,1} & a_{3,2} & a_{3,3}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\color{red}{b_{1,1}} & b_{1,2} & b_{1,3}\\\\\n",
    "\\color{red}{b_{2,1}} & b_{2,2} & b_{2,3}\\\\\n",
    "\\color{red}{b_{3,1}} & b_{3,2} & b_{3,3}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "c_{1,1} & c_{1,2} & c_{1,3}\\\\\n",
    "\\color{red}{c_{2,1}} & c_{2,2} & c_{2,3}\\\\\n",
    "c_{3,1} & c_{3,2} & c_{3,3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where \n",
    "#### $$ \\color{red}{c_{2,1}} = a_{2,1}b_{1,1} + a_{2,2}b_{2,1} + a_{2,3}b_{3,1}$$\n",
    "\n",
    "\n",
    "Matrix multiplication can be seen as a linear transformation on basis vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75639e52",
   "metadata": {},
   "source": [
    "## 3. Linear Dependence and Independence\n",
    "\n",
    "\n",
    "A set of vectors ${x_1, \\dots, x_n } \\subset \\mathbb R^n $ is said to be __linearly dependent__ if vectors can be represented as a linear combination of the remaining vectors. \n",
    "\n",
    "$$ x_n = \\sum_{i=1} ^{n-1} \\alpha_i x_i $$\n",
    "\n",
    "for scalar values $\\alpha_i,\\dots, \\alpha_{n-1}$. For instance\n",
    "\n",
    "$$\n",
    "x_1= \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{bmatrix} \\;\n",
    "x_2= \\begin{bmatrix}\n",
    "4 \\\\\n",
    "1 \\\\\n",
    "5 \\\\\n",
    "\\end{bmatrix} \\; \n",
    "x_3= \\begin{bmatrix}\n",
    "2 \\\\\n",
    "-3 \n",
    "-1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "are linearly dependend as $x_3 = -2x_1 + x_2$\n",
    "\n",
    "\n",
    "\n",
    "## 4. RANK\n",
    "\n",
    "__The column rank of a matrix $A \\in \\mathbb R^{m \\times n}$__ is the size of the largest subset of columns of A that constitute a linearly independent set.\n",
    "\n",
    "__The row  rank of a matrix A__ is the size of the largest subset of rows of A that constitute a linearly independent set.\n",
    "\n",
    "For any matrix $A \\in \\mathbb{R}^{m \\times n }$, $rank(A) \\leq min(m,n)$, then A is said to be __full rank__\n",
    "\n",
    "\n",
    "For $A \\in \\mathbb R^{m \\times n }$, $B \\in \\mathbb R^{n \\times p }$, $rank(AB) \\leq min(rank(A),rank(B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95372c",
   "metadata": {},
   "source": [
    "# The Inverse\n",
    "\n",
    "The inverse of a $ A \\in \\mathbb R^{n \\times n }$ is denoted $A^{-1}$ \n",
    "\n",
    "\n",
    "$$ A^{-1} A = I =  AA^{-1}$$\n",
    " \n",
    " \n",
    "### $(A^{-1})^{-1} = A$\n",
    "\n",
    "### $(AB)^{-1} = B^{-1} A^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a3fe0",
   "metadata": {},
   "source": [
    "# Orthogonal Matrices\n",
    "\n",
    "1. $ x,y \\in \\mathbb R^n $ are orthogonal if $x^T y= 0$. \n",
    "2. A vector $x \\in R^n$ is normalized if $||x||_2 =1$  \n",
    "3. A square matrix $U \\in \\mathbb R^{n \\times n}$ is orthogonal if all its columns are orthogonal to each other and are normalized.\n",
    "\n",
    "\n",
    "\n",
    "$$ U^T U = I UU^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100fb66",
   "metadata": {},
   "source": [
    "# Span of a set of vectors\n",
    "__The span of a set of vectors__ $\\{ x_1, \\dots, x_n \\}$ is the set of all vector that can be expressed as a linar combination of $\\{ x_1, \\dots, x_n \\}$\n",
    "\n",
    "$$ span( \\{ x_1, \\dots, x_n \\} ) = \\{ v : v = \\sum_{i=1} ^n \\alpha_i x_i , \\alpha_i \\in \\mathbb{R} \\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0355c",
   "metadata": {},
   "source": [
    "# Projection\n",
    "__The projection of a vector $y$ on to the span of $\\{ x_1, \\dots, x_n \\}$__ is\n",
    "$$ Proj( y; \\{ x_1, \\dots, x_n \\}) = argmin_{v \\in span( \\{ x_1, \\dots, x_n \\})} \\mid\\mid y - v \\mid\\mid_2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438cce9",
   "metadata": {},
   "source": [
    "## Range\n",
    "__The range of a matrix__ $A \\in \\mathbb R^{m \\times n} $ is the span of the columns of $A$, i.e.,\n",
    "\n",
    "$$ R(A) = { v \\in \\mathbb R^m : v = Ax, x\\in R^n}$$\n",
    "\n",
    "\n",
    "\n",
    "Let $ A \\in \\mathbb R , n < m, \\text{and A is full rank}$, the projection of a vector $y \\in \\mathbb R^m$ onto the range of A is given by\n",
    "\n",
    "\n",
    "#### $$ Proj(y;A) = argmin_{v \\in R(A)} \\mid\\mid v - y \\mid\\mid_2 $$\n",
    "\n",
    "#### $$ = A (A^T A )^{-1} A^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd4504",
   "metadata": {},
   "source": [
    "# Null Space\n",
    "\n",
    "The nullspace of a matrix $A \\in \\mathbb R^{m \\times n}$ is the set of all vectors\n",
    "\n",
    "$$ N(A) = \\{ x \\in \\mathbb R \\; : \\; Ax=0 \\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a9708",
   "metadata": {},
   "source": [
    "### Determinant\n",
    "\n",
    "$$ A \\in \\mathbb {n \\times n} $$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "--- a_1 ^T --- \\\\\n",
    "--- a_2 ^T --- \\\\\n",
    "\\dots \\\\\n",
    "--- a_n ^T --- \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Consider the set of points $S \\subset \\mathbb R^n $ formed by taking all possible linear combinations of the row vectors $a_1, \\dots, a_n \\in \\mathbb R^n $ of $A$, where the coefficients of the linear combinations are all between 0 and 1; that is\n",
    "\n",
    "\n",
    "\n",
    "$$ S = \\{ v \\in \\mathbb R^n \\; : \\; v = \\sum_{i=1} ^n \\alpha_i a_i \\text{ where } 0 \\leq \\alpha_i \\leq 1, i = 1, \\dots, n\\}$$\n",
    "\n",
    "The absolute value of the determinant of $A$ is a measure of the volume of the set $S^2$.\n",
    "\n",
    "+ $det(A) = det(A^T)$\n",
    "+ $det(AB) = det(A) det(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7497de2",
   "metadata": {},
   "source": [
    "# Eigenvalue and Eigenvectors\n",
    "\n",
    "\n",
    "\n",
    "#### Let $ A \\in \\mathbb R^{n \\times n}\\; \\lambda \\in \\mathbb C$, and $x \\in \\mathbb C \\not=0$ \n",
    "\n",
    "### $x \\text{ and } \\lambda $ are an eigen value and eigen vector if $$ Ax = \\lambda x \\\\ (\\lambda I - A)x=0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de2ccff",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "\n",
    "## Eigendecomposition:\n",
    "\n",
    "Write equations for all $n$ eigen values as $AX = XV$\n",
    "$$\n",
    "A \\begin{bmatrix}\n",
    "     |  & &| \\\\\n",
    "    x_1 & \\dots &x_n\\\\\n",
    "     |  & &|   \n",
    "  \\end{bmatrix} = \\begin{bmatrix}\n",
    "     |  & &| \\\\\n",
    "    x_1 & \\dots &x_n\\\\\n",
    "     |  & &|   \n",
    "  \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "     \\lambda_1  & \\dots& \\vdots\\\\\n",
    "     \\dots      & \\ddots& \\vdots \\\\\n",
    "     0  & \\dots &\\lambda_n   \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "\n",
    "##  $$ A = X V X^{-1}$$\n",
    "\n",
    "\n",
    "### Important properties\n",
    "\n",
    "1. tr $A = \\sum_i ^n \\lambda_i$\n",
    "2. Det $A = \\prod _i ^n \\lambda_i$\n",
    "\n",
    "\n",
    "## Cholesky Decomposition:\n",
    "?\n",
    "\n",
    "## Singular Value Decomposition\n",
    "\n",
    "\n",
    "Any $n \\times m$ matrix $A$ can be written as \n",
    "\n",
    "### $$ A = U \\Sigma V^T$$\n",
    "\n",
    "1. $U$ eigenvectors of $AA^T$ (n by n)\n",
    "2. $\\Sigma= \\sqrt{diag(eig(AA^T))}$ (n by m)\n",
    "3. $V$ eigenvectors of $A^T A$ (m by m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae7df1",
   "metadata": {},
   "source": [
    "## Matrix Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c154b",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "Let $f: \\mathbb R^{m \\times n} \\to \\mathbb R$. The gradient of $f$ w.r.t $A \\in \\mathbb{R}^{m\\times n}$ is the matrix of partial derivatives in a form of $m \\times n$ matrix :\n",
    "\n",
    "### $$\\nabla_A f(A) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f(A)}{\\partial A_{11}} & \\frac{\\partial f(A)}{\\partial A_{12}} \\dots \\frac{\\partial f(A)}{\\partial A_{1n}}\\\\\n",
    "\\vdots & \\dots \\\\\n",
    "\\frac{\\partial f(A)}{\\partial A_{m1}} &\\frac{\\partial f(A)}{\\partial A_{m2}} \\dots \\frac{\\partial f(A)}{\\partial A_{mn}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### $$ (\\nabla_A f(A))_{ij} = \\frac{\\partial f(A)}{\\partial A_{ij}}$$\n",
    "\n",
    "\n",
    "\n",
    "+ $\\nabla_x (f(x) + g(x)) = \\nabla_x f(x) + \\nabla_x g(x)$\n",
    "\n",
    "+ $\\forall t \\in \\mathbb R , \\nabla_x (t(f(x)) = t \\nabla_x f(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66216d68",
   "metadata": {},
   "source": [
    "### Hessian\n",
    "\n",
    "\n",
    "$f: \\mathbb R^{n} \\to \\mathbb R$. The gradient of $f$ w.r.t $A \\in \\mathbb{R}^{n}$ is the vector of partial derivatives is a $n$ vector:\n",
    "\n",
    "### $$\\nabla_A f(A) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f(x)}{\\partial x_{1}}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f(x)}{\\partial x_{n}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The Hessian of $f$ w.r.t. $x$ denoted as $\\nabla_x ^2 f(x)$ or H is the $n\\times n$ ,matrix of partial derivatives\n",
    "\n",
    "\n",
    "\n",
    "### $$\\nabla_A f(x) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f(x)}{\\partial^2 x_1 ^2} & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_2} \\dots \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_n }\\\\\n",
    "\\vdots & \\dots \\\\\n",
    "\\frac{\\partial^2 f(x)}{\\partial^2 x_n \\partial x_1 ^2} & \\frac{\\partial^2 f(x)}{\\partial x_n \\partial x_2} \\dots \\frac{\\partial^2 f(x)}{\\partial x_n ^2 }\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### $$(\\nabla_x ^2 f(x) )_{ij} = \\frac{\\partial^2 f(x)}{\\partial x_i \\partial x_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfdb49a",
   "metadata": {},
   "source": [
    "### Gradients with different sizes\n",
    "\n",
    "\n",
    "### $$ f: \\mathbb R^n \\to \\mathbb R \\; \\nabla f \\in \\mathbb R^n, \\nabla f_i = \\frac{\\partial f}{\\partial x_i} $$\n",
    "\n",
    "\n",
    "### $$ f: \\mathbb R^{m \\times n} \\to \\mathbb R \\; \\nabla f \\in \\mathbb R^{m \\times n}, \\nabla f_{ij} = \\frac{\\partial f_i}{\\partial x_{ij}} $$\n",
    "\n",
    "\n",
    "### $$ f: \\mathbb R^{n} \\to \\mathbb R^m \\; \\nabla f \\in \\mathbb R^{m \\times n}, \\nabla f_{ij} = \\frac{\\partial f_i}{\\partial x_{j}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ebde2",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "\n",
    "\n",
    "### $$ y := f_1(f_2(\\dots(f_k(X)))$$\n",
    "\n",
    "### $$ \\frac{\\Delta y}{ \\Delta X} = \\big(\\frac{\\Delta f_k}{\\Delta X} \\big)^T \\big( \\frac{\\Delta f_{k-1}}{\\Delta f_k} \\big)^T \\big(\\frac{\\Delta f_{k-2}}{\\Delta f_{k-1}} \\big)^T \\dots \\frac{\\Delta f_1}{\\Delta f_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe749558",
   "metadata": {},
   "source": [
    "### Least Squares\n",
    "\n",
    "Let  $A \\in \\mathbb R ^{m \\times n}, b\\in \\mathbb R^m s.t. b \\not \\in R(A)$. Note that $R(A) = \\{ v \\in \\mathbb R ^m : v =Ax, x\\in \\mathbb R^n \\}$. The latter condition implies $Ax \\approx b$ as good as we can get. $\\mid\\mid x \\mid\\mid_2 ^2 = x^T x$\n",
    "\n",
    "### $$ \\mid\\mid Ax - b \\mid\\mid_2 ^2 = (Ax -b)^T (Ax-b)$$\n",
    "### $$ = x^T A^T Ax - 2b^T Ax + b^T b$$\n",
    "\n",
    "\n",
    "### Taking the gradient w.r.t $x$\n",
    "\n",
    "#### $$ \\nabla_x (x^T A^T Ax - 2b^T Ax + b^T b) = \\nabla_x x^T A^T Ax - \\nabla_x 2b^T Ax + \\nabla_x b^T b) $$\n",
    "\n",
    "### $$ = 2 A^T Ax  -2A^T b$$\n",
    "\n",
    "\n",
    "Setting this last equation equal to zero and solving for x gives us\n",
    "\n",
    "\n",
    "### $$ x = (A^T A)^{-1} A^T b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f899d2",
   "metadata": {},
   "source": [
    "### Affine Approximation\n",
    "\n",
    "\n",
    "First-order Tayplor approximation of differentiable $f: \\mathbb R^n \\to \\mathbb R^m$ around z:\n",
    "\n",
    "$$ \\hat f_i (x) = f_i(z) + \\frac{\\partial f_i}{\\partial x_1} (z) (x_1 - z_1) + \\dots + \\frac{\\partial f_i}{\\partial x_n} (z) (x_n -z_n), i = 1, \\dots, m$$\n",
    "\n",
    "\n",
    "In matrix notation: $$ \\hat f(x) = f(z) + \\Delta f(z)(x-z) $$\n",
    "\n",
    "\n",
    "\n",
    "$$ \\Delta f(z) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1}(z) & \\frac{\\partial f_1}{\\partial x_2}(z) & \\dots & \\frac{\\partial f_1}{\\partial x_z}(z)\\\\\n",
    "\\vdots & \\dots &\\dots &\\frac{\\partial f_2}{\\partial x_z}(z)\\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1}(z) & \\frac{\\partial f_m}{\\partial x_2}(z) & \\dots & \\frac{\\partial f_m}{\\partial x_z}(z)\\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca446905",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "\n",
    "## $$ P(D | \\beta, \\sigma^2) = P (y | X, \\beta, \\sigma^2) = \\prod_i ^n \\mathcal N (y_i | x_i , \\beta, \\sigma^2)$$\n",
    "\n",
    "## $$ =(2 \\pi \\sigma^2)^{-\\frac{n}{2}} exp\\big( -\\frac{1}{2\\sigma^2} (y -X\\beta)^T (y-X\\beta) \\big)$$\n",
    "Apply normal distribution and use vector notation.\n",
    "\n",
    "\n",
    "### $$-log P(D | \\beta, \\sigma^2) = \\frac{n}{2} log(\\sigma^2) + \n",
    "\\frac{1}{2\\sigma^2} (y - X\\beta)^T (y -X\\beta) $$\n",
    "Take the negative log of the both sides and remove constants.\n",
    "\n",
    "\n",
    "### $$ \\mid\\mid y - X\\beta \\mid\\mid = (y - X\\beta)^T (y -X\\beta)$$\n",
    "\n",
    "\n",
    "### $$= \\beta^T X^T X \\beta - 2(X^T y)^T \\beta^T + y^T y $$\n",
    "\n",
    "To find the minimum, we first take the derivative and set it to zero.\n",
    "\n",
    "\n",
    "### $$0= -2 (X^T) + 2X^T X \\beta $$\n",
    "\n",
    "\n",
    "\n",
    "### $$X^T X \\beta = X^T y$$\n",
    "\n",
    "### $$\\beta = (X^T X)^{-1} X^T y $$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pDL",
   "language": "python",
   "name": "pdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
