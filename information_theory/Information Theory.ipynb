{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy, Cross-Entropy and KL-Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In shannons theory, to transmit one bit of information means to reduce recipient's uncertainty by a factor of 2\n",
    "\n",
    "\n",
    "Let's understand this statement\n",
    "\n",
    "Assume that you live in an area where the weather is rainy in 50% chance and 50% sunny.\n",
    "\n",
    "If weather station tells you that tommorrow will be rainy, then weather statin reduce your uncertainly by factor of 2. So Weather station send you 1 bit of information. This is true no matter how such information is sendded to you.\n",
    "\n",
    "What does last sentence mean ?\n",
    "\n",
    "If W encoded the information that tomorrow will be rainy as string \"Rainy\", then W send you 40 bits (this means that five character is encoided on byte) message but still W comminicated one bit of information! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Suppose the weather has 8 possible states each of which is equally likely (12.5%).\n",
    "The information that the weather station sends to divides the uncertainty by factor of 8. Therefore, the weather station sends you 3 ($8=2^3$) bits of information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Suppose the weather has 2 possible states:\n",
    "Sunny: 75% and Rainy: 25%\n",
    "\n",
    "The weather station         -> RAINY -> You $log_2(4)=2$ bits of information sended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "The uncertainty reduction is the inverse of the events probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-log_2 (\\textbf{probability of event occurring})= \\textbf{the entropy of event}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### On average uncertainty reduction (Entropy)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_of_rainy=0.25\n",
    "proba_of_sunny=0.75\n",
    "\n",
    "(-math.log2(proba_of_rainy)) * proba_of_rainy + (-math.log2(proba_of_sunny)) * proba_of_sunny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "Entropy means average mounth of information received.\n",
    "\n",
    "$$ H(p) = - \\Sigma_i p_i log_2 (p_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is then Cross- Entropy ?\n",
    "\n",
    "Briefly, Cross - Entropy is the average message length.\n",
    "\n",
    "\n",
    "Suppose the weather has 8 possible states each of which has same probability of occurring. The weather station uses 3 bits to encode each weather state.\n",
    "\n",
    "( 000, 001, 010, 011, 100, 101, 110, 111 )\n",
    "\n",
    "Therefore, Cross-Entropy = 3 bits.\n",
    "\n",
    "### With different probabilities\n",
    "Suppose the weather has 8 possible states that have the following probabilities:\n",
    "<br> 0.35 0.35 0.1 0.1 0.04 0.4 0.1 0.1\n",
    "( 000, 001, 010, 011, 100, 101, 110, 111 )\n",
    "\n",
    "\n",
    "While entropy of probability distribution of weather is 2.23, the weather station use 3.bits to send an information of 2.23 bits,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.228972458935776"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(0.35 * math.log2(0.35) + 0.35 * math.log2(0.35)+ 0.1 * math.log2(0.1) + 0.1 * math.log2(0.1) + 0.04 * math.log2(0.04) + 0.04 * math.log2(0.04) + 0.01 * math.log2(0.01) + 0.01 * math.log2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to understand when H(p) = 2.23 but Cross Entropy(p)=3\n",
    "\n",
    "The weather station sends 0.77 unnecessary bits per message on average. Conculusion is that weather station should encode the state of weather differently.\n",
    "\n",
    "For instance:\n",
    "<br> 0.35 0.35 0.1 0.1 0.04 0.4 0.1 0.1\n",
    "\n",
    "( 00, 01, 100, 101, 1100, 1101, 11100, 11101 )\n",
    "\n",
    "Cross entropy of such is 2.42 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Entropy: $ H(p) = - \\Sigma_i p_i log_2 (p_i)$\n",
    "### Cross-Entropy: $ H(p,q) = - \\Sigma_i p_i log_2 (q_i)$\n",
    "### KL Divergence: $D_{KL} ( p || q ) = H(p,q) - H(p)$\n",
    "\n",
    "where $p$ and $q$ stand for true and predicted distribution.\n",
    "\n",
    "The amounth that cross-entropy exceeds the entropy is called as the relative entropy or KL-Divergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross- Entropy as a Cost Function\n",
    "\n",
    "Assume we have multi class classification problem wheter\n",
    "\n",
    "True Distribution: 0% 0% 100% 0%\n",
    "Predicted Distributuin: 10% 20% 50% 20%\n",
    "\n",
    "We can modify cross-entropy namely using natural log rather than log of 2.\n",
    "#### Cross-Entropy Loss: $ H(p,q) = - \\Sigma_i p_i log (q_i)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (temp)",
   "language": "python",
   "name": "temp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
