{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we use **only numpy** to implement **Feed Forward Neural Network** as a **Computational Graph** by employing the idea of forward and backward API. Therafter we will use our implementation on datasets provided in **sklearn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from util import spiral_data_gen\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self):\n",
    "        self.gates = []\n",
    "\n",
    "    def add(self, gate):\n",
    "        self.gates.append(gate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for g in self.gates:\n",
    "            inputs = g.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, dL):\n",
    "        for g in reversed(self.gates):\n",
    "            dL = g.backward(dL)\n",
    "\n",
    "    def update(self):\n",
    "        for g in self.gates:\n",
    "            g.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate:\n",
    "    def __init__(self, shape, learning_rate=.001, xavier_init=False):\n",
    "        fout, fin = shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = np.random.randn(fout, fin)\n",
    "        self.b = np.ones((fout, 1))\n",
    "        if xavier_init:\n",
    "            self.W /= np.sqrt(fin)\n",
    "        else:\n",
    "            self.W *= .001\n",
    "            self.b *= .001\n",
    "        self.X = None\n",
    "        self.S = None  # S=W.dot(x)+b\n",
    "        self.Z = None  # activation\n",
    "        self.dLdW, self.dLdb, self.dLdX = None, None, None\n",
    "        self.dZdS = None\n",
    "        self.dSdW = None\n",
    "        self.dSdX = None\n",
    "\n",
    "        self.decay_rate = .99\n",
    "        self.cache_w = np.zeros(self.W.shape)\n",
    "        self.cache_b = np.zeros(self.b.shape)\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Gradient Descent\n",
    "        # self.W += -self.learning_rate * self.dLdW\n",
    "        # self.b += -self.learning_rate * self.dLdb\n",
    "\n",
    "        # AdaGrad update\n",
    "        # self.cache_w += self.dLdW ** 2\n",
    "        # self.cache_b += self.dLdb ** 2\n",
    "        # self.W += -self.learning_rate * self.dLdW / (np.sqrt(self.cache_w) + 1e-7)\n",
    "        # self.b += -self.learning_rate * self.dLdb / (np.sqrt(self.cache_b) + 1e-7)\n",
    "\n",
    "        # RMSProb\n",
    "        self.cache_w = self.decay_rate * self.cache_w + (1 - self.decay_rate) * self.dLdW ** 2\n",
    "        self.cache_b = self.decay_rate * self.cache_b + (1 - self.decay_rate) * self.dLdb ** 2\n",
    "        self.W += -self.learning_rate * self.dLdW / (np.sqrt(self.cache_w) + 1e-7)\n",
    "        self.b += -self.learning_rate * self.dLdb / (np.sqrt(self.cache_b) + 1e-7)\n",
    "\n",
    "        # ADAM\n",
    "        # m =  beta1 * m + (1-beta1) * dx # Momentum\n",
    "        # v = beta2 * v + (1-beta2) * (dx**2) # RMSPROB like\n",
    "        # m /= 1-beta1**t\n",
    "        # v /= 1 - beta2 ** t\n",
    "        # self.W += -self.learning_rate * m / (np.sqrt(v) + 1e-7) # RMSPROB like\n",
    "\n",
    "\n",
    "class SigmoidGate(Gate):\n",
    "    def __init__(self, shape, learning_rate, xavier_init=False):\n",
    "        super().__init__(shape, learning_rate, xavier_init)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def dsigmoid(self, x):\n",
    "        return (1.0 - self.sigmoid(x)) * self.sigmoid(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.X = x\n",
    "        self.S = self.W.dot(self.X) + self.b\n",
    "        self.Z = self.sigmoid(self.S)\n",
    "        # compute local gradients\n",
    "        self.dZdS = self.dsigmoid(self.S)\n",
    "        self.dSdW = self.X\n",
    "        self.dSdX = self.W\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dLdZ):\n",
    "        assert self.Z.shape == dLdZ.shape\n",
    "        # dL/dS= dZ/dS * dL/dZ\n",
    "        dLdS = self.dZdS * dLdZ\n",
    "        # dL/dW= dS/dW * dL/dS\n",
    "        self.dLdW = dLdS.dot(self.dSdW.T)\n",
    "        # dL/dX= dS/dX * dL/dS\n",
    "        self.dLdX = self.dSdX.T.dot(dLdS)\n",
    "        self.dLdb = np.sum(dLdS, axis=1, keepdims=True)\n",
    "        return copy.deepcopy(dLdS)\n",
    "\n",
    "\n",
    "class SoftmaxGate(Gate):\n",
    "    def __init__(self, shape, learning_rate, xavier_init=False):\n",
    "        super().__init__(shape, learning_rate, xavier_init)\n",
    "\n",
    "    def softmax(self, x, axis=0):\n",
    "        x -= np.max(x, axis=axis, keepdims=True)\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores, axis=axis, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.X = x\n",
    "        self.S = self.W.dot(self.X) + self.b\n",
    "        self.dSdW = self.X\n",
    "        self.dSdX = self.W\n",
    "        return self.softmax(self.S)\n",
    "\n",
    "    def backward(self, dLdS):\n",
    "        assert self.S.shape == dLdS.shape\n",
    "        # Propagate dLdZ into dW,db, dS\n",
    "        # dLdW= dS/dW * dL/dS\n",
    "        self.dLdW = dLdS.dot(self.dSdW.T)\n",
    "        assert self.dLdW.shape == self.W.shape\n",
    "\n",
    "        # dLdX= dS/dX * dL/dS\n",
    "        dLdX = self.dSdX.T.dot(dLdS)\n",
    "        assert dLdX.shape == self.X.shape\n",
    "\n",
    "        self.dLdb = np.sum(dLdS, axis=1, keepdims=True)\n",
    "        return copy.deepcopy(dLdX)\n",
    "\n",
    "\n",
    "class ReluGate(Gate):\n",
    "    def __init__(self, shape, learning_rate, xavier_init=False):\n",
    "        super().__init__(shape, learning_rate, xavier_init)\n",
    "\n",
    "        if xavier_init:\n",
    "            self.W /= np.sqrt(fin / 2)  # He et al. 2015\n",
    "        else:\n",
    "            self.W *= .001\n",
    "            self.b *= .001\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.S = self.W.dot(x) + self.b\n",
    "        self.dSdW = x\n",
    "        self.dSdX = self.W\n",
    "        self.Z = self.relu(self.S)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dLdZ):\n",
    "        try:\n",
    "            assert self.Z.shape == dLdZ.shape\n",
    "        except:\n",
    "            print(self.Z.shape)\n",
    "            print(dLdZ.shape)\n",
    "            exit(1)\n",
    "        dLdZ[self.S <= 0] = 0\n",
    "        dZdS = dLdZ\n",
    "        # Propagate dLdZ into dW,db, dS\n",
    "        # dLdW= dS/dW * dL/dS\n",
    "        self.dLdW = dZdS.dot(self.dSdW.T)\n",
    "        # dLdX= dS/dX * dL/dS\n",
    "        dLdX = self.dSdX.T.dot(dZdS)\n",
    "        self.dLdb = np.sum(dZdS, axis=1, keepdims=True)\n",
    "        return copy.deepcopy(dLdX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 300)\n",
      "0.th epoch Loss:1.0690534873792346\n",
      "1000.th epoch Loss:0.3154230348059171\n",
      "2000.th epoch Loss:0.17371204566429593\n",
      "3000.th epoch Loss:0.09685781695093648\n",
      "4000.th epoch Loss:0.054512089125164184\n",
      "5000.th epoch Loss:0.03409565771282187\n",
      "6000.th epoch Loss:0.024718949602783133\n",
      "7000.th epoch Loss:0.019324454305583886\n",
      "8000.th epoch Loss:0.015667583673292265\n",
      "9000.th epoch Loss:0.013101627620210166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       100\n",
      "           1       1.00      0.99      0.99       100\n",
      "           2       1.00      0.99      0.99       100\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.99      0.99      0.99       300\n",
      "weighted avg       0.99      0.99      0.99       300\n",
      "\n",
      "(13, 178)\n",
      "0.th epoch Loss:1.069055140709139\n",
      "1000.th epoch Loss:-0.009648465911185972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        59\n",
      "           1       1.00      1.00      1.00        71\n",
      "           2       1.00      1.00      1.00        48\n",
      "\n",
      "    accuracy                           1.00       178\n",
      "   macro avg       1.00      1.00      1.00       178\n",
      "weighted avg       1.00      1.00      1.00       178\n",
      "\n",
      "(30, 569)\n",
      "0.th epoch Loss:0.673344153656415\n",
      "1000.th epoch Loss:0.007530776831090706\n",
      "2000.th epoch Loss:-0.0061669451291793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       212\n",
      "           1       1.00      1.00      1.00       357\n",
      "\n",
      "    accuracy                           1.00       569\n",
      "   macro avg       1.00      1.00      1.00       569\n",
      "weighted avg       1.00      1.00      1.00       569\n",
      "\n",
      "(4, 150)\n",
      "0.th epoch Loss:1.0690534746855442\n",
      "1000.th epoch Loss:0.02906579763153429\n",
      "2000.th epoch Loss:0.028890279383741103\n",
      "3000.th epoch Loss:0.02888899918038346\n",
      "4000.th epoch Loss:0.0288886005458535\n",
      "5000.th epoch Loss:0.028888277165607552\n",
      "6000.th epoch Loss:0.028887992561954116\n",
      "7000.th epoch Loss:0.02888772999447994\n",
      "8000.th epoch Loss:0.028887467546223143\n",
      "9000.th epoch Loss:0.0288872703435955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       0.98      0.98      0.98        50\n",
      "           2       0.98      0.98      0.98        50\n",
      "\n",
      "    accuracy                           0.99       150\n",
      "   macro avg       0.99      0.99      0.99       150\n",
      "weighted avg       0.99      0.99      0.99       150\n",
      "\n",
      "(64, 1797)\n",
      "0.th epoch Loss:2.2072749607170974\n",
      "1000.th epoch Loss:-0.009935568886070808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       178\n",
      "           1       1.00      1.00      1.00       182\n",
      "           2       1.00      1.00      1.00       177\n",
      "           3       1.00      1.00      1.00       183\n",
      "           4       1.00      1.00      1.00       181\n",
      "           5       1.00      1.00      1.00       182\n",
      "           6       1.00      1.00      1.00       181\n",
      "           7       1.00      1.00      1.00       179\n",
      "           8       1.00      1.00      1.00       174\n",
      "           9       1.00      1.00      1.00       180\n",
      "\n",
      "    accuracy                           1.00      1797\n",
      "   macro avg       1.00      1.00      1.00      1797\n",
      "weighted avg       1.00      1.00      1.00      1797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for X, y in [\n",
    "    spiral_data_gen(False),\n",
    "    (load_wine()['data'], load_wine()['target']),\n",
    "    (load_breast_cancer()['data'], load_breast_cancer()['target']),\n",
    "    (load_iris()['data'], load_iris()['target']),\n",
    "    (load_digits()['data'], load_digits()['target']),\n",
    "]:  # ,\n",
    "    X -= np.mean(X, axis=0)  # zero-centerring.\n",
    "    X = X.T\n",
    "    D, N = X.shape\n",
    "    K = len(np.unique(y))\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    hidden_size = 100\n",
    "    model = Net()  # TODO weight decay impleement.\n",
    "    model.add(ReluGate(shape=(hidden_size, D), learning_rate=.001))\n",
    "    model.add(SoftmaxGate(shape=(K, hidden_size), learning_rate=.001))\n",
    "    num_epoch = 10_000\n",
    "    mode = num_epoch // 10\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        # forward\n",
    "        f = model.forward(X)\n",
    "        if epoch % mode == 0:\n",
    "            loss = (-np.log(f[y, range(N)] + .01)).mean()  # compute the loss\n",
    "            print('{0}.th epoch Loss:{1}'.format(epoch, loss))\n",
    "            if loss < .001:\n",
    "                break\n",
    "        # backward\n",
    "        dLdf = f\n",
    "        dLdf[y, range(N)] -= 1\n",
    "        dLdf /= N\n",
    "        model.backward(dLdf)\n",
    "        model.update()\n",
    "\n",
    "    y_head = np.argmax(model.forward(X), axis=0)\n",
    "    print(classification_report(y, y_head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgement\n",
    "\n",
    "Thank your Andrej Karpathy for CS231n Winter 2016.\n",
    "\n",
    "https://www.youtube.com/watch?v=NfnWJUyUJYU&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conex)",
   "language": "python",
   "name": "conex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
