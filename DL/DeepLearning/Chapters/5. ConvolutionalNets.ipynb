{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Convolutions\n",
    "\n",
    "## 1. Image, Padding, Kernel, Stride, Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "N,N_H,N_W,n_C=1,5,5,1\n",
    "X = (np.random.randn(N, N_H,N_W,n_C) * 255).astype(np.uint8)\n",
    "plt.imshow(X[0,:,:,:], interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Selecting Special Location and Zero Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAD8CAYAAAB6iWHJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMnUlEQVR4nO3dYahk9XnH8e+vurGlhmhVdFm3mqJIQmg1ytYgFDERVIJbqKX6wrjBcEuIjSkNJqlgbV5Y0xcJBEPKohINMTFoarfBEje4IQaqdV1Wo2tNtkJxN1KNJmuWpIYrT1/M0V4n/+vqzpkzc/d+PzDcc+b87zz/YZffPXPOmfOkqpCkcb816wlImk+Gg6Qmw0FSk+EgqclwkNRkOEhqmigckvxekq1Jftz9PHqZca8k2dk9tkxSU9IwMsl1Dkn+EXixqm5M8mng6Kr6VGPc/qo6coJ5ShrYpOHwFHBuVT2bZC3wvao6rTHOcJBWmEnD4edVdVS3HOBnr66PjVsEdgKLwI1Vdc8yr7cALHSrZx70xCS9WT+tquNaGw4/0G8m+S5wQmPTtUtXqqqSLJc0J1XV3iR/ANyf5IdV9V/jg6pqM7C5q+t13dL0/fdyGw4YDlX1geW2JfmfJGuXfKx4bpnX2Nv9fDrJ94AzgN8IB0nzY9JTmVuAK7rlK4B/GR+Q5OgkR3TLxwLnALsmrCtpyiYNhxuB85P8GPhAt06Ss5Lc3I15F7A9yaPANkbHHAwHac5NdEBymjzmIA3ikao6q7XBKyQlNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUZDpKaDAdJTYaDpCbDQVKT4SCpyXCQ1GQ4SGoyHCQ1GQ6SmnoJhyQXJHkqye6u89X49iOS3NltfyjJyX3UlTQ9E4dDksOALwEXAu8GLkvy7rFhVzJqeHMK8AXgc5PWlTRdfew5bAB2V9XTVfVr4BvAxrExG4HbuuW7gPd3HbIkzak+wmEd8MyS9T3dc80xVbUI7AOO6aG2pCk5YMerIY31ypQ0Q33sOewF1i9ZP7F7rjkmyeHAO4AXxl+oqjZX1VnL3Udf0nD6CIeHgVOTvDPJ24BLGbXJW2pp27xLgPtrXrvpSAJ6+FhRVYtJrgK+AxwG3FpVTyT5LLC9qrYAtwBfTbIbeJFRgEiaY7bDk1Y32+FJemsMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUZDpKaDAdJTYaDpCbDQVKT4SCpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6SmoXplbkryfJKd3eMjfdSVND0T3316Sa/M8xl1u3o4yZaq2jU29M6qumrSepKG0UfHq9d6ZQIkebVX5ng4CLjjjjtmPYWpeeCBB2Y9hak4/vjjZz2Fqbn++uuX3TZUr0yAP0vyWJK7kqxvbCfJQpLtSbb3MC9JExjqgOS/AidX1R8CW/n/jtuvYzs8aX4M0iuzql6oqpe71ZuBM3uoK2mKBumVmWTtktWLgSd7qCtpiobqlfnxJBcDi4x6ZW6atK6k6erjbAVVdS9w79hz1y1Z/gzwmT5qSRqGV0hKajIcJDUZDpKaDAdJTYaDpCbDQVKT4SCpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNfXVDu/WJM8leXyZ7Unyxa5d3mNJ3ttHXUnT09eew1eAC95g+4XAqd1jAfhyT3UlTUkv4VBV32d0V+nlbARur5EHgaPGblcvac4MdczhTbXMsx2eND96uTV9X6pqM7AZIEnNeDrSqjbUnsMBW+ZJmi9DhcMW4EPdWYuzgX1V9exAtSUdhF4+ViT5OnAucGySPcDfAWsAquqfGHXDugjYDfwS+HAfdSVNT1/t8C47wPYCPtZHLUnD8ApJSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUZDpKaDAdJTYaDpKah2uGdm2Rfkp3d47o+6kqanr76VnwFuAm4/Q3GPFBVH+ypnqQpG6odnqQVZsiOV+9L8ijwE+CTVfXE+IAkC4wa7R6y1q49dFuEXn755bOewlRce+21s57CTAwVDjuAk6pqf5KLgHsYddx+HdvhSfNjkLMVVfVSVe3vlu8F1iQ5dojakg7OIOGQ5IQk6ZY3dHVfGKK2pIMzVDu8S4CPJlkEfgVc2nXBkjSnhmqHdxOjU52SVgivkJTUZDhIajIcJDUZDpKaDAdJTYaDpCbDQVKT4SCpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqmjgckqxPsi3JriRPJLm6MSZJvphkd5LHkrx30rqSpquPe0guAn9TVTuSvB14JMnWqtq1ZMyFjPpUnAr8MfDl7qekOTXxnkNVPVtVO7rlXwBPAuvGhm0Ebq+RB4Gjkhy6rZ+kQ0CvxxySnAycATw0tmkd8MyS9T38ZoCQZCHJ9iTb+5yXpLeut3Z4SY4E7gY+UVUvHcxr2A5Pmh+97DkkWcMoGL5WVd9qDNkLrF+yfmL3nKQ51cfZigC3AE9W1eeXGbYF+FB31uJsYF9VPTtpbUnT08fHinOAy4EfJtnZPfe3wO/Da+3w7gUuAnYDvwQ+3ENdSVM0cThU1Q+AHGBMAR+btJak4XiFpKQmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUZDpKaDAdJTYaDpCbDQVLTUO3wzk2yL8nO7nHdpHUlTddQ7fAAHqiqD/ZQT9IAhmqHJ2mF6a3jFbxhOzyA9yV5FPgJ8MmqeqLx+wvAQp9zmjf33XffrKcwNTfccMOspzAV11xzzaynMDXbtm1bdttQ7fB2ACdV1f4kFwH3MOq4/Tq2w5PmxyDt8Krqpara3y3fC6xJcmwftSVNxyDt8JKc0I0jyYau7guT1pY0PUO1w7sE+GiSReBXwKVdFyxJc2qodng3ATdNWkvScLxCUlKT4SCpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKmpjxvM/naS/0jyaNcO7+8bY45IcmeS3Uke6vpbSJpjfew5vAycV1V/BJwOXJDk7LExVwI/q6pTgC8An+uhrqQp6qMdXr3akwJY0z3G7yy9EbitW74LeP+rt6qXNJ/6ampzWHdb+ueArVU13g5vHfAMQFUtAvuAY/qoLWk6egmHqnqlqk4HTgQ2JHnPwbxOkoUk25Ns72Nekg5er2crqurnwDbggrFNe4H1AEkOB95Bo+NVVW2uqrOq6qw+5yXprevjbMVxSY7qln8HOB/4z7FhW4AruuVLgPvteCXNtz7a4a0FbktyGKOw+WZVfTvJZ4HtVbWFUS/NrybZDbwIXNpDXUlT1Ec7vMeAMxrPX7dk+X+BP5+0lqTheIWkpCbDQVKT4SCpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKlpqF6Zm5I8n2Rn9/jIpHUlTVcfd59+tVfm/iRrgB8k+beqenBs3J1VdVUP9SQNoI+7TxdwoF6ZklaY9NFbputZ8QhwCvClqvrU2PZNwD8AzwM/Av66qp5pvM4CsNCtngY8NfHk3rxjgZ8OWG8ovq+VZ8j3dlJVHdfa0Es4vPZio85X/wz8VVU9vuT5Y4D9VfVykr8E/qKqzuutcA+SbD8U2/D5vlaeeXlvg/TKrKoXqurlbvVm4Mw+60rq3yC9MpOsXbJ6MfDkpHUlTddQvTI/nuRiYJFRr8xNPdTt2+ZZT2BKfF8rz1y8t16POUg6dHiFpKQmw0FS06oPhyQXJHkqye4kn571fPqS5NYkzyV5/MCjV44k65NsS7Kru1z/6lnPqQ9v5msIg89pNR9z6A6i/ojRGZY9wMPAZVW1a6YT60GSP2F05ertVfWeWc+nL92Zr7VVtSPJ2xldfPenK/3fLEmA3136NQTg6sbXEAaz2vccNgC7q+rpqvo18A1g44zn1Iuq+j6jM0OHlKp6tqp2dMu/YHRafN1sZzW5GpmrryGs9nBYByy9jHsPh8B/tNUiycnAGcBDM55KL5IclmQn8Bywtapm+r5WezhohUpyJHA38ImqemnW8+lDVb1SVacDJwIbksz04+BqD4e9wPol6yd2z2mOdZ/J7wa+VlXfmvV8+rbc1xCGttrD4WHg1CTvTPI24FJgy4znpDfQHbi7BXiyqj4/6/n05c18DWFoqzocqmoRuAr4DqMDW9+sqidmO6t+JPk68O/AaUn2JLly1nPqyTnA5cB5S+4sdtGsJ9WDtcC2JI8x+qO1taq+PcsJrepTmZKWt6r3HCQtz3CQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Sm/wNjAeRYR8AGEQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. \n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = 0)    \n",
    "    return X_pad\n",
    "\n",
    "def select(X,ith_slice,rec_shape=None):\n",
    "    \"\"\"\n",
    "    Select rectange \n",
    "    Argument:\n",
    "    X -- \n",
    "    ith_slice --\n",
    "    rec_shape --\n",
    "    Returns:\n",
    "    X_select -- \n",
    "    \"\"\" \n",
    "    # rec_shape denote a rectangle shape.\n",
    "    # a--c\n",
    "    # b--d\n",
    "    if rec_shape:\n",
    "        a,b,c,d=rec_shape\n",
    "        return X[ith_slice,a:b,c:d,:]\n",
    "    else:\n",
    "        _,b,d,_=X.shape\n",
    "        return X[ith_slice,0:b,0:d,:]\n",
    "        \n",
    "def show_slice(x):\n",
    "    plt.imshow(x, interpolation='none')\n",
    "    plt.show()\n",
    "    \n",
    "X_padded=zero_pad(X,1)\n",
    "show_slice(select(X_padded,0))\n",
    "show_slice(select(X_padded,0,rec_shape=(0,4,0,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Kernels/ Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0., 158., 101., 122., 239., 220.,   0.],\n",
       "       [  0., 182., 188.,  62.,  81., 193.,   0.],\n",
       "       [  0., 116., 243., 174., 159.,  33.,   0.],\n",
       "       [  0., 232., 213.,  33.,  10., 148.,   0.],\n",
       "       [  0., 232.,  35., 229., 128., 229.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a filter/kernel has the same size of the image.\n",
    "w_H,w_W=3,3\n",
    "W0 = np.random.randn(w_H,w_W)\n",
    "x=select(X_padded,0)[:,:,0]\n",
    "x.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolution Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_step(x,w,b,S):\n",
    "    \"\"\" convolve (slide) over all spatial location.\"\"\"    \n",
    "    xD, xH, xW = x.shape\n",
    "    wD, wH, wW = w.shape\n",
    "    assert xH==xW and wH== wW and xD==wD\n",
    "    assert (xW-wW) % S ==0\n",
    "    num_conv_opt=int((xW-wW)/S+1)\n",
    "    Z = np.zeros((num_conv_opt, num_conv_opt))\n",
    "    F=wH\n",
    "    for i in range(num_conv_opt): # vertical\n",
    "        for j in range(num_conv_opt): #horiz\n",
    "        \n",
    "            x_loc = x[:, #depth.\n",
    "                      S * i: (S * i) + F,\n",
    "                      S * j: (S * j) + F]\n",
    "            Z[i,j]=np.sum(x_loc*w)+ b\n",
    "    return Z\n",
    "\n",
    "def convolve(X, W, b, S, pad=0):\n",
    "    # parameter check\n",
    "    xN, xD, xH, xW = X.shape\n",
    "    wN, wD, wH, wW = W.shape\n",
    "    assert wH == wW\n",
    "    assert (xH - wH) % S == 0\n",
    "    assert (xW - wW) % S == 0\n",
    "    \n",
    "    zH, zW = (xH - wH) // S + 1, (xW - wW) // S + 1\n",
    "\n",
    "    zD,zN = wN,xN\n",
    "    Z = np.zeros((zN, zD, zH, zW))\n",
    "\n",
    "    for n in range(zN):\n",
    "        x=X[n]\n",
    "        for d in range(zD):\n",
    "            # convolve d.th kernel on n.th input with S.\n",
    "            Z[n,d,:,:]=convolve_step(x,W[d],b[d],S)\n",
    "            \n",
    "    cache = (X, W, b, S)\n",
    "    return Z,cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_forward_step(x,W,b,S):\n",
    "    \"\"\" \n",
    "    x.shape = xD, xH, xW\n",
    "    W.shape = wN,wD, wH, wW\n",
    "    \n",
    "    convolve (slide) over all spatial location.\"\"\"    \n",
    "    xD, xH, xW = x.shape\n",
    "    wN,wD, wH, wW = W.shape\n",
    "    assert (xW-wW) % S ==0\n",
    "    num_conv_opt=int((xW-wW)/S+1)\n",
    "    Z = np.zeros((wN,num_conv_opt, num_conv_opt))\n",
    "    F=wH\n",
    "    \n",
    "    for d in range(wN): # .th kernel\n",
    "        for i in range(num_conv_opt): # vertical\n",
    "            for j in range(num_conv_opt): #horiz\n",
    "        \n",
    "                x_loc = x[:, #depth.\n",
    "                          S * i: (S * i) + F,\n",
    "                          S * j: (S * j) + F]\n",
    "                Z[d,i,j]=np.sum(x_loc*W[d])+ b[d]\n",
    "    return Z\n",
    "\n",
    "def convolve_forward(X, W, b, S, pad=0):\n",
    "    # parameter check\n",
    "    xN, xD, xH, xW = X.shape\n",
    "    wN, wD, wH, wW = W.shape\n",
    "    assert wH == wW\n",
    "    assert (xH - wH) % S == 0\n",
    "    assert (xW - wW) % S == 0\n",
    "    \n",
    "    zH, zW = (xH - wH) // S + 1, (xW - wW) // S + 1\n",
    "\n",
    "    zD,zN = wN,xN\n",
    "    Z = np.zeros((zN, zD, zH, zW))\n",
    "    dZ = np.zeros((zN, zD, zH, zW))\n",
    "\n",
    "    for n in range(zN):\n",
    "        Z[n,:,:,:]=convolve_forward_step(X[n],W,b,S)\n",
    "    cache = (X, W, b, S)\n",
    "    return Z,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.3669,  0.3132,  1.0201],\n",
      "          [ 0.0223, -0.7450, -0.3775],\n",
      "          [ 0.9611,  0.1796,  0.1493]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "xN, xD, xH, xW =1, 3, 7, 7\n",
    "X = np.random.randn(xN, xD, xH, xW)\n",
    "#kernel init\n",
    "nW, k, s = 1, 3, 2\n",
    "conv = nn.Conv1d(in_channels=xD, out_channels=nW,kernel_size=(k, k), stride=s)\n",
    "\n",
    "x_torch = torch.from_numpy(X)\n",
    "x_torch = x_torch.float()\n",
    "res=conv(x_torch)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.36690428  0.31318232  1.02012504]\n",
      "   [ 0.02232347 -0.74501389 -0.37748065]\n",
      "   [ 0.96114047  0.17957789  0.14925982]]]]\n"
     ]
    }
   ],
   "source": [
    "W=conv.weight.data.detach().numpy()\n",
    "b=conv.bias.data.detach().numpy()\n",
    "\n",
    "Z,cache=convolve_forward(X, W, b, s)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_backward_step(x,dz,W,b,S):\n",
    "    \n",
    "    dzD, dzH, dzW = dz.shape\n",
    "\n",
    "    xD, xH, xW = x.shape\n",
    "    wN,wD, wH, wW = W.shape\n",
    "    \n",
    "    assert dzD==wN\n",
    "    assert (xW-wW) % S ==0\n",
    "    num_conv_opt=int((xW-wW)/S+1)\n",
    "    F=wH\n",
    "    dx = np.zeros((xD, xH, xW))\n",
    "    dW = np.zeros((wN, wD, wH, wW))\n",
    "    db = np.zeros(wN)\n",
    "    \n",
    "    for n in range(wN): # .th kernel\n",
    "        for i in range(num_conv_opt): # vertical\n",
    "            for j in range(num_conv_opt): #horiz\n",
    "        \n",
    "                x_loc = x[:, #depth.\n",
    "                          S * i: (S * i) + F,\n",
    "                          S * j: (S * j) + F]\n",
    "                dx[:,S * i: (S * i) + F,S * j: (S * j) + F]=W[n]*dz[n] # dz ignored.\n",
    "\n",
    "                dW[n] = x_loc*dz[n]\n",
    "                db[n] = np.sum(dz[n])\n",
    "                    \n",
    "                return dx,dW,db\n",
    "    \n",
    "\n",
    "def convolve_backward(dZ,cache):\n",
    "    # parameter check\n",
    "    dZN, dZD, dZH, dZW = dZ.shape\n",
    "    X, W, b, S = cache\n",
    "    xN, xD, xH, xW = X.shape\n",
    "    wN, wD, wH, wW = W.shape\n",
    "\n",
    "    assert wH == wW and wN == dZD\n",
    "    assert (xH - wH) % S == 0\n",
    "    assert (xW - wW) % S == 0\n",
    "    \n",
    "    zH, zW = (xH - wH) // S + 1, (xW - wW) // S + 1\n",
    "\n",
    "    zD,zN = wN,xN\n",
    "    \n",
    "    dX = np.zeros((xN, xD, xH, xW))\n",
    "    dW = np.zeros((xN,wN, wD, wH, wW)) # \n",
    "    db = np.zeros(xN)\n",
    "    for n in range(zN):\n",
    "        \n",
    "        dX[n],dW[n],db[n]=convolve_backward_step(X[n],dZ[n],W,b,S)\n",
    "        \n",
    "        #print(dx.shape)\n",
    "        #print(dW.shape)\n",
    "    \n",
    "\n",
    "\n",
    "convolve_backward(Z,cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "xN, xD, xH, xW =2, 3, 7, 7\n",
    "X = np.random.randn(xN, xD, xH, xW)\n",
    "#kernel init\n",
    "nW, k, s = 2, 1, 2\n",
    "conv = nn.Conv1d(in_channels=xD, out_channels=nW,kernel_size=(k, k), stride=s)\n",
    "x_torch = torch.from_numpy(X)\n",
    "x_torch = x_torch.float()\n",
    "res=conv(x_torch)\n",
    "W=conv.weight.data.detach().numpy()\n",
    "b=conv.bias.data.detach().numpy()\n",
    "Z,cache=convolve(X, W, b, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprogataion Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(X, W):\n",
    "    '''\n",
    "    The forward computation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    X -- output activations of the previous layer, numpy array of shape (n_H_prev, n_W_prev) assuming input channels = 1\n",
    "    W -- Weights, numpy array of size (f, f) assuming number of filters = 1\n",
    "    \n",
    "    Returns:\n",
    "    H -- conv output, numpy array of size (n_H, n_W)\n",
    "    cache -- cache of values needed for conv_backward() function\n",
    "    '''\n",
    "    \n",
    "    # Retrieving dimensions from X's shape\n",
    "    (n_H_prev, n_W_prev) = X.shape\n",
    "    \n",
    "    # Retrieving dimensions from W's shape\n",
    "    (f, f) = W.shape\n",
    "    \n",
    "    # Compute the output dimensions assuming no padding and stride = 1\n",
    "    n_H = n_H_prev - f + 1\n",
    "    n_W = n_W_prev - f + 1\n",
    "    \n",
    "    # Initialize the output H with zeros\n",
    "    H = np.zeros((n_H, n_W))\n",
    "    \n",
    "    # Looping over vertical(h) and horizontal(w) axis of output volume\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            x_slice = X[h:h+f, w:w+f]\n",
    "            H[h,w] = np.sum(x_slice * W)\n",
    "            \n",
    "    # Saving information in 'cache' for backprop\n",
    "    cache = (X, W)\n",
    "    \n",
    "    return H, cache\n",
    "\n",
    "\n",
    "def conv_backward(dH, cache):\n",
    "    '''\n",
    "    The backward computation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dH -- gradient of the cost with respect to output of the conv layer (H), numpy array of shape (n_H, n_W) assuming channels = 1\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dX -- gradient of the cost with respect to input of the conv layer (X), numpy array of shape (n_H_prev, n_W_prev) assuming channels = 1\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W), numpy array of shape (f,f) assuming single filter\n",
    "    '''\n",
    "    \n",
    "    # Retrieving information from the \"cache\"\n",
    "    (X, W) = cache\n",
    "    \n",
    "    # Retrieving dimensions from X's shape\n",
    "    (n_H_prev, n_W_prev) = X.shape\n",
    "    \n",
    "    # Retrieving dimensions from W's shape\n",
    "    (f, f) = W.shape\n",
    "    \n",
    "    # Retrieving dimensions from dH's shape\n",
    "    (n_H, n_W) = dH.shape\n",
    "    \n",
    "    # Initializing dX, dW with the correct shapes\n",
    "    dX = np.zeros(X.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    \n",
    "    # Looping over vertical(h) and horizontal(w) axis of the output\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            dX[h:h+f, w:w+f] += W * dH(h,w)\n",
    "            dW += X[h:h+f, w:w+f] * dH(h,w)\n",
    "    \n",
    "    return dX, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.5510, -0.0690,  0.1572, -1.1135],\n",
      "          [-0.7062, -0.2047,  0.2572, -0.0537],\n",
      "          [ 0.0431, -0.1442,  1.9533,  0.0230],\n",
      "          [ 1.8321,  1.7977,  0.7392,  1.8597]],\n",
      "\n",
      "         [[ 0.5237,  0.5749,  0.2272,  0.3974],\n",
      "          [ 0.6568,  0.3183,  1.1487,  1.1175],\n",
      "          [ 0.2561,  0.3824,  0.2088,  0.0113],\n",
      "          [-0.3637, -0.4128, -0.0566,  0.9529]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4037, -0.6299, -0.1272, -0.3270],\n",
      "          [ 0.2388,  0.2696,  1.0443,  2.0536],\n",
      "          [ 0.9007,  1.3874, -0.0511,  1.9850],\n",
      "          [-0.1807,  0.0550,  0.0670, -0.3091]],\n",
      "\n",
      "         [[-0.1590,  0.6069, -0.3165,  1.0168],\n",
      "          [ 0.1648,  0.2015, -0.1999,  0.7849],\n",
      "          [ 0.2939,  0.9357,  0.8330,  0.4595],\n",
      "          [ 0.4909,  0.3967,  1.0961, -0.5626]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n",
      "torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# pool\n",
    "\n",
    "\n",
    "xN, xD, xH, xW =2, 3, 7, 7\n",
    "X = np.random.randn(xN, xD, xH, xW)\n",
    "#kernel init\n",
    "nW, k, s = 2, 1, 2\n",
    "conv = nn.Conv1d(in_channels=xD, out_channels=nW,kernel_size=(k, k), stride=s)\n",
    "\n",
    "x_torch = torch.from_numpy(X)\n",
    "x_torch = x_torch.float()\n",
    "res=conv(x_torch)\n",
    "print(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5510, 0.2572],\n",
      "          [1.8321, 1.9533]],\n",
      "\n",
      "         [[0.6568, 1.1487],\n",
      "          [0.3824, 0.9529]]],\n",
      "\n",
      "\n",
      "        [[[1.4037, 2.0536],\n",
      "          [1.3874, 1.9850]],\n",
      "\n",
      "         [[0.6069, 1.0168],\n",
      "          [0.9357, 1.0961]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[0.55101329, 0.25721157],\n",
       "         [1.83209276, 1.95325613]],\n",
       "\n",
       "        [[0.65677232, 1.14872611],\n",
       "         [0.38240495, 0.95293182]]],\n",
       "\n",
       "\n",
       "       [[[1.40366983, 2.05356979],\n",
       "         [1.38743043, 1.98496807]],\n",
       "\n",
       "        [[0.60688061, 1.01683474],\n",
       "         [0.93565553, 1.09606278]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def max_pool(x,F,S):\n",
    "    \"\"\" convolve (slide) over all spatial location.\"\"\"    \n",
    "    xD, xH, xW = x.shape\n",
    "    assert (xW-F) % S ==0\n",
    "    num_opt=int((xW-F)/S+1)\n",
    "    Z = np.zeros((xD,num_opt, num_opt))\n",
    "    for i in range(num_opt): # vertical\n",
    "        for j in range(num_opt): #horiz\n",
    "            for d in range(xD):            \n",
    "                Z[d,i,j] = x[d, # depth\n",
    "                          S * i: (S * i) + F,\n",
    "                          S * j: (S * j) + F].max()\n",
    "    return Z\n",
    "\n",
    "def pooling(X, k, S):\n",
    "    # parameter check\n",
    "    xN, xD, xH, xW = X.shape\n",
    "    assert (xH - k) % S == 0\n",
    "    assert (xW - k) % S == 0\n",
    "    \n",
    "    zH, zW = (xH - k) // S + 1, (xW - k) // S + 1\n",
    "    zN,zD=xN,xD\n",
    "    Z = np.zeros((zN, zD, zH, zW))\n",
    "\n",
    "    for n in range(zN):\n",
    "        x=X[n]\n",
    "        Z[n,:,:,:]=max_pool(x,k,S)\n",
    "        \n",
    "    return Z\n",
    "\n",
    "v=res.data.detach()\n",
    "m = nn.MaxPool2d(2, stride=s)\n",
    "\n",
    "print(m(v))\n",
    "\n",
    "pooling(v.numpy(),2,s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
    "    s = np.multiply(a_slice_prev, W)\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z += float(b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = int((n_H_prev - f + 2 * pad)/stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad)/stride) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                                  # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i]                      # Select ith training example's padded activation\n",
    "        for h in range(n_H):                            # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                        # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                    # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f \n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])\n",
    "                                        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "\n",
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                          # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, w, h, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                           # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "\n",
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = np.max(x) == x\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz / (n_H * n_W)\n",
    "\n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a\n",
    "\n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    for i in range(m):                          # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask, dA[i, h, w, c]) \n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616852\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "#print(\"Z's mean =\", np.mean(Z))\n",
    "#print(\"Z[3,2,1] =\", Z[3,2,1])\n",
    "#print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])\n",
    "\n",
    "\n",
    "#A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "#print(\"mode = max\")\n",
    "#print(\"A =\", A)\n",
    "#print()\n",
    "#A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "#print(\"mode = average\")\n",
    "#print(\"A =\", A)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#x = np.random.randn(2,3)\n",
    "#mask = create_mask_from_window(x)\n",
    "#print('x = ', x)\n",
    "#print(\"mask = \", mask)\n",
    "\n",
    "#a = distribute_value(2, (2,2))\n",
    "#print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
