{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refresher: Derivatives, Gradients, Jacobians\n",
    "\n",
    "\n",
    "#### Scalar valued functions\n",
    "Let $f$ be a scalar valued function and defined as\n",
    "(say $f:\\mathbb{R}^1 \\mapsto \\mathbb{R}^1$). The derivative of $f$ is computed as \n",
    "\n",
    "$$ \\frac{\\partial f(x)}{\\partial x} = lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$\n",
    "\n",
    "\n",
    "Let $f$ be a scalar valued function and defined as\n",
    "(say $f:\\mathbb{R}^m \\mapsto \\mathbb{R}^1$). The derivative of $f$ is a vector of partial derivatives. Similarly,\n",
    "$\\frac{\\partial f(x)}{\\partial x_i}$ reveals us how much $f(x)$ increases if $x_i$ increases. Strictly speaking, **gradients** are only defined for scalar functions. For vector valued functions we are dealing with vector of partial derivatives.\n",
    "\n",
    "#### Vector  valued functions\n",
    "Let $f$ be a vector valued function (say $f:\\mathbb{R}^n \\mapsto \\mathbb{R}^m$).\n",
    "\n",
    "$$ f(\\vec{x}):\\begin{bmatrix}\n",
    "\\vec{x}_1\\\\\n",
    "\\vec{x}_2\\\\\n",
    "\\cdots\\\\\n",
    "\\vec{x}_n\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "\\vec{y}_1\\\\\n",
    "\\vec{y}_2\\\\\n",
    "\\cdots\\\\\n",
    "\\vec{y}_m\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Then the gradient of $\\vec{y}=f(\\vec{x})$ with respect to $\\vec{x}$ is a Jacobian matrix:\n",
    "\n",
    "$$ \\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align} $$\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobian of Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[-3.4 -1.6  0. ] \t \n",
      "S:\n",
      "[0.02701699 0.16344326 0.80953975]\n",
      "J:\n",
      " [[ 0.02628707 -0.00441574 -0.02187133]\n",
      " [-0.00441574  0.13672956 -0.13231381]\n",
      " [-0.02187133 -0.13231381  0.15418514]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def softmax_func(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z), axis=0)).T\n",
    "    return sm\n",
    "def softmax_grad(s):\n",
    "    # Take the derivative of softmax element w.r.t the each logit which is usually Wi * X\n",
    "    # input s is softmax value of the original input x.\n",
    "    # s.shape = (1, n)\n",
    "    # i.e. s = np.array([0.3, 0.7]), x = np.array([0, 1])\n",
    "    # initialize the 2-D jacobian matrix.\n",
    "    jacobian_m = np.diag(s)\n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m)):\n",
    "            if i == j:\n",
    "                jacobian_m[i][j] = s[i] * (1 - s[i])\n",
    "            else:\n",
    "                jacobian_m[i][j] = -s[i] * s[j]\n",
    "    return jacobian_m\n",
    "\n",
    "def softmax_grad_vec(softmax):\n",
    "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "    s = softmax.reshape(-1, 1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "x=np.array([-1.3, 0.5, 2.1])\n",
    "\n",
    "sx=softmax_func(x)\n",
    "print('X:\\n{0} \\t \\nS:\\n{1}'.format(x,sx))\n",
    "print('J:\\n',softmax_grad(sx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The diagonal of the Jacobian of Softmax** is always **positive** wherase all others are **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Lets combine all\n",
    "\n",
    "Let $\\vec{x}\\in \\mathbb{R}^n$ and $y \\in \\{0,1\\}$. Moreover, let $f:\\mathbb{R}^m \\mapsto \\mathbb{R}^n$ and $g:\\mathbb{R}^n \\mapsto \\mathbb{R}^1$. Let $f$ and $g$ are defined as $g(\\vec{x})= \\sum_i \\vec{x}_i , f(\\vec{x})=\\vec{x} \\circ 2 \\text{, where } \\circ \\text{ denotes Hadamard product}$. Then derivatives are computed as shown below.\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial f(\\vec{x})}{\\partial \\vec{x}}=\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align} $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial g(\\vec{y})}{\\partial \\vec{y} }=\\begin{align}\n",
    "   \\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial y_{m}}\n",
    "   \\end{array}\\right)\n",
    "   \\end{align}$$\n",
    "\n",
    "where $l=g(\\vec{y})$ and $\\vec{y}=f(\\vec{x})$. Then by the chain rule, the vector-Jacobian product would be the\n",
    "gradient of $l$ with respect to $\\vec{x}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial y_{m}}\n",
    "   \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial g(x)}{\\partial f(x)}=1, \\frac{\\partial f(x)}{\\partial x_i}=2$$\n",
    "\n",
    "$$\\frac{\\partial g(x)}{\\partial x_i}=  \\frac{\\partial f(x)}{\\partial x_i} *(\\frac{\\partial g(x)}{\\partial f(x)})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]], grad_fn=<MulBackward0>)\n",
      "tensor(8., grad_fn=<SumBackward0>)\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "def f(x):\n",
    "    return x*2\n",
    "def g(x):\n",
    "    return x.sum()\n",
    "fx=f(x)\n",
    "print(fx)\n",
    "gfx=g(fx)\n",
    "print(gfx)\n",
    "gfx.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= x+2\n",
    "y # AddBackward0 object indicating previous computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "print(z) # MulBackward0 indicating previous computation.\n",
    "o = z.mean() # to obtain a scalar value\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-334.0398, -726.5619,  751.9890], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4336,  0.6889, -0.8934]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((1,3), requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax=nn.Softmax(dim=1)\n",
    "l=softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=l*2\n",
    "k=k.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conex)",
   "language": "python",
   "name": "conex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
