{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functions import sigmoid,softmax,dsigmoid\n",
    "from util import spiral_data_gen\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive implementation of 3-layered vanilla NN with relu and softmax activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 178\n",
      "Epoch 0: loss 1.097287\n",
      "Epoch 10000: loss 0.570037\n",
      "Epoch 20000: loss 0.468397\n",
      "Epoch 30000: loss 0.423234\n",
      "Epoch 40000: loss 0.356129\n",
      "Epoch 50000: loss 0.276574\n",
      "Epoch 60000: loss 0.281639\n",
      "Epoch 70000: loss 0.186600\n",
      "Epoch 80000: loss 0.130987\n",
      "Epoch 90000: loss 0.456909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93        59\n",
      "           1       0.97      0.82      0.89        71\n",
      "           2       0.86      1.00      0.92        48\n",
      "\n",
      "    accuracy                           0.91       178\n",
      "   macro avg       0.91      0.92      0.91       178\n",
      "weighted avg       0.92      0.91      0.91       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#X,y=spiral_data_gen(False)\n",
    "X,y=load_wine()['data'],load_wine()['target']\n",
    "\n",
    "X=X.T\n",
    "D,N=X.shape\n",
    "print(D,N)\n",
    "K=len(np.unique(y))\n",
    "\n",
    "hidden_size=50\n",
    "W0 = 0.01 * np.random.randn(hidden_size,D)\n",
    "b0 = np.ones((hidden_size,1))*.001 # against dead ReLU neurons\n",
    "W1 = 0.01 * np.random.randn(hidden_size,hidden_size)\n",
    "b1 = np.ones((hidden_size,1))*.001\n",
    "W2 = 0.01 * np.random.randn(K,hidden_size)\n",
    "b2 = np.zeros((K,1))\n",
    "    \n",
    "num_epoch=100_000\n",
    "mode=num_epoch//10\n",
    "step_size=.001\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "        # forward pass\n",
    "        S0 = W0.dot(X)+ b0\n",
    "        Z0 = np.maximum(0, S0)\n",
    "        \n",
    "        S1 = W1.dot(Z0)+ b1\n",
    "        Z1 = np.maximum(0, S1)\n",
    "        \n",
    "        S2 = W2.dot(Z1) + b2\n",
    "        Z2  = softmax(S2)\n",
    "\n",
    "        # compute the loss\n",
    "        corect_logprobs = -np.log(Z2[y,range(N)])\n",
    "        loss = np.sum(corect_logprobs)/N\n",
    "        \n",
    "        if epoch % mode == 0:\n",
    "            print(\"Epoch %d: loss %f\" % (epoch, loss))\n",
    "\n",
    "        # backward pass\n",
    "\n",
    "        # Derivative of loss w.r.t. S2 the input of of softmax.\n",
    "        dS2 = Z2\n",
    "        dS2[y,range(N)] -= 1\n",
    "        dS2 /= N\n",
    "\n",
    "        # ScoreGate => Multiply and Addition gate\n",
    "        db2 = np.sum(dS2, axis=1, keepdims=True)\n",
    "        dW2 = dS2.dot(Z1.T)\n",
    "        dZ1  = W2.T.dot(dS2) \n",
    "\n",
    "        # ReluGate\n",
    "        dZ1[S1 <= 0] = 0\n",
    "        dS1=dZ1\n",
    "        \n",
    "        # ScoreGate\n",
    "        dW1 = dS1.dot(Z0.T)\n",
    "        db1 = np.sum(dS1, axis=1, keepdims=True)\n",
    "        dZ0  = W1.T.dot(dS1)\n",
    "\n",
    "        # ReluGate\n",
    "        dZ0[S0 <= 0] = 0\n",
    "        \n",
    "        # ScoreGate\n",
    "        dS0=dZ0\n",
    "        dW0=dS0.dot(X.T)\n",
    "        db0 = np.sum(dS0, axis=1, keepdims=True)\n",
    "        #dX=W0.T.dot(dZ0)\n",
    "\n",
    "\n",
    "\n",
    "        # perform a parameter update\n",
    "        W0 += -step_size * dW0\n",
    "        b0 += -step_size * db0\n",
    "        \n",
    "        W1 += -step_size * dW1\n",
    "        b1 += -step_size * db1\n",
    "        \n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "    \n",
    "# forward pass\n",
    "S0 = W0.dot(X)+ b0\n",
    "Z0 = np.maximum(0, S0)\n",
    "\n",
    "S1 = W1.dot(Z0)+ b1\n",
    "Z1 = np.maximum(0, S1)\n",
    "\n",
    "S2 = W2.dot(Z1) + b2\n",
    "Z2  = softmax(S2)\n",
    "predicted_class = np.argmax(Z2, axis=0)\n",
    "\n",
    "print(classification_report(y, predicted_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# NN as computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMulGate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,W,b,X):\n",
    "        self.W=W\n",
    "        self.X=X\n",
    "        return self.W.dot(self.X)+b\n",
    "    def backward(self,dL):\n",
    "        dW=dL.dot(self.X.T)\n",
    "        dX=self.W.T.dot(dL)\n",
    "        db=np.sum(dL, axis=1, keepdims=True)\n",
    "        return dW,dX,db\n",
    "\n",
    "class ReluGate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,X):\n",
    "        self.X=X\n",
    "        return np.maximum(0, self.X)\n",
    "    def backward(self,dL):\n",
    "        dL[self.X <= 0] = 0\n",
    "        return dL\n",
    "\n",
    "class SoftmaxGate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,X):\n",
    "        self.Z=softmax(X)\n",
    "        return self.Z\n",
    "    def backward(self,dL):\n",
    "        return dL\n",
    "    \n",
    "class ComputationalGraph:\n",
    "    def __init__(self):\n",
    "        self.gates=[]\n",
    "    def add(self,shape,gate):\n",
    "        fout,fin=shape\n",
    "        W = 0.01 * np.random.randn(fout,fin)\n",
    "        b = np.ones((fout,1))*.001\n",
    "        self.gates.append(((W,b), MatMulGate(),gate))\n",
    "    def forward(self,X):\n",
    "        for t in self.gates:\n",
    "            (W,b),score,sigma=t\n",
    "            S=score.forward(W,b,X)\n",
    "            X=sigma.forward(S)\n",
    "        self.Z=X\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self,y):\n",
    "        # Derivative of loss w.r.t. S2 the input of of softmax.\n",
    "        dL = self.Z\n",
    "        dL[y,range(N)] -= 1\n",
    "        for t in reversed(self.gates):\n",
    "            (W,b),score,sigma=t\n",
    "            dW,dL,db=score.backward(sigma.backward(dL))\n",
    "            W+=-.001*dW\n",
    "            b+=-.001*db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.th epoch Loss:1.0986035985370461\n",
      "1000.th epoch Loss:0.3363575663676093\n",
      "2000.th epoch Loss:0.036526840180824104\n",
      "3000.th epoch Loss:0.02558364402726342\n",
      "4000.th epoch Loss:0.020615359935970905\n",
      "5000.th epoch Loss:0.020835372518647404\n",
      "6000.th epoch Loss:0.01870629886306993\n",
      "7000.th epoch Loss:0.017806767778799255\n",
      "8000.th epoch Loss:0.017432439581720912\n",
      "9000.th epoch Loss:0.015239174073848616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       100\n",
      "           1       0.98      1.00      0.99       100\n",
      "           2       1.00      0.99      0.99       100\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.99      0.99      0.99       300\n",
      "weighted avg       0.99      0.99      0.99       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y=spiral_data_gen(False)\n",
    "X=X.T\n",
    "D,N=X.shape\n",
    "K=len(np.unique(y))\n",
    "\n",
    "hidden_size=50\n",
    "\n",
    "model=ComputationalGraph()\n",
    "model.add((hidden_size,D),ReluGate())\n",
    "model.add((hidden_size,hidden_size),ReluGate())\n",
    "model.add((K,hidden_size),SoftmaxGate())\n",
    "num_epoch=10_000\n",
    "mode=num_epoch//10\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    preds=model.forward(X)\n",
    "    \n",
    "    if epoch%mode==0:\n",
    "        loss = (-np.log(preds[y,range(N)])).mean()# mean corect_logprobs\n",
    "        print('{0}.th epoch Loss:{1}'.format(epoch,loss))\n",
    "    model.backward(y)\n",
    "    \n",
    "    \n",
    "y_head = np.argmax(model.forward(X), axis=0)\n",
    "print(classification_report(y, y_head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.th epoch Loss:1.0986127970515298\n",
      "1000.th epoch Loss:1.0984531965136364\n",
      "2000.th epoch Loss:0.5452662860827222\n",
      "3000.th epoch Loss:0.039976832852391966\n",
      "4000.th epoch Loss:0.026662964448613533\n",
      "5000.th epoch Loss:0.021519567642350936\n",
      "6000.th epoch Loss:0.019624775449742664\n",
      "7000.th epoch Loss:0.01873121818890523\n",
      "8000.th epoch Loss:0.01799883200089535\n",
      "9000.th epoch Loss:0.017166354828606737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       100\n",
      "           1       0.98      1.00      0.99       100\n",
      "           2       1.00      0.99      0.99       100\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.99      0.99      0.99       300\n",
      "weighted avg       0.99      0.99      0.99       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y=spiral_data_gen(False)\n",
    "X=X.T\n",
    "D,N=X.shape\n",
    "K=len(np.unique(y))\n",
    "\n",
    "hidden_size=10\n",
    "\n",
    "model=ComputationalGraph()\n",
    "model.add((hidden_size,D),ReluGate())\n",
    "model.add((hidden_size,hidden_size),ReluGate())\n",
    "model.add((K,hidden_size),SoftmaxGate())\n",
    "num_epoch=10_000\n",
    "mode=num_epoch//10\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    preds=model.forward(X)\n",
    "    \n",
    "    if epoch%mode==0:\n",
    "        loss = (-np.log(preds[y,range(N)])).mean()# mean corect_logprobs\n",
    "        print('{0}.th epoch Loss:{1}'.format(epoch,loss))\n",
    "    model.backward(y)\n",
    "    \n",
    "    \n",
    "y_head = np.argmax(model.forward(X), axis=0)\n",
    "print(classification_report(y, y_head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.th epoch Loss:1.0986124329899025\n",
      "1000.th epoch Loss:1.0986122997309662\n",
      "2000.th epoch Loss:1.098612227197544\n",
      "3000.th epoch Loss:1.0986120860203632\n",
      "4000.th epoch Loss:1.0986113751635018\n",
      "5000.th epoch Loss:1.098590780052423\n",
      "6000.th epoch Loss:0.8678096677100844\n",
      "7000.th epoch Loss:0.7448633032637624\n",
      "8000.th epoch Loss:0.6485987119180769\n",
      "9000.th epoch Loss:0.6543618666477579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80       100\n",
      "           1       0.91      0.52      0.66       100\n",
      "           2       0.65      0.61      0.63       100\n",
      "\n",
      "    accuracy                           0.71       300\n",
      "   macro avg       0.74      0.71      0.70       300\n",
      "weighted avg       0.74      0.71      0.70       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y=spiral_data_gen(False)\n",
    "X=X.T\n",
    "D,N=X.shape\n",
    "K=len(np.unique(y))\n",
    "\n",
    "hidden_size=2\n",
    "\n",
    "model=ComputationalGraph()\n",
    "model.add((hidden_size,D),ReluGate())\n",
    "model.add((hidden_size,hidden_size),ReluGate())\n",
    "model.add((K,hidden_size),SoftmaxGate())\n",
    "num_epoch=10_000\n",
    "mode=num_epoch//10\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    preds=model.forward(X)\n",
    "    \n",
    "    if epoch%mode==0:\n",
    "        loss = (-np.log(preds[y,range(N)])).mean()# mean corect_logprobs\n",
    "        print('{0}.th epoch Loss:{1}'.format(epoch,loss))\n",
    "    model.backward(y)\n",
    "    \n",
    "    \n",
    "y_head = np.argmax(model.forward(X), axis=0)\n",
    "print(classification_report(y, y_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.th epoch Loss:1.0986122578268582\n",
      "1000.th epoch Loss:1.0986121457447826\n",
      "2000.th epoch Loss:1.0986114667613445\n",
      "3000.th epoch Loss:1.098590054680897\n",
      "4000.th epoch Loss:0.6322695540594208\n",
      "5000.th epoch Loss:0.5810257553547651\n",
      "6000.th epoch Loss:0.5703092413348116\n",
      "7000.th epoch Loss:0.5718422355032038\n",
      "8000.th epoch Loss:0.5732302550770745\n",
      "9000.th epoch Loss:0.6514727162328073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.57      0.62       100\n",
      "           1       0.65      0.89      0.75       100\n",
      "           2       0.95      0.75      0.84       100\n",
      "\n",
      "    accuracy                           0.74       300\n",
      "   macro avg       0.76      0.74      0.74       300\n",
      "weighted avg       0.76      0.74      0.74       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y=spiral_data_gen(False)\n",
    "X-=np.mean(X,axis=0) # Zero centered data is always good.\n",
    "X=X.T\n",
    "D,N=X.shape\n",
    "K=len(np.unique(y))\n",
    "\n",
    "hidden_size=2\n",
    "\n",
    "model=ComputationalGraph()\n",
    "model.add((hidden_size,D),ReluGate())\n",
    "model.add((hidden_size,hidden_size),ReluGate())\n",
    "model.add((K,hidden_size),SoftmaxGate())\n",
    "num_epoch=10_000\n",
    "mode=num_epoch//10\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    preds=model.forward(X)\n",
    "    \n",
    "    if epoch%mode==0:\n",
    "        loss = (-np.log(preds[y,range(N)])).mean()# mean corect_logprobs\n",
    "        print('{0}.th epoch Loss:{1}'.format(epoch,loss))\n",
    "    model.backward(y)\n",
    "    \n",
    "    \n",
    "y_head = np.argmax(model.forward(X), axis=0)\n",
    "print(classification_report(y, y_head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next let's use Momentum, Rmsprop and ADAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conex)",
   "language": "python",
   "name": "conex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
