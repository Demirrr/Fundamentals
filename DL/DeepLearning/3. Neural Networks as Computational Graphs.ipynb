{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functions import sigmoid,softmax,dsigmoid\n",
    "from util import spiral_data_gen\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 178\n",
      "Epoch 0: loss 1.092955\n",
      "Epoch 10000: loss 0.578790\n",
      "Epoch 20000: loss 0.474608\n",
      "Epoch 30000: loss 0.411107\n",
      "Epoch 40000: loss 0.418407\n",
      "Epoch 50000: loss 0.446248\n",
      "Epoch 60000: loss 0.309231\n",
      "Epoch 70000: loss 0.377409\n",
      "Epoch 80000: loss 0.477124\n",
      "Epoch 90000: loss 0.449588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92        59\n",
      "           1       0.89      0.94      0.92        71\n",
      "           2       0.96      0.98      0.97        48\n",
      "\n",
      "    accuracy                           0.93       178\n",
      "   macro avg       0.94      0.93      0.94       178\n",
      "weighted avg       0.93      0.93      0.93       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#X,y=spiral_data_gen(False)\n",
    "X,y=load_wine()['data'],load_wine()['target']\n",
    "\n",
    "\n",
    "X=X.T\n",
    "D,N=X.shape\n",
    "print(D,N)\n",
    "K=len(np.unique(y))\n",
    "\n",
    "hidden_size=50\n",
    "W0 = 0.01 * np.random.randn(hidden_size,D)\n",
    "b0 = np.zeros((hidden_size,1))\n",
    "W1 = 0.01 * np.random.randn(hidden_size,hidden_size)\n",
    "b1 = np.zeros((hidden_size,1))\n",
    "W2 = 0.01 * np.random.randn(K,hidden_size)\n",
    "b2 = np.zeros((K,1))\n",
    "    \n",
    "num_epoch=100_000\n",
    "mode=num_epoch//10\n",
    "step_size=.001\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "        # forward pass\n",
    "        S0 = W0.dot(X)+ b0\n",
    "        Z0 = np.maximum(0, S0)\n",
    "        \n",
    "        S1 = W1.dot(Z0)+ b1\n",
    "        Z1 = np.maximum(0, S1)\n",
    "        \n",
    "        S2 = W2.dot(Z1) + b2\n",
    "        Z2  = softmax(S2)\n",
    "\n",
    "        # compute the loss\n",
    "        corect_logprobs = -np.log(Z2[y,range(N)])\n",
    "        loss = np.sum(corect_logprobs)/N\n",
    "        \n",
    "        if epoch % mode == 0:\n",
    "            print(\"Epoch %d: loss %f\" % (epoch, loss))\n",
    "\n",
    "        # backward pass\n",
    "\n",
    "        # Derivative of loss w.r.t. S2 the input of of softmax.\n",
    "        dS2 = Z2\n",
    "        dS2[y,range(N)] -= 1\n",
    "        dS2 /= N\n",
    "\n",
    "        # ScoreGate => Multiply and Addition gate\n",
    "        db2 = np.sum(dS2, axis=1, keepdims=True)\n",
    "        dW2 = dS2.dot(Z1.T)\n",
    "        dZ1  = W2.T.dot(dS2) \n",
    "\n",
    "        # ReluGate\n",
    "        dZ1[S1 <= 0] = 0\n",
    "        dS1=dZ1\n",
    "        \n",
    "        # ScoreGate\n",
    "        dW1 = dS1.dot(Z0.T)\n",
    "        db1 = np.sum(dS1, axis=1, keepdims=True)\n",
    "        dZ0  = W1.T.dot(dS1)\n",
    "\n",
    "        # ReluGate\n",
    "        dZ0[S0 <= 0] = 0\n",
    "        \n",
    "        # ScoreGate\n",
    "        dS0=dZ0\n",
    "        dW0=dS0.dot(X.T)\n",
    "        db0 = np.sum(dS0, axis=1, keepdims=True)\n",
    "        #dX=W0.T.dot(dZ0)\n",
    "\n",
    "\n",
    "\n",
    "        # perform a parameter update\n",
    "        W0 += -step_size * dW0\n",
    "        b0 += -step_size * db0\n",
    "        \n",
    "        W1 += -step_size * dW1\n",
    "        b1 += -step_size * db1\n",
    "        \n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "    \n",
    "# forward pass\n",
    "S0 = W0.dot(X)+ b0\n",
    "Z0 = np.maximum(0, S0)\n",
    "\n",
    "S1 = W1.dot(Z0)+ b1\n",
    "Z1 = np.maximum(0, S1)\n",
    "\n",
    "S2 = W2.dot(Z1) + b2\n",
    "Z2  = softmax(S2)\n",
    "predicted_class = np.argmax(Z2, axis=0)\n",
    "\n",
    "print(classification_report(y, predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMulGate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,W,b,X):\n",
    "        self.W=W\n",
    "        self.X=X\n",
    "        return self.W.dot(self.X)+b\n",
    "    def backward(self,dL):\n",
    "        dW=dL.dot(self.X.T)\n",
    "        dX=self.W.T.dot(dL)\n",
    "        db=np.sum(dL, axis=1, keepdims=True)\n",
    "        return dW,dX,db\n",
    "class ReluGate:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,X):\n",
    "        self.X=X\n",
    "        return np.maximum(0, self.X)\n",
    "\n",
    "    def backward(self,dL):\n",
    "        dL[self.X <= 0] = 0\n",
    "        return dL\n",
    "    \n",
    "class SoftmaxGate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,X):\n",
    "        self.Z=softmax(X)\n",
    "        return self.Z\n",
    "    def backward(self,dL):\n",
    "        return dL\n",
    "    \n",
    "class ComputationalGraph:\n",
    "    def __init__(self):\n",
    "        self.gates=[]\n",
    "        \n",
    "    def add(self,shape,gate):\n",
    "        fout,fin=shape\n",
    "        W = 0.01 * np.random.randn(fout,fin)\n",
    "        b = np.zeros((fout,1))\n",
    "        \n",
    "        self.gates.append(((W,b), MatMulGate(),gate))\n",
    "        \n",
    "    def forward(self,X):\n",
    "        for t in self.gates:\n",
    "            (W,b),score,sigma=t\n",
    "            S=score.forward(W,b,X)\n",
    "            X=sigma.forward(S)\n",
    "        self.Z=X\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self,y):\n",
    "        \n",
    "        # Derivative of loss w.r.t. S2 the input of of softmax.\n",
    "        dL = self.Z\n",
    "        dL[y,range(N)] -= 1\n",
    "        for t in reversed(self.gates):\n",
    "            (W,b),score,sigma=t\n",
    "            dW,dL,db=score.backward(sigma.backward(dL))\n",
    "            W+=-.001*dW\n",
    "            b+=-.001*db\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.th epoch Loss:1.0986098813186431\n",
      "1000.th epoch Loss:0.5582987428439258\n",
      "2000.th epoch Loss:0.041315717949864474\n",
      "3000.th epoch Loss:0.02692384151150519\n",
      "4000.th epoch Loss:0.025109393590978225\n",
      "5000.th epoch Loss:0.02032171608476702\n",
      "6000.th epoch Loss:0.019059972354041003\n",
      "7000.th epoch Loss:0.01786430545616846\n",
      "8000.th epoch Loss:0.017478225684652512\n",
      "9000.th epoch Loss:0.017359418203780588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       100\n",
      "           1       1.00      0.99      0.99       100\n",
      "           2       1.00      0.99      0.99       100\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.99      0.99      0.99       300\n",
      "weighted avg       0.99      0.99      0.99       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y=spiral_data_gen(False)\n",
    "X=X.T\n",
    "D,N=X.shape\n",
    "K=len(np.unique(y))\n",
    "\n",
    "hidden_size=50\n",
    "\n",
    "model=ComputationalGraph()\n",
    "model.add((hidden_size,D),ReluGate())\n",
    "model.add((hidden_size,hidden_size),ReluGate())\n",
    "model.add((K,hidden_size),SoftmaxGate())\n",
    "num_epoch=10_000\n",
    "mode=num_epoch//10\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    preds=model.forward(X)\n",
    "    \n",
    "    if epoch%mode==0:\n",
    "        loss = (-np.log(preds[y,range(N)])).mean()# mean corect_logprobs\n",
    "        print('{0}.th epoch Loss:{1}'.format(epoch,loss))\n",
    "    model.backward(y)\n",
    "    \n",
    "    \n",
    "y_head = np.argmax(model.forward(X), axis=0)\n",
    "print(classification_report(y, y_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.th epoch Loss:1.0986120099501044\n",
      "1000.th epoch Loss:1.097885759337879\n",
      "2000.th epoch Loss:0.4622809226917157\n",
      "3000.th epoch Loss:0.09837451180661645\n",
      "4000.th epoch Loss:0.03345261940724648\n",
      "5000.th epoch Loss:0.023814132218072825\n",
      "6000.th epoch Loss:0.02014862473904211\n",
      "7000.th epoch Loss:0.01846945397651559\n",
      "8000.th epoch Loss:0.017486040516799917\n",
      "9000.th epoch Loss:0.01672988052676375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       100\n",
      "           1       1.00      0.99      0.99       100\n",
      "           2       0.98      1.00      0.99       100\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.99      0.99      0.99       300\n",
      "weighted avg       0.99      0.99      0.99       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y=spiral_data_gen(False)\n",
    "X=X.T\n",
    "D,N=X.shape\n",
    "K=len(np.unique(y))\n",
    "\n",
    "hidden_size=10\n",
    "\n",
    "model=ComputationalGraph()\n",
    "model.add((hidden_size,D),ReluGate())\n",
    "model.add((hidden_size,hidden_size),ReluGate())\n",
    "model.add((K,hidden_size),SoftmaxGate())\n",
    "num_epoch=10_000\n",
    "mode=num_epoch//10\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    preds=model.forward(X)\n",
    "    \n",
    "    if epoch%mode==0:\n",
    "        loss = (-np.log(preds[y,range(N)])).mean()# mean corect_logprobs\n",
    "        print('{0}.th epoch Loss:{1}'.format(epoch,loss))\n",
    "    model.backward(y)\n",
    "    \n",
    "    \n",
    "y_head = np.argmax(model.forward(X), axis=0)\n",
    "print(classification_report(y, y_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.th epoch Loss:1.0986123239301477\n",
      "1000.th epoch Loss:1.0986122377292749\n",
      "2000.th epoch Loss:1.0986121278608665\n",
      "3000.th epoch Loss:1.0986114522096948\n",
      "4000.th epoch Loss:1.0985913032777168\n",
      "5000.th epoch Loss:0.7958225122948998\n",
      "6000.th epoch Loss:0.6694200444507132\n",
      "7000.th epoch Loss:0.6682061308835694\n",
      "8000.th epoch Loss:0.6668168552679932\n",
      "9000.th epoch Loss:0.6652112322770667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.24      0.27       100\n",
      "           1       0.71      0.70      0.70       100\n",
      "           2       0.60      0.75      0.67       100\n",
      "\n",
      "    accuracy                           0.56       300\n",
      "   macro avg       0.54      0.56      0.55       300\n",
      "weighted avg       0.54      0.56      0.55       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y=spiral_data_gen(False)\n",
    "X=X.T\n",
    "D,N=X.shape\n",
    "K=len(np.unique(y))\n",
    "\n",
    "hidden_size=2\n",
    "\n",
    "model=ComputationalGraph()\n",
    "model.add((hidden_size,D),ReluGate())\n",
    "model.add((hidden_size,hidden_size),ReluGate())\n",
    "model.add((K,hidden_size),SoftmaxGate())\n",
    "num_epoch=10_000\n",
    "mode=num_epoch//10\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    preds=model.forward(X)\n",
    "    \n",
    "    if epoch%mode==0:\n",
    "        loss = (-np.log(preds[y,range(N)])).mean()# mean corect_logprobs\n",
    "        print('{0}.th epoch Loss:{1}'.format(epoch,loss))\n",
    "    model.backward(y)\n",
    "    \n",
    "    \n",
    "y_head = np.argmax(model.forward(X), axis=0)\n",
    "print(classification_report(y, y_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an NN API\n",
    "# Forward() and Backward()\n",
    "# Layers()\n",
    "# Implement ReLU, Sigmoid, TanH, Softmax\n",
    "# IT should be possible to use anyone them in anywhere of the NN.\n",
    "\n",
    "# Implement ADAM etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functions import sigmoid,softmax,dsigmoid\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staged Computation and Matrix-Matrix Multiply Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# forward pass\n",
    "W = np.random.randn(5, 10)\n",
    "X = np.random.randn(10, 3)\n",
    "D = W.dot(X)\n",
    "\n",
    "# now suppose we had the gradient on D from above in the circuit\n",
    "dD = np.random.randn(*D.shape) # same shape as D\n",
    "dW = dD.dot(X.T) #.T gives the transpose of the matrix\n",
    "dX = W.T.dot(dD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p,q):\n",
    "    \"\"\"Vectorized Cross entropy - p index of true class\n",
    "    q true and predicted dist.\"\"\"\n",
    "    assert np.all(np.sum(p,axis=1)==1)\n",
    "    assert np.all(np.sum(q,axis=1)==1)\n",
    "    return -np.nansum(p*log2(q),axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conex)",
   "language": "python",
   "name": "conex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
