{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utilities import softmax, tanh_gradient\n",
    "from preprocessing import build_dictionary, to_indices\n",
    "\n",
    "from sgd import bind_cost_gradient, get_stochastic_sampler\n",
    "from neural_network import flatten_cost_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant(learning_rate=0.5):\n",
    "    \"\"\"\n",
    "    Constant learning rate for gradient descent\n",
    "    \"\"\"\n",
    "    def constant(gradient):\n",
    "        return learning_rate\n",
    "    return constant\n",
    "\n",
    "def get_adagrad(learning_rate=0.5):\n",
    "    \"\"\"\n",
    "    Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\n",
    "    John Duchi, Elad Hazan and Yoram Singer, Journal of Machine Learning Research 12 (2011) 2121-2159\n",
    "    http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n",
    "    \"\"\"\n",
    "    sum_square_gradient = None\n",
    "\n",
    "    def adagrad(gradient):\n",
    "        nonlocal sum_square_gradient\n",
    "\n",
    "        if sum_square_gradient is None:\n",
    "            sum_square_gradient = np.ones_like(gradient)\n",
    "        sum_square_gradient += gradient ** 2\n",
    "        return learning_rate / np.sqrt(sum_square_gradient)\n",
    "\n",
    "    return adagrad\n",
    "\n",
    "def gradient_descent(cost_gradient, initial_parameters, iterations=1000, learning_rate=get_constant()):\n",
    "    \"\"\"\n",
    "    Gradient Descent finds parameters that minimizes cost function\n",
    "    :param cost_gradient: function to get cost and gradient given parameters\n",
    "    :param initial_parameters: the initial point to start gradient descent\n",
    "    :param iterations: total iterations to run gradient descent\n",
    "    :param learning_rate: algorithm to get and update learning rate\n",
    "    :return: final parameters and history of cost\n",
    "    \"\"\"\n",
    "    parameters = initial_parameters\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        cost, gradient = cost_gradient(parameters)\n",
    "\n",
    "        # Stop update if cost is not improved anymore\n",
    "        if len(cost_history) > 0 and cost_history[-1] == cost:\n",
    "            continue\n",
    "\n",
    "        step = learning_rate(gradient)\n",
    "        parameters -= step * gradient\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return parameters, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nplm_cost_gradient(parameters, input, output):\n",
    "    \"\"\"\n",
    "    Cost function for NPLM\n",
    "    :param parameters: tuple of (W, U, H, C)\n",
    "    :param input: indices of context word\n",
    "    :param output: index of current word\n",
    "    :return: cost and gradient\n",
    "    \"\"\"\n",
    "    W, U, H, C = parameters\n",
    "    context_size = len(input)\n",
    "    x = np.concatenate([C[input[i]] for i in range(context_size)])\n",
    "    x = np.append(x, 1.)    # Append bias term\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden_layer = np.tanh(H.dot(x))\n",
    "    y = W.dot(x) + U.dot(hidden_layer)\n",
    "    prediction = softmax(y.reshape(-1)).reshape(-1, 1)\n",
    "    cost = -np.sum(np.log(prediction[output]))\n",
    "\n",
    "    one_hot = np.zeros_like(prediction)\n",
    "    one_hot[output] = 1\n",
    "    delta = prediction - one_hot\n",
    "    gradient_W = delta.dot(x.T)\n",
    "    gradient_U = delta.dot(hidden_layer.T)\n",
    "    gradient_H = tanh_gradient(hidden_layer) * U.T.dot(delta).dot(x.T)\n",
    "    gradient_C = np.zeros_like(C)\n",
    "    gradient_y_x = W + U.dot(tanh_gradient(hidden_layer) * H)\n",
    "    gradient_x = gradient_y_x.T.dot(delta)\n",
    "    gradient_x = gradient_x[:-1, :]\n",
    "\n",
    "    gradient_x_split = np.split(gradient_x, context_size)\n",
    "    for i in range(context_size):\n",
    "        gradient_C[input[i]] += gradient_x_split[i].flatten()\n",
    "\n",
    "    gradient = [gradient_W, gradient_U, gradient_H, gradient_C]\n",
    "    return cost, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPLM:\n",
    "    \"\"\"\n",
    "    Neural Probabilistic Language Model (Bengio 2003)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabulary_size, feature_size, context_size, hidden_size):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.feature_size = feature_size\n",
    "        self.context_size = context_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_shape = (vocabulary_size + 1, feature_size * context_size + 1)\n",
    "        self.U_shape = (vocabulary_size + 1, hidden_size)\n",
    "        self.H_shape = (hidden_size, feature_size * context_size + 1)\n",
    "        self.C_shape = (vocabulary_size + 1, feature_size)\n",
    "\n",
    "        self.dictionary = None\n",
    "        self.reverse_dictionary = None\n",
    "        self.parameters = None\n",
    "\n",
    "    def train(self, sentences, iterations=1000):\n",
    "        # Preprocess sentences to create indices of context and next words\n",
    "        self.dictionary = build_dictionary(sentences, self.vocabulary_size)\n",
    "        indices = to_indices(sentences, self.dictionary)\n",
    "        self.reverse_dictionary = {index: word for word, index in self.dictionary.items()}\n",
    "        inputs, outputs = self.create_context(indices)\n",
    "\n",
    "        # Create cost and gradient function for gradient descent\n",
    "        shapes = [self.W_shape, self.U_shape, self.H_shape, self.C_shape]\n",
    "        flatten_nplm_cost_gradient = flatten_cost_gradient(nplm_cost_gradient, shapes)\n",
    "        cost_gradient = bind_cost_gradient(flatten_nplm_cost_gradient, inputs, outputs,\n",
    "                                           sampler=get_stochastic_sampler(10))\n",
    "\n",
    "        # Train neural network\n",
    "        parameters_size = np.sum(np.product(shape) for shape in shapes)\n",
    "        initial_parameters = np.random.normal(size=parameters_size)\n",
    "        self.parameters, cost_history = gradient_descent(cost_gradient, initial_parameters, iterations)\n",
    "        return cost_history\n",
    "\n",
    "    def predict(self, context):\n",
    "        if self.dictionary is None or self.parameters is None:\n",
    "            print('Train before predict!')\n",
    "            return\n",
    "        context = context[-self.context_size:]\n",
    "        input = []\n",
    "        for word in context:\n",
    "            if word in self.dictionary:\n",
    "                input.append(self.dictionary[word])\n",
    "            else:\n",
    "                input.append(0)\n",
    "        W_size = np.product(self.W_shape)\n",
    "        U_size = np.product(self.U_shape)\n",
    "        H_size = np.product(self.H_shape)\n",
    "        split_indices = [W_size, W_size + U_size, W_size + U_size + H_size]\n",
    "        W, U, H, C = np.split(self.parameters, split_indices)\n",
    "        W = W.reshape(self.W_shape)\n",
    "        U = U.reshape(self.U_shape)\n",
    "        H = H.reshape(self.H_shape)\n",
    "        C = C.reshape(self.C_shape)\n",
    "\n",
    "        x = np.concatenate([C[input[i]] for i in range(self.context_size)])\n",
    "        x = np.append(x, 1.)    # Append bias term\n",
    "        x = x.reshape(-1, 1)\n",
    "        y = W.dot(x) + U.dot(np.tanh(H.dot(x)))\n",
    "\n",
    "        # You don't want to predict unknown words (index 0)\n",
    "        prediction = np.argmax(y[1:]) + 1\n",
    "        return self.reverse_dictionary[prediction]\n",
    "\n",
    "    def create_context(self, sentences):\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        for sentence in sentences:\n",
    "            context = []\n",
    "            for word in sentence:\n",
    "                if len(context) >= self.context_size:\n",
    "                    context = context[-self.context_size:]\n",
    "                    inputs.append(context)\n",
    "                    outputs.append(word)\n",
    "                context = context + [word]\n",
    "        return inputs, outputs\n",
    "\n",
    "    def gradient_check(self, inputs, outputs):\n",
    "        # Create cost and gradient function for gradient check\n",
    "        shapes = [self.W_shape, self.U_shape, self.H_shape, self.C_shape]\n",
    "        flatten_nplm_cost_gradient = flatten_cost_gradient(nplm_cost_gradient, shapes)\n",
    "        cost_gradient = bind_cost_gradient(flatten_nplm_cost_gradient, inputs, outputs)\n",
    "\n",
    "        # Gradient check!\n",
    "        parameters_size = np.sum(np.product(shape) for shape in shapes)\n",
    "        initial_parameters = np.random.normal(size=parameters_size)\n",
    "        result = gradient_check(cost_gradient, initial_parameters)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-5bddcd2bb18d>:34: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  parameters_size = np.sum(np.product(shape) for shape in shapes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:am - True Val :am\n",
      "Prediction:are - True Val :are\n"
     ]
    }
   ],
   "source": [
    "# Train NPLM\n",
    "sentences = [['^', 'i', 'am'], ['^', 'you', 'are']]\n",
    "# Test settings\n",
    "vocabulary_size = 5\n",
    "context_size = 1\n",
    "hidden_size = 1\n",
    "feature_size = 2\n",
    "\n",
    "nplm = NPLM(vocabulary_size, feature_size, context_size, hidden_size)\n",
    "nplm.train(sentences, 100)\n",
    "\n",
    "# Check if next word is predicted from context\n",
    "for sentence in sentences:\n",
    "    context = sentence[:-1]\n",
    "    word = sentence[-1]\n",
    "    prediction = nplm.predict(context)\n",
    "    print('Prediction:{0} - True Val :{1}'.format(prediction,word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
