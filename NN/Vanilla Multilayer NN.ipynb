{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "np.seterr(divide=\"raise\")\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def grad_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def grad_relu(z):\n",
    "    return (z >= 0) * 1\n",
    "\n",
    "\n",
    "def leakyrelu(z):\n",
    "    return np.maximum(0.01 * z, z)\n",
    "\n",
    "\n",
    "def grad_leakyrelu(z):\n",
    "    return 0.01 * (z < 0) + 1 * (z >= 0)\n",
    "\n",
    "\n",
    "def grad_tanh(z):\n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "\n",
    "class ActivationType(Enum):\n",
    "    sigmoid = \"Sigmoid\"\n",
    "    relu = \"\"\"ReLU\"\"\"\n",
    "    leakyrelu = \"\"\"LeakyReLU\"\"\"\n",
    "    tanh = \"\"\"Tanh\"\"\"\n",
    "\n",
    "\n",
    "class ActivationFunction(object):\n",
    "    def __init__(self, activation_type: ActivationType = ActivationType.relu):\n",
    "        \"\"\"\n",
    "        Class containing the possible activation functions for the nodes of the neural network.\n",
    "        Select the desired activation function amongst the one provided by the Enum ActivationType\n",
    "\n",
    "        :param activation_type:\n",
    "        :type activation_type: ActivationType\n",
    "        \"\"\"\n",
    "        self.__f = None\n",
    "        self.__grad_f = None\n",
    "\n",
    "        if type(activation_type) is ActivationType:\n",
    "            self.__activation_type = activation_type\n",
    "        else:\n",
    "            raise ValueError(\"The activation function must be of the type specified in the Activations Enum\")\n",
    "\n",
    "        if self.__activation_type is ActivationType.sigmoid:\n",
    "            self.__f = sigmoid\n",
    "            self.__grad_f = grad_sigmoid\n",
    "        elif self.__activation_type is ActivationType.relu:\n",
    "            self.__f = relu\n",
    "            self.__grad_f = grad_relu\n",
    "        elif self.__activation_type is ActivationType.leakyrelu:\n",
    "            self.__f = leakyrelu\n",
    "            self.__grad_f = grad_leakyrelu\n",
    "        elif self.__activation_type is ActivationType.tanh:\n",
    "            self.__f = lambda x: np.tanh(x)\n",
    "            self.__grad_f = grad_tanh\n",
    "        else:\n",
    "            raise ValueError(\"An Enum is missing\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{0} activation function\".format(self.__activation_type.value)\n",
    "\n",
    "    @property\n",
    "    def f(self):\n",
    "        \"\"\"\n",
    "        This function returns the activation function.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.__f\n",
    "\n",
    "    @property\n",
    "    def grad_f(self):\n",
    "        \"\"\"\n",
    "        This function returns the derivative of the activation function.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.__grad_f\n",
    "\n",
    "    @property\n",
    "    def activation_type(self):\n",
    "        \"\"\"\n",
    "        Type of activation function\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.__activation_type\n",
    "\n",
    "\n",
    "class OptimizationScheme(Enum):\n",
    "    gradientdescent = \"Gradient Descent\"\n",
    "    momentum = \"Gradient Descent with momentum\"\n",
    "    adam = \"Gradient Descent with adam\"\n",
    "\n",
    "\n",
    "class Regularization(Enum):\n",
    "    ridge = \"L2 regularization\"\n",
    "    lasso = \"Lasso\"\n",
    "    dropout = \"Dropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DeepNeuralNetwork(object):\n",
    "    def __init__(self, layer_dimensions):\n",
    "        \"\"\"\n",
    "        This class is used to create a deep neural network with L layers.\n",
    "        Specify the number of layers, the number of nodes of each layer and the type of activation function via the\n",
    "        `layer_dimension` parameter.\n",
    "\n",
    "        Layer dimension must be a list of tuples containing (number of nodes, ActivationFunction).\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "\n",
    "        layer_dimensions : list of tuples containing the layout of the DNN\n",
    "\n",
    "        parameters : dictionary with the parameters (W,b) of the neural network\n",
    "\n",
    "        :param layer_dimensions:\n",
    "        \"\"\"\n",
    "        self.layer_dimensions = layer_dimensions\n",
    "        self.__parameters = self.__initialize_deep_nn(self.layer_dimensions)\n",
    "        self.__cache = {}\n",
    "        self.__beta1 = None\n",
    "        self.__beta2 = None\n",
    "        self.__beta = None\n",
    "        self.__iteration = None\n",
    "        self.__cost = []\n",
    "        self.__regularization = None\n",
    "        self.__reg_lambda = 0\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self.__parameters\n",
    "\n",
    "    def __initialize_deep_nn(self, layer_dims):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        layer_dims -- python array (list) containing a tuple with the dimensions of each layer in our network\n",
    "        and the activation function\n",
    "\n",
    "        Returns:\n",
    "        parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                        Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                        bl -- bias vector of shape (layer_dims[l], 1)\n",
    "        \"\"\"\n",
    "        parameters = {}\n",
    "        for l in range(1, len(layer_dims)):\n",
    "            parameters[\"act_\" + str(l)] = layer_dims[l][1]\n",
    "            parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l][0], layer_dims[l - 1][0]) * 0.01\n",
    "            parameters[\"b\" + str(l)] = np.zeros((layer_dims[l][0], 1))\n",
    "        parameters[\"L\"] = len(layer_dims) - 1\n",
    "        return parameters\n",
    "\n",
    "    # Forward propagation step\n",
    "    def __forward_block(self, A_previous, thisW, thisb, thisactivation):\n",
    "        \"\"\"\n",
    "        Block to compute the forward propagation\n",
    "        \"\"\"\n",
    "        Z = np.dot(thisW, A_previous) + thisb\n",
    "        A = thisactivation.f(Z)\n",
    "        return A, Z\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        A = X\n",
    "        L = (self.parameters[\"L\"])\n",
    "        self.__cache[\"A0\"] = X\n",
    "        for l in range(1, L + 1):\n",
    "            A, Z = self.__forward_block(A, self.parameters[\"W\" + str(l)], self.parameters[\"b\" + str(l)],\n",
    "                                        self.parameters[\"act_\" + str(l)])\n",
    "            self.__cache[\"A\" + str(l)] = A\n",
    "            self.__cache[\"Z\" + str(l)] = Z\n",
    "            self.__cache[\"VdW\" + str(l)] = 0\n",
    "            self.__cache[\"Vdb\" + str(l)] = 0\n",
    "            self.__cache[\"SdW\" + str(l)] = 0\n",
    "            self.__cache[\"Sdb\" + str(l)] = 0\n",
    "        return A\n",
    "\n",
    "    # Backward propagation\n",
    "    def __backward_block(self, dA, A_prev, thisW, thisZ, thisactivation):\n",
    "        m = thisZ.shape[1]\n",
    "        dZ = dA * thisactivation.grad_f(thisZ)\n",
    "        dW = np.dot(dZ, A_prev.T) / m\n",
    "        db = dZ.sum(axis=1, keepdims=True) / m\n",
    "        dA_prev = np.dot(thisW.T, dZ)\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def backward_propagation(self, Y):\n",
    "        L = self.parameters[\"L\"]\n",
    "        AL = self.__cache[\"A\" + str(L)]\n",
    "        if self.parameters[\"act_\" + str(self.parameters[\"L\"])].activation_type is ActivationType.sigmoid:\n",
    "            dA = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not yet implemented\")\n",
    "\n",
    "        gradients = {}\n",
    "        for l in range(self.parameters[\"L\"], 0, -1):\n",
    "            dA, dW, db = self.__backward_block(dA, self.__cache[\"A\" + str(l - 1)], self.parameters[\"W\" + str(l)],\n",
    "                                               self.__cache[\"Z\" + str(l)], self.parameters[\"act_\" + str(l)])\n",
    "            gradients[\"dW\" + str(l)] = dW\n",
    "            gradients[\"db\" + str(l)] = db\n",
    "        return gradients\n",
    "\n",
    "    def upgrade_weights(self, grads, learning_rate=0.05,\n",
    "                        opt_scheme: OptimizationScheme = OptimizationScheme.gradientdescent):\n",
    "        L = self.parameters[\"L\"]\n",
    "        for l in range(1, L + 1):\n",
    "            if opt_scheme == OptimizationScheme.gradientdescent:\n",
    "                correction_dW = grads[\"dW\" + str(l)]\n",
    "                correction_db = grads[\"db\" + str(l)]\n",
    "            elif opt_scheme == OptimizationScheme.momentum:\n",
    "                self.__cache[\"VdW\" + str(l)] = self.__beta * self.__cache[\"VdW\" + str(l)] + (1 - self.__beta) * grads[\n",
    "                    \"dW\" + str(l)]\n",
    "                self.__cache[\"Vdb\" + str(l)] = self.__beta * self.__cache[\"Vdb\" + str(l)] + (1 - self.__beta) * grads[\n",
    "                    \"db\" + str(l)]\n",
    "                correction_dW = self.__cache[\"VdW\" + str(l)]\n",
    "                correction_db = self.__cache[\"Vdb\" + str(l)]\n",
    "            elif opt_scheme == OptimizationScheme.adam:\n",
    "                self.__cache[\"VdW\" + str(l)] = self.__beta1 * self.__cache[\"VdW\" + str(l)] + (1 - self.__beta1) * grads[\n",
    "                    \"dW\" + str(l)]\n",
    "                self.__cache[\"Vdb\" + str(l)] = self.__beta1 * self.__cache[\"Vdb\" + str(l)] + (1 - self.__beta1) * grads[\n",
    "                    \"db\" + str(l)]\n",
    "                self.__cache[\"SdW\" + str(l)] = self.__beta2 * self.__cache[\"SdW\" + str(l)] + (1 - self.__beta2) * grads[\n",
    "                    \"dW\" + str(l)] ** 2\n",
    "                self.__cache[\"Sdb\" + str(l)] = self.__beta2 * self.__cache[\"Sdb\" + str(l)] + (1 - self.__beta2) * grads[\n",
    "                    \"db\" + str(l)] ** 2\n",
    "\n",
    "                # bias correction\n",
    "                VdW = self.__cache[\"VdW\" + str(l)] / (1 - self.__beta1 ** self.__iteration)\n",
    "                Vdb = self.__cache[\"Vdb\" + str(l)] / (1 - self.__beta1 ** self.__iteration)\n",
    "                SdW = self.__cache[\"SdW\" + str(l)] / (1 - self.__beta2 ** self.__iteration)\n",
    "                Sdb = self.__cache[\"Sdb\" + str(l)] / (1 - self.__beta2 ** self.__iteration)\n",
    "                correction_dW = VdW / (np.sqrt(SdW) + 1e-8)\n",
    "                correction_db = Vdb / (np.sqrt(Sdb) + 1e-8)\n",
    "            else:\n",
    "                correction_dW = grads[\"dW\" + str(l)]\n",
    "                correction_db = grads[\"db\" + str(l)]\n",
    "\n",
    "            # Add regularization contribution\n",
    "            if self.__regularization == Regularization.ridge:\n",
    "                reg_contr = self.parameters[\"W\" + str(l)]\n",
    "            elif self.__regularization == Regularization.lasso:\n",
    "                reg_contr = np.sign(self.parameters[\"W\" + str(l)])\n",
    "            else:\n",
    "                reg_contr = 0\n",
    "\n",
    "            m = self.__cache[\"A0\"].shape[1]\n",
    "\n",
    "            self.__parameters[\"W\" + str(l)] -= learning_rate * (correction_dW + self.__reg_lambda * reg_contr / m)\n",
    "            self.__parameters[\"b\" + str(l)] -= learning_rate * correction_db\n",
    "        return self.parameters\n",
    "\n",
    "    def compute_cost(self, Y, Yhat):\n",
    "        \"\"\"\n",
    "        Implement the cost function defined by equation (7).\n",
    "\n",
    "        Arguments:\n",
    "        AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "        Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        cost -- cross-entropy cost\n",
    "        \"\"\"\n",
    "\n",
    "        m = Y.shape[1]\n",
    "        AL = Yhat\n",
    "        # Compute loss from aL and y.\n",
    "\n",
    "        AL[AL >= 1] = 1 - np.finfo(float).eps\n",
    "        AL[AL <= 0] = np.finfo(float).eps\n",
    "        try:\n",
    "            cost = -(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)).sum() / m\n",
    "        except FloatingPointError:\n",
    "            print(AL)\n",
    "\n",
    "        # Add regularization\n",
    "        if self.__regularization is not None:\n",
    "            for l in range(1, self.parameters[\"L\"] + 1):\n",
    "                if self.__regularization == Regularization.ridge:\n",
    "                    cost += self.__reg_lambda / m / 2 * (self.parameters[\"W\" + str(l)] ** 2).sum()\n",
    "                elif self.__regularization == Regularization.lasso:\n",
    "                    cost += self.__reg_lambda / m * np.abs(self.parameters[\"W\" + str(l)]).sum()\n",
    "\n",
    "        cost = np.squeeze(cost)  # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "        assert (cost.shape == ())\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def single_iteration(self, X, Y, learning_rate=0.05, minibatch_size=None,\n",
    "                         opt_scheme: OptimizationScheme = OptimizationScheme.gradientdescent, dev_set=None):\n",
    "\n",
    "        m = X.shape[1]\n",
    "        if minibatch_size is not None:\n",
    "            if minibatch_size >= m:\n",
    "                minibatch_size = m\n",
    "        perm = np.random.permutation(m)\n",
    "        X_shuffled = X[:, perm]\n",
    "        Y_shuffled = Y[:, perm]\n",
    "        if minibatch_size is None:\n",
    "            minibatch_size = m\n",
    "        num_minibatch_iteration = int(np.floor(m / minibatch_size))\n",
    "\n",
    "        nn_parameters, cost = None, None\n",
    "\n",
    "        for miniter in range(num_minibatch_iteration):\n",
    "            self.__iteration += 1\n",
    "            k0 = miniter * minibatch_size\n",
    "            k1 = (miniter + 1) * minibatch_size\n",
    "            yhat = self.forward_propagation(X_shuffled[:, k0:k1])\n",
    "            cost = self.compute_cost(Y_shuffled[:, k0:k1], yhat)\n",
    "            gradients = self.backward_propagation(Y_shuffled[:, k0:k1])\n",
    "            nn_parameters = self.upgrade_weights(gradients, learning_rate, opt_scheme)\n",
    "            if dev_set is not None:\n",
    "                yhat = self.predict(dev_set[0])\n",
    "                cost_dev = self.compute_cost(dev_set[1], yhat)\n",
    "            else:\n",
    "                cost_dev = np.nan\n",
    "            self.__cost.append([cost, cost_dev])\n",
    "\n",
    "        if m % minibatch_size != 0:\n",
    "            self.__iteration += 1\n",
    "            yhat = self.forward_propagation(X_shuffled[:, k1:])\n",
    "            cost = self.compute_cost(Y_shuffled[:, k1:], yhat)\n",
    "            gradients = self.backward_propagation(Y_shuffled[:, k1:])\n",
    "            nn_parameters = self.upgrade_weights(gradients, learning_rate, opt_scheme)\n",
    "            if dev_set is not None:\n",
    "                yhat = self.predict(dev_set[0])\n",
    "                cost_dev = self.compute_cost(dev_set[1], yhat)\n",
    "            else:\n",
    "                cost_dev = np.nan\n",
    "            self.__cost.append([cost, cost_dev])\n",
    "\n",
    "        return nn_parameters, cost\n",
    "\n",
    "    def gradient_descent(self, X, Y, learning_rate=0.5, num_iter=3000,\n",
    "                         minibatch_size=None,\n",
    "                         optimization_scheme: OptimizationScheme = OptimizationScheme.gradientdescent,\n",
    "                         regularization: Regularization = None,\n",
    "                         **kwargs):\n",
    "\n",
    "        dev_set = kwargs.get(\"dev_set\", None)\n",
    "\n",
    "        self.__iteration = 0\n",
    "        self.__regularization = regularization\n",
    "        print(optimization_scheme.value)\n",
    "\n",
    "        if optimization_scheme == OptimizationScheme.momentum:\n",
    "            self.__beta = kwargs.get(\"beta\", 0.9)\n",
    "        elif optimization_scheme == OptimizationScheme.adam:\n",
    "            self.__beta1 = kwargs.get(\"beta1\", 0.9)\n",
    "            self.__beta2 = kwargs.get(\"beta2\", 0.999)\n",
    "\n",
    "        if regularization is not None:\n",
    "            self.__reg_lambda = kwargs.get(\"lambd\")\n",
    "        for i in range(num_iter):\n",
    "            nn_parameters, cost = self.single_iteration(X, Y, learning_rate, minibatch_size, optimization_scheme,\n",
    "                                                        dev_set)\n",
    "            if i % 500 == 0 and kwargs.get(\"verbose\", True):\n",
    "                print(\"Iteration {0}; Cost {1}\".format(i, cost))\n",
    "\n",
    "        return self.__cost\n",
    "\n",
    "    def predict(self, X, pred_type=\"binary\"):\n",
    "        ypred = self.forward_propagation(X)\n",
    "        if pred_type == \"binary\":\n",
    "            return np.round(ypred)\n",
    "\n",
    "    def plot_learning_curve(self, fig=None, title=\"\", logarithmic=True):\n",
    "        if fig is None:\n",
    "            fig = plt.figure()\n",
    "        else:\n",
    "            plt.figure(num=fig.number)\n",
    "        cost = np.array(self.__cost)\n",
    "        iternum = np.arange(cost.shape[0])\n",
    "\n",
    "        plt.plot(iternum, cost[:, 0], label=\"Training error\")\n",
    "        if cost.shape[1] == 2:\n",
    "            plt.plot(iternum, cost[:, 1], label=\"Dev error\")\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        if logarithmic:\n",
    "            plt.yscale(\"log\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    data = load_breast_cancer()\n",
    "    X = data[\"data\"]\n",
    "    y = data[\"target\"]\n",
    "\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    X_train = X_train.T\n",
    "    X_test = X_test.T\n",
    "    y_train = y_train.reshape(1, -1)\n",
    "    y_test = y_test.reshape(1, -1)\n",
    "\n",
    "    nn_structure = [(X_train.shape[0], ActivationFunction(activation_type=ActivationType.leakyrelu)),\n",
    "                    (30, ActivationFunction(activation_type=ActivationType.leakyrelu)),\n",
    "                    (30, ActivationFunction(activation_type=ActivationType.leakyrelu)),\n",
    "                    (1, ActivationFunction(activation_type=ActivationType.sigmoid))]\n",
    "\n",
    "    # Same learning rate, different optimizations\n",
    "    for reg in [Regularization.lasso]:\n",
    "        for lambd in np.logspace(-5, -3, 5):\n",
    "            thisNN = DeepNeuralNetwork(nn_structure)\n",
    "            cost = thisNN.gradient_descent(X_train, y_train, learning_rate=0.00001, num_iter=20000,\n",
    "                                           minibatch_size=None,\n",
    "                                           optimization_scheme=OptimizationScheme.adam,\n",
    "                                           regularization=reg,\n",
    "                                           lambd=lambd,\n",
    "                                           dev_set=[X_test, y_test])\n",
    "            thisNN.plot_learning_curve(title=\"{0}, lambda: {1}\".format(reg.value, lambd), logarithmic=True)\n",
    "            ypred = thisNN.predict(X_test)\n",
    "            print(classification_report(y_test.T, ypred.T))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (temp)",
   "language": "python",
   "name": "temp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
