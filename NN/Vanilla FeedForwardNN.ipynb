{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from funcs import softmax,sigmoid, sigmoid_grad,gradcheck_naive\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(object):\n",
    "    \"\"\"\n",
    "    Implements 3 layer feed forward neural network\n",
    "    \n",
    "    Input + Hidden + Output\n",
    "    Sigmoid + SoftMax\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions,initial_params):\n",
    "        \n",
    "        self.dimensions=dimensions\n",
    "        self.params=initial_params#np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "        ofs = 0\n",
    "        self.Dx, self.H, self.Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "        self.W1 = np.reshape(self.params[ofs:ofs+ self.Dx * self.H], (self.Dx, self.H))\n",
    "        ofs += self.Dx * self.H\n",
    "        self.b1 = np.reshape(self.params[ofs:ofs + self.H], (1, self.H))\n",
    "        ofs += self.H\n",
    "        self.W2 = np.reshape(self.params[ofs:ofs + self.H * self.Dy], (self.H, self.Dy))\n",
    "        ofs += self.H * self.Dy\n",
    "        self.b2 = np.reshape(self.params[ofs:ofs + self.Dy], (1, self.Dy))\n",
    "    \n",
    "    def update_weights(self,gradient):\n",
    "        self.params -= gradient\n",
    "        \n",
    "        ofs = 0\n",
    "        self.Dx, self.H, self.Dy = (self.dimensions[0], self.dimensions[1], self.dimensions[2])\n",
    "\n",
    "        self.W1 = np.reshape(self.params[ofs:ofs+ self.Dx * self.H], (self.Dx, self.H))\n",
    "        ofs += self.Dx * self.H\n",
    "        self.b1 = np.reshape(self.params[ofs:ofs + self.H], (1, self.H))\n",
    "        ofs += self.H\n",
    "        self.W2 = np.reshape(self.params[ofs:ofs + self.H * self.Dy], (self.H, self.Dy))\n",
    "        ofs += self.H * self.Dy\n",
    "        self.b2 = np.reshape(self.params[ofs:ofs + self.Dy], (1, self.Dy))\n",
    "    \n",
    "    def predict(self,x):\n",
    "        h = sigmoid(np.dot(data,self.W1) + self.b1)\n",
    "        yhat = softmax(np.dot(h,self.W2) + self.b2)\n",
    "\n",
    "    def train(self, x, y,epoch=50):\n",
    "        \n",
    "        cost_history=[]\n",
    "        for i in range(epoch):\n",
    "            ### forward propagation\n",
    "            h = sigmoid(np.dot(x,self.W1) + self.b1)\n",
    "            yhat = softmax(np.dot(h,self.W2) + self.b2)\n",
    "\n",
    "            # calculate cost\n",
    "            cost = np.sum(-np.log(yhat[labels==1])) / data.shape[0]\n",
    "            cost_history.append(cost)\n",
    "            #=====backward propagation=====\n",
    "            d3 = (yhat - labels) / data.shape[0]\n",
    "            gradW2 = np.dot(h.T, d3)\n",
    "            gradb2 = np.sum(d3,0,keepdims=True)\n",
    "\n",
    "            dh = np.dot(d3,self.W2.T)\n",
    "            grad_h = sigmoid_grad(h) * dh\n",
    "\n",
    "            gradW1 = np.dot(data.T,grad_h)\n",
    "            gradb1 = np.sum(grad_h,0)\n",
    "\n",
    "            ### Stack gradients (do not modify)\n",
    "            grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),gradW2.flatten(), gradb2.flatten()))\n",
    "\n",
    "\n",
    "            self.update_weights(grad)\n",
    "        \n",
    "\n",
    "\n",
    "        return cost,cost_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "\n",
    "for i in range(N):\n",
    "    labels[i, random.randint(0,dimensions[2]-1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation for a two-layer sigmoidal network\n",
    "\n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "\n",
    "    Arguments:\n",
    "    data -- M x Dx matrix, where each row is a training example.\n",
    "    labels -- M x Dy matrix, where each row is a one-hot vector.\n",
    "    params -- Model parameters, these are unpacked for you.\n",
    "    dimensions -- A tuple of input dimension, number of hidden units\n",
    "                  and output dimension\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### forward propagation\n",
    "    h = sigmoid(np.dot(data,W1) + b1)\n",
    "    yhat = softmax(np.dot(h,W2) + b2)\n",
    "    #################################\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    cost = np.sum(-np.log(yhat[labels==1])) / data.shape[0]\n",
    "\n",
    "    d3 = (yhat - labels) / data.shape[0]\n",
    "    gradW2 = np.dot(h.T, d3)\n",
    "    gradb2 = np.sum(d3,0,keepdims=True)\n",
    "\n",
    "    dh = np.dot(d3,W2.T)\n",
    "    grad_h = sigmoid_grad(h) * dh\n",
    "\n",
    "    gradW1 = np.dot(data.T,grad_h)\n",
    "    gradb1 = np.sum(grad_h,0)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),\n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params=np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9608410135525742"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=FNN(dimensions,copy.deepcopy(initial_params))\n",
    "\n",
    "cost,hcost=model.train(data,labels)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    cost, gradient =forward_backward_prop(data, labels, initial_params, dimensions)\n",
    "    initial_params -= gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9608410135525742"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (temp)",
   "language": "python",
   "name": "temp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
