{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(x, prev_h, Wx, Wh, b):\n",
    "    # We separate on steps to make the backpropagation easier\n",
    "    #forward pass in steps\n",
    "    # step 1\n",
    "    xWx = np.dot(x, Wx)\n",
    "\n",
    "    # step 2\n",
    "    phWh = np.dot(prev_h,Wh)\n",
    "\n",
    "    # step 3\n",
    "    # total\n",
    "    affine = xWx + phWh + b.T\n",
    "\n",
    "    # step 4\n",
    "    next_h = np.tanh(t)\n",
    "\n",
    "    # Cache iputs, state, and weights\n",
    "    # we are having prev_h.copy() since python params are pass by reference.\n",
    "    cache = (x, prev_h.copy(), Wx, Wh, next_h, affine)\n",
    "\n",
    "    return next_h, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dnext_h, cache):\n",
    "    (x, prev_h, Wx, Wh, next_h, affine) = cache\n",
    "\n",
    "    #backward in step\n",
    "    # step 4\n",
    "    # dt delta of total\n",
    "    # Gradient of tanh times dnext_h\n",
    "    dt = (1 - np.square(np.tanh(affine))) * (dnext_h)\n",
    "\n",
    "    # step 3\n",
    "    # Gradient of sum block\n",
    "    dxWx = dt\n",
    "    dphWh = dt\n",
    "    db = np.sum(dt, axis=0)\n",
    "\n",
    "    # step 2\n",
    "    # Gradient of the mul block\n",
    "    dWh = prev_h.T.dot(dphWh)\n",
    "    dprev_h = Wh.dot(dphWh.T).T\n",
    "\n",
    "    # step 1\n",
    "    # Gradient of the mul block\n",
    "    dx = dxWx.dot(Wx.T)\n",
    "    dWx = x.T.dot(dxWx)\n",
    "\n",
    "    return dx, dprev_h, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, h0, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n",
    "    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running\n",
    "    the RNN forward, we return the hidden states for all timesteps.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
    "    - h0: Initial hidden state, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
    "    - cache: Values needed in the backward pass\n",
    "    \"\"\"\n",
    "\n",
    "    # Get shapes\n",
    "    N, T, D = x.shape\n",
    "    # Initialization\n",
    "    h, cache = None, None\n",
    "    H = h0.shape[1]\n",
    "    h = np.zeros((N,T,H))\n",
    "\n",
    "    # keeping the inital value in the last element\n",
    "    # it will be overwritten\n",
    "    h[:,-1,:] = h0\n",
    "    cache = []\n",
    "\n",
    "    # For each time-step\n",
    "    for t in xrange(T):\n",
    "        h[:,t,:], cache_step = rnn_step_forward(x[:,t,:], h[:,t-1,:], Wx, Wh, b)\n",
    "        cache.append(cache_step)\n",
    "\n",
    "    # Return current state and cache\n",
    "    return h, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rnn_backward(dh, cache):\n",
    "    \"\"\"\n",
    "    Compute the backward pass for a vanilla RNN over an entire sequence of data.\n",
    "\n",
    "    Inputs:\n",
    "    - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of inputs, of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradient of biases, of shape (H,)\n",
    "    \"\"\"\n",
    "    dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
    "    # Get shapes\n",
    "    N,T,H = dh.shape\n",
    "    D = cache[0][0].shape[1] # D taken from x in cache\n",
    "\n",
    "    # Initialization keeping the gradients with the same shape it's respective inputs/weights\n",
    "    dx, dprev_h = np.zeros((N, T, D)),np.zeros((N, H))\n",
    "    dWx, dWh, db = np.zeros((D, H)), np.zeros((H, H)), np.zeros((H,))\n",
    "    dh = dh.copy()\n",
    "\n",
    "    # For each time-step\n",
    "    for t in reversed(xrange(T)):\n",
    "        dh[:,t,:]  += dprev_h # updating the previous layer dh\n",
    "        dx_, dprev_h, dWx_, dWh_, db_ = rnn_step_backward(dh[:,t,:], cache[t])\n",
    "        # Observe that we sum each time-step gradient\n",
    "        dx[:,t,:] += dx_\n",
    "        dWx += dWx_\n",
    "        dWh += dWh_\n",
    "        db += db_\n",
    "\n",
    "    dh0 = dprev_h\n",
    "\n",
    "    return dx, dh0, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):\n",
    "  N,H = prev_c.shape\n",
    "\n",
    "  #forward pass in steps\n",
    "  # step 1: calculate activation vector\n",
    "  a = np.dot(x, Wx) + np.dot(prev_h,Wh) + b.T\n",
    "\n",
    "  # step 2: input gate\n",
    "  a_i = sigmoid(a[:,0:H])\n",
    "\n",
    "  # step 3: forget gate\n",
    "  a_f = sigmoid(a[:,H:2*H])\n",
    "\n",
    "  # step 4: output gate\n",
    "  a_o = sigmoid(a[:,2*H:3*H])\n",
    "\n",
    "  # step 5: block input gate\n",
    "  a_g= np.tanh(a[:,3*H:4*H])\n",
    "\n",
    "  # step 6: next cell state\n",
    "  next_c = a_f * prev_c +  a_i * a_g\n",
    "\n",
    "  # step 7: next hidden state\n",
    "  next_h = a_o * np.tanh(next_c)\n",
    "\n",
    "  # we are having *.copy() since python params are pass by reference.\n",
    "  cache = (x, prev_h.copy(), prev_c.copy(), a, a_i, a_f, a_o, a_g, next_h, next_c, Wx, Wh)\n",
    "\n",
    "  return next_h, next_c, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_step_backward(dnext_h, dnext_c, cache):\n",
    "  (x, prev_h, prev_c, a, a_i, a_f, a_o, a_g, next_h, next_c, Wx, Wh) = cache\n",
    "  N,H = dnext_h.shape\n",
    "  da = np.zeros(a.shape)\n",
    "\n",
    "  # step 7:\n",
    "  dnext_c = dnext_c.copy()\n",
    "  dnext_c += dnext_h * a_o * (1 - np.tanh(next_c) ** 2)\n",
    "  da_o = np.tanh(next_c) * dnext_h\n",
    "\n",
    "  # step 6:\n",
    "  da_f    = dnext_c * prev_c\n",
    "  dprev_c = dnext_c * a_f\n",
    "  da_i    = dnext_c * a_g\n",
    "  da_g    = dnext_c * a_i\n",
    "\n",
    "  # step 5:\n",
    "  da[:,3*H:4*H] = (1 - np.square(a_g)) * da_g\n",
    "\n",
    "  # step 4:\n",
    "  da[:,2*H:3*H] = (1 - a_o) * a_o * da_o\n",
    "\n",
    "  # step 3:\n",
    "  da[:,H:2*H] = (1 - a_f) * a_f * da_f\n",
    "\n",
    "  # step 2:\n",
    "  da[:,0:H] = (1 - a_i) * a_i * da_i\n",
    "\n",
    "  # step 1:\n",
    "  db = np.sum(da, axis=0)\n",
    "  dx = da.dot(Wx.T)\n",
    "  dWx = x.T.dot(da)\n",
    "  dprev_h = da.dot(Wh.T)\n",
    "  dWh = prev_h.T.dot(da)\n",
    "\n",
    "  return dx, dprev_h, dprev_c, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pl2vec)",
   "language": "python",
   "name": "pl2vec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
