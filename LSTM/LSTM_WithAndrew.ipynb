{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,1000000000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    return\n",
    "                    print(self.id)\n",
    "                    print(self.creation_op)\n",
    "                    print(len(self.creators))\n",
    "                    for c in self.creators:\n",
    "                        print(c.creation_op)\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        return softmax_output\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "    \n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "    \n",
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "        \n",
    "    def step(self, zero=True):\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            \n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_bias = bias\n",
    "        \n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        if(self.use_bias):\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "        if(self.use_bias):        \n",
    "            self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if(self.use_bias):\n",
    "            return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
    "        return input.mm(self.weight)\n",
    "\n",
    "\n",
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "\n",
    "\n",
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # this random initialiation style is just a convention from word2vec\n",
    "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "    \n",
    "class LSTMCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "\n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)        \n",
    "        self.xc = Linear(n_inputs, n_hidden)        \n",
    "        \n",
    "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_hidden, n_hidden, bias=False)        \n",
    "        \n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "\n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()        \n",
    "        self.parameters += self.ho.get_parameters()        \n",
    "        self.parameters += self.hc.get_parameters()                \n",
    "        \n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        prev_hidden = hidden[0]        \n",
    "        prev_cell = hidden[1]\n",
    "        \n",
    "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
    "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
    "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        \n",
    "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        \n",
    "        c = (f * prev_cell) + (i * g)\n",
    "\n",
    "        h = o * c.tanh()\n",
    "        \n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
    "        init_hidden.data[:,0] += 1\n",
    "        init_cell.data[:,0] += 1\n",
    "        return (init_hidden, init_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN Character Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# dataset from http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "f = open('shakespear.txt','r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
    "\n",
    "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "model.w_ho.weight.data *= 0\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "#         output.data *= 25\n",
    "#         temp_dist = output.softmax()\n",
    "#         temp_dist /= temp_dist.sum()\n",
    "\n",
    "#         m = (temp_dist > np.random.rand()).argmax()\n",
    "        m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches-1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations=400):\n",
    "    min_loss = 1000\n",
    "    for i in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "    #     batches_to_train = 32\n",
    "        for batch_i in range(batches_to_train):\n",
    "\n",
    "            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
    "\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)    \n",
    "                batch_loss = criterion.forward(output, target)\n",
    "\n",
    "                if(t == 0):\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])\n",
    "\n",
    "            loss = losses[-1]\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data / bptt\n",
    "\n",
    "            epoch_loss = np.exp(total_loss / (batch_i+1))\n",
    "            if(epoch_loss < min_loss):\n",
    "                min_loss = epoch_loss\n",
    "                print()\n",
    "\n",
    "            log = \"\\r Iter:\" + str(i)\n",
    "            log += \" - Alpha:\" + str(optim.alpha)[0:5]\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Min Loss:\" + str(min_loss)[0:5]\n",
    "            log += \" - Loss:\" + str(epoch_loss)\n",
    "            if(batch_i == 0):\n",
    "                log += \" - \" + generate_sample(n=70, init_char='T').replace(\"\\n\",\" \")\n",
    "            if(batch_i % 1 == 0):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "    #     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iter:0 - Alpha:0.05 - Batch 1/249 - Min Loss:62.00 - Loss:62.000000000000064 -          eeeeeeeeee sccc c cw w cw cw cw cw cw cw cw cw cw cw cw cw cw\n",
      " Iter:0 - Alpha:0.05 - Batch 2/249 - Min Loss:61.99 - Loss:61.99983657274221\n",
      " Iter:0 - Alpha:0.05 - Batch 3/249 - Min Loss:61.99 - Loss:61.990777900705616\n",
      " Iter:0 - Alpha:0.05 - Batch 4/249 - Min Loss:61.97 - Loss:61.97465873282837\n",
      " Iter:0 - Alpha:0.05 - Batch 5/249 - Min Loss:61.94 - Loss:61.94524911078736\n",
      " Iter:0 - Alpha:0.05 - Batch 6/249 - Min Loss:61.88 - Loss:61.88924576001193\n",
      " Iter:0 - Alpha:0.05 - Batch 7/249 - Min Loss:61.79 - Loss:61.79536596547225\n",
      " Iter:0 - Alpha:0.05 - Batch 8/249 - Min Loss:61.57 - Loss:61.57293597014469\n",
      " Iter:0 - Alpha:0.05 - Batch 9/249 - Min Loss:61.10 - Loss:61.1059243741745\n",
      " Iter:0 - Alpha:0.05 - Batch 10/249 - Min Loss:60.42 - Loss:60.42491419488334\n",
      " Iter:0 - Alpha:0.05 - Batch 11/249 - Min Loss:59.05 - Loss:59.05688011013684\n",
      " Iter:0 - Alpha:0.05 - Batch 12/249 - Min Loss:57.21 - Loss:57.21980032785642\n",
      " Iter:0 - Alpha:0.05 - Batch 13/249 - Min Loss:54.63 - Loss:54.6343604776348\n",
      " Iter:0 - Alpha:0.05 - Batch 14/249 - Min Loss:53.25 - Loss:53.25266167715341\n",
      " Iter:0 - Alpha:0.05 - Batch 15/249 - Min Loss:51.54 - Loss:51.54007631458389\n",
      " Iter:0 - Alpha:0.05 - Batch 16/249 - Min Loss:49.80 - Loss:49.808359906421565\n",
      " Iter:0 - Alpha:0.05 - Batch 17/249 - Min Loss:48.21 - Loss:48.2133622999548\n",
      " Iter:0 - Alpha:0.05 - Batch 18/249 - Min Loss:47.05 - Loss:47.05798869267075\n",
      " Iter:0 - Alpha:0.05 - Batch 19/249 - Min Loss:46.10 - Loss:46.10712716679951\n",
      " Iter:0 - Alpha:0.05 - Batch 20/249 - Min Loss:44.96 - Loss:44.96207198502095\n",
      " Iter:0 - Alpha:0.05 - Batch 21/249 - Min Loss:43.74 - Loss:43.74854415356936\n",
      " Iter:0 - Alpha:0.05 - Batch 23/249 - Min Loss:43.47 - Loss:43.488963575223126\n",
      " Iter:0 - Alpha:0.05 - Batch 24/249 - Min Loss:42.96 - Loss:42.96643474089105\n",
      " Iter:0 - Alpha:0.05 - Batch 25/249 - Min Loss:42.11 - Loss:42.118877399236375\n",
      " Iter:0 - Alpha:0.05 - Batch 26/249 - Min Loss:41.15 - Loss:41.15827199821596\n",
      " Iter:0 - Alpha:0.05 - Batch 27/249 - Min Loss:40.59 - Loss:40.59856389063494\n",
      " Iter:0 - Alpha:0.05 - Batch 28/249 - Min Loss:39.86 - Loss:39.86269786135888\n",
      " Iter:0 - Alpha:0.05 - Batch 29/249 - Min Loss:39.29 - Loss:39.2988068301034\n",
      " Iter:0 - Alpha:0.05 - Batch 30/249 - Min Loss:38.72 - Loss:38.72411071875421\n",
      " Iter:0 - Alpha:0.05 - Batch 31/249 - Min Loss:38.33 - Loss:38.33280547938428\n",
      " Iter:0 - Alpha:0.05 - Batch 32/249 - Min Loss:37.86 - Loss:37.86864849407287\n",
      " Iter:0 - Alpha:0.05 - Batch 33/249 - Min Loss:37.42 - Loss:37.42809860462747\n",
      " Iter:0 - Alpha:0.05 - Batch 34/249 - Min Loss:37.00 - Loss:37.008206031747946\n",
      " Iter:0 - Alpha:0.05 - Batch 35/249 - Min Loss:36.74 - Loss:36.74893765363816\n",
      " Iter:0 - Alpha:0.05 - Batch 36/249 - Min Loss:36.48 - Loss:36.48836653299209\n",
      " Iter:0 - Alpha:0.05 - Batch 37/249 - Min Loss:36.12 - Loss:36.12538305056167\n",
      " Iter:0 - Alpha:0.05 - Batch 38/249 - Min Loss:35.85 - Loss:35.852431042366355\n",
      " Iter:0 - Alpha:0.05 - Batch 39/249 - Min Loss:35.78 - Loss:35.78903189469104\n",
      " Iter:0 - Alpha:0.05 - Batch 40/249 - Min Loss:35.43 - Loss:35.43890269786248\n",
      " Iter:0 - Alpha:0.05 - Batch 42/249 - Min Loss:35.36 - Loss:35.47943540036312\n",
      " Iter:0 - Alpha:0.05 - Batch 43/249 - Min Loss:35.25 - Loss:35.25255062660062\n",
      " Iter:0 - Alpha:0.05 - Batch 44/249 - Min Loss:34.95 - Loss:34.958697319368376\n",
      " Iter:0 - Alpha:0.05 - Batch 45/249 - Min Loss:34.69 - Loss:34.69644536883856\n",
      " Iter:0 - Alpha:0.05 - Batch 46/249 - Min Loss:34.50 - Loss:34.50989523960506\n",
      " Iter:0 - Alpha:0.05 - Batch 47/249 - Min Loss:34.43 - Loss:34.43543943592192\n",
      " Iter:0 - Alpha:0.05 - Batch 48/249 - Min Loss:34.22 - Loss:34.22833162388896\n",
      " Iter:0 - Alpha:0.05 - Batch 49/249 - Min Loss:34.07 - Loss:34.076667018233486\n",
      " Iter:0 - Alpha:0.05 - Batch 50/249 - Min Loss:33.91 - Loss:33.91809827044763\n",
      " Iter:0 - Alpha:0.05 - Batch 51/249 - Min Loss:33.61 - Loss:33.612291150197294\n",
      " Iter:0 - Alpha:0.05 - Batch 52/249 - Min Loss:33.41 - Loss:33.41707712749826\n",
      " Iter:0 - Alpha:0.05 - Batch 53/249 - Min Loss:33.30 - Loss:33.30614100828295\n",
      " Iter:0 - Alpha:0.05 - Batch 54/249 - Min Loss:33.11 - Loss:33.11709763370113\n",
      " Iter:0 - Alpha:0.05 - Batch 55/249 - Min Loss:32.89 - Loss:32.89899170792932\n",
      " Iter:0 - Alpha:0.05 - Batch 56/249 - Min Loss:32.70 - Loss:32.70274164088967\n",
      " Iter:0 - Alpha:0.05 - Batch 57/249 - Min Loss:32.47 - Loss:32.47215494373155\n",
      " Iter:0 - Alpha:0.05 - Batch 58/249 - Min Loss:32.29 - Loss:32.29221576986845\n",
      " Iter:0 - Alpha:0.05 - Batch 59/249 - Min Loss:32.07 - Loss:32.07238912269907\n",
      " Iter:0 - Alpha:0.05 - Batch 60/249 - Min Loss:31.87 - Loss:31.877663732385333\n",
      " Iter:0 - Alpha:0.05 - Batch 61/249 - Min Loss:31.63 - Loss:31.63195981496253\n",
      " Iter:0 - Alpha:0.05 - Batch 62/249 - Min Loss:31.43 - Loss:31.430651834121655\n",
      " Iter:0 - Alpha:0.05 - Batch 63/249 - Min Loss:31.17 - Loss:31.175767772681613\n",
      " Iter:0 - Alpha:0.05 - Batch 64/249 - Min Loss:30.96 - Loss:30.96161438360349\n",
      " Iter:0 - Alpha:0.05 - Batch 65/249 - Min Loss:30.87 - Loss:30.872669250384934\n",
      " Iter:0 - Alpha:0.05 - Batch 66/249 - Min Loss:30.80 - Loss:30.80319265363615\n",
      " Iter:0 - Alpha:0.05 - Batch 67/249 - Min Loss:30.64 - Loss:30.647820515867007\n",
      " Iter:0 - Alpha:0.05 - Batch 68/249 - Min Loss:30.42 - Loss:30.420843995940096\n",
      " Iter:0 - Alpha:0.05 - Batch 69/249 - Min Loss:30.31 - Loss:30.311162739134467\n",
      " Iter:0 - Alpha:0.05 - Batch 70/249 - Min Loss:30.15 - Loss:30.152260556025208\n",
      " Iter:0 - Alpha:0.05 - Batch 71/249 - Min Loss:30.04 - Loss:30.045582356397055\n",
      " Iter:0 - Alpha:0.05 - Batch 72/249 - Min Loss:29.99 - Loss:29.99738330088125\n",
      " Iter:0 - Alpha:0.05 - Batch 73/249 - Min Loss:29.88 - Loss:29.889596672956955\n",
      " Iter:0 - Alpha:0.05 - Batch 74/249 - Min Loss:29.69 - Loss:29.696781827659187\n",
      " Iter:0 - Alpha:0.05 - Batch 75/249 - Min Loss:29.55 - Loss:29.555146466894335\n",
      " Iter:0 - Alpha:0.05 - Batch 76/249 - Min Loss:29.44 - Loss:29.444425410880577\n",
      " Iter:0 - Alpha:0.05 - Batch 77/249 - Min Loss:29.29 - Loss:29.296363671556968\n",
      " Iter:0 - Alpha:0.05 - Batch 78/249 - Min Loss:29.20 - Loss:29.206499706886873\n",
      " Iter:0 - Alpha:0.05 - Batch 79/249 - Min Loss:29.07 - Loss:29.073553413259113\n",
      " Iter:0 - Alpha:0.05 - Batch 80/249 - Min Loss:28.95 - Loss:28.951240829276223\n",
      " Iter:0 - Alpha:0.05 - Batch 81/249 - Min Loss:28.78 - Loss:28.783599071598488\n",
      " Iter:0 - Alpha:0.05 - Batch 82/249 - Min Loss:28.69 - Loss:28.69255373564771\n",
      " Iter:0 - Alpha:0.05 - Batch 86/249 - Min Loss:28.61 - Loss:28.717729410594124\n",
      " Iter:0 - Alpha:0.05 - Batch 87/249 - Min Loss:28.54 - Loss:28.547283869810215\n",
      " Iter:0 - Alpha:0.05 - Batch 88/249 - Min Loss:28.38 - Loss:28.389974234256886\n",
      " Iter:0 - Alpha:0.05 - Batch 89/249 - Min Loss:28.22 - Loss:28.229235119054056\n",
      " Iter:0 - Alpha:0.05 - Batch 90/249 - Min Loss:28.08 - Loss:28.084997873896153\n",
      " Iter:0 - Alpha:0.05 - Batch 91/249 - Min Loss:27.95 - Loss:27.951851828479906\n",
      " Iter:0 - Alpha:0.05 - Batch 92/249 - Min Loss:27.84 - Loss:27.84794449172883\n",
      " Iter:0 - Alpha:0.05 - Batch 93/249 - Min Loss:27.70 - Loss:27.700724663248554\n",
      " Iter:0 - Alpha:0.05 - Batch 94/249 - Min Loss:27.57 - Loss:27.57141664501556\n",
      " Iter:0 - Alpha:0.05 - Batch 95/249 - Min Loss:27.43 - Loss:27.43042559708271\n",
      " Iter:0 - Alpha:0.05 - Batch 96/249 - Min Loss:27.34 - Loss:27.34090088596974\n",
      " Iter:0 - Alpha:0.05 - Batch 97/249 - Min Loss:27.22 - Loss:27.22143129549187\n",
      " Iter:0 - Alpha:0.05 - Batch 98/249 - Min Loss:27.10 - Loss:27.105912715526006\n",
      " Iter:0 - Alpha:0.05 - Batch 99/249 - Min Loss:26.94 - Loss:26.942608818595772\n",
      " Iter:0 - Alpha:0.05 - Batch 100/249 - Min Loss:26.80 - Loss:26.801103683053125\n",
      " Iter:0 - Alpha:0.05 - Batch 101/249 - Min Loss:26.65 - Loss:26.658151543178704\n",
      " Iter:0 - Alpha:0.05 - Batch 102/249 - Min Loss:26.52 - Loss:26.5284743335239\n",
      " Iter:0 - Alpha:0.05 - Batch 103/249 - Min Loss:26.41 - Loss:26.411601000624792\n",
      " Iter:0 - Alpha:0.05 - Batch 104/249 - Min Loss:26.31 - Loss:26.319628449143565\n",
      " Iter:0 - Alpha:0.05 - Batch 105/249 - Min Loss:26.24 - Loss:26.249549422284662\n",
      " Iter:0 - Alpha:0.05 - Batch 106/249 - Min Loss:26.16 - Loss:26.164627101091835\n",
      " Iter:0 - Alpha:0.05 - Batch 107/249 - Min Loss:26.12 - Loss:26.127467024839458\n",
      " Iter:0 - Alpha:0.05 - Batch 108/249 - Min Loss:26.02 - Loss:26.020124807031067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Alpha:0.05 - Batch 109/249 - Min Loss:25.92 - Loss:25.926472927251197\n",
      " Iter:0 - Alpha:0.05 - Batch 110/249 - Min Loss:25.84 - Loss:25.84066319943876\n",
      " Iter:0 - Alpha:0.05 - Batch 111/249 - Min Loss:25.76 - Loss:25.761883994656483\n",
      " Iter:0 - Alpha:0.05 - Batch 112/249 - Min Loss:25.66 - Loss:25.668647320747212\n",
      " Iter:0 - Alpha:0.05 - Batch 113/249 - Min Loss:25.56 - Loss:25.56925679164181\n",
      " Iter:0 - Alpha:0.05 - Batch 114/249 - Min Loss:25.49 - Loss:25.49800076366808\n",
      " Iter:0 - Alpha:0.05 - Batch 115/249 - Min Loss:25.45 - Loss:25.454291164803955\n",
      " Iter:0 - Alpha:0.05 - Batch 116/249 - Min Loss:25.36 - Loss:25.369425733916813\n",
      " Iter:0 - Alpha:0.05 - Batch 117/249 - Min Loss:25.28 - Loss:25.289368319260397\n",
      " Iter:0 - Alpha:0.05 - Batch 118/249 - Min Loss:25.18 - Loss:25.189903940857047\n",
      " Iter:0 - Alpha:0.05 - Batch 119/249 - Min Loss:25.12 - Loss:25.120163928540265\n",
      " Iter:0 - Alpha:0.05 - Batch 120/249 - Min Loss:25.02 - Loss:25.0246353302297\n",
      " Iter:0 - Alpha:0.05 - Batch 121/249 - Min Loss:24.93 - Loss:24.935143887893094\n",
      " Iter:0 - Alpha:0.05 - Batch 122/249 - Min Loss:24.83 - Loss:24.83684844602597\n",
      " Iter:0 - Alpha:0.05 - Batch 123/249 - Min Loss:24.73 - Loss:24.737233737508028\n",
      " Iter:0 - Alpha:0.05 - Batch 124/249 - Min Loss:24.64 - Loss:24.64623603694101\n",
      " Iter:0 - Alpha:0.05 - Batch 125/249 - Min Loss:24.57 - Loss:24.570317447210513\n",
      " Iter:0 - Alpha:0.05 - Batch 126/249 - Min Loss:24.50 - Loss:24.504381545928833\n",
      " Iter:0 - Alpha:0.05 - Batch 127/249 - Min Loss:24.42 - Loss:24.421972914159298\n",
      " Iter:0 - Alpha:0.05 - Batch 128/249 - Min Loss:24.31 - Loss:24.315610303727585\n",
      " Iter:0 - Alpha:0.05 - Batch 129/249 - Min Loss:24.22 - Loss:24.22160867215345\n",
      " Iter:0 - Alpha:0.05 - Batch 130/249 - Min Loss:24.15 - Loss:24.15021102988389\n",
      " Iter:0 - Alpha:0.05 - Batch 131/249 - Min Loss:24.05 - Loss:24.05658673760712\n",
      " Iter:0 - Alpha:0.05 - Batch 132/249 - Min Loss:23.96 - Loss:23.960321509363553\n",
      " Iter:0 - Alpha:0.05 - Batch 133/249 - Min Loss:23.87 - Loss:23.873925373840123\n",
      " Iter:0 - Alpha:0.05 - Batch 134/249 - Min Loss:23.78 - Loss:23.783167134055287\n",
      " Iter:0 - Alpha:0.05 - Batch 135/249 - Min Loss:23.73 - Loss:23.731826248300194\n",
      " Iter:0 - Alpha:0.05 - Batch 136/249 - Min Loss:23.71 - Loss:23.716336302349802\n",
      " Iter:0 - Alpha:0.05 - Batch 137/249 - Min Loss:23.64 - Loss:23.64688522199768\n",
      " Iter:0 - Alpha:0.05 - Batch 138/249 - Min Loss:23.56 - Loss:23.563876147324248\n",
      " Iter:0 - Alpha:0.05 - Batch 140/249 - Min Loss:23.55 - Loss:23.59183963940461\n",
      " Iter:0 - Alpha:0.05 - Batch 141/249 - Min Loss:23.54 - Loss:23.540677009579905\n",
      " Iter:0 - Alpha:0.05 - Batch 142/249 - Min Loss:23.45 - Loss:23.452979306158635\n",
      " Iter:0 - Alpha:0.05 - Batch 143/249 - Min Loss:23.37 - Loss:23.37093302714607\n",
      " Iter:0 - Alpha:0.05 - Batch 144/249 - Min Loss:23.28 - Loss:23.289255049554004\n",
      " Iter:0 - Alpha:0.05 - Batch 145/249 - Min Loss:23.23 - Loss:23.230301483559934\n",
      " Iter:0 - Alpha:0.05 - Batch 146/249 - Min Loss:23.22 - Loss:23.227389539707794\n",
      " Iter:0 - Alpha:0.05 - Batch 147/249 - Min Loss:23.16 - Loss:23.169704738703707\n",
      " Iter:0 - Alpha:0.05 - Batch 148/249 - Min Loss:23.09 - Loss:23.09951591698129\n",
      " Iter:0 - Alpha:0.05 - Batch 149/249 - Min Loss:23.02 - Loss:23.02299819924927\n",
      " Iter:0 - Alpha:0.05 - Batch 150/249 - Min Loss:22.95 - Loss:22.955782543832687\n",
      " Iter:0 - Alpha:0.05 - Batch 151/249 - Min Loss:22.91 - Loss:22.914500152678855\n",
      " Iter:0 - Alpha:0.05 - Batch 152/249 - Min Loss:22.85 - Loss:22.85782417389782\n",
      " Iter:0 - Alpha:0.05 - Batch 153/249 - Min Loss:22.77 - Loss:22.772191162090433\n",
      " Iter:0 - Alpha:0.05 - Batch 154/249 - Min Loss:22.70 - Loss:22.702602915384766\n",
      " Iter:0 - Alpha:0.05 - Batch 155/249 - Min Loss:22.66 - Loss:22.664921046274884\n",
      " Iter:0 - Alpha:0.05 - Batch 156/249 - Min Loss:22.64 - Loss:22.645634567470754\n",
      " Iter:0 - Alpha:0.05 - Batch 157/249 - Min Loss:22.60 - Loss:22.60088943584266\n",
      " Iter:0 - Alpha:0.05 - Batch 158/249 - Min Loss:22.54 - Loss:22.54354122859766\n",
      " Iter:0 - Alpha:0.05 - Batch 159/249 - Min Loss:22.50 - Loss:22.505888023126722\n",
      " Iter:0 - Alpha:0.05 - Batch 160/249 - Min Loss:22.44 - Loss:22.444277820164462\n",
      " Iter:0 - Alpha:0.05 - Batch 161/249 - Min Loss:22.37 - Loss:22.376999722480203\n",
      " Iter:0 - Alpha:0.05 - Batch 162/249 - Min Loss:22.31 - Loss:22.319978504314758\n",
      " Iter:0 - Alpha:0.05 - Batch 163/249 - Min Loss:22.25 - Loss:22.259900358559964\n",
      " Iter:0 - Alpha:0.05 - Batch 164/249 - Min Loss:22.21 - Loss:22.211858387401097\n",
      " Iter:0 - Alpha:0.05 - Batch 165/249 - Min Loss:22.17 - Loss:22.17121671372962\n",
      " Iter:0 - Alpha:0.05 - Batch 166/249 - Min Loss:22.11 - Loss:22.11912574165254\n",
      " Iter:0 - Alpha:0.05 - Batch 167/249 - Min Loss:22.09 - Loss:22.09069363976322\n",
      " Iter:0 - Alpha:0.05 - Batch 168/249 - Min Loss:22.05 - Loss:22.056705974382375\n",
      " Iter:0 - Alpha:0.05 - Batch 169/249 - Min Loss:21.99 - Loss:21.999523591336988\n",
      " Iter:0 - Alpha:0.05 - Batch 170/249 - Min Loss:21.95 - Loss:21.952563860857115\n",
      " Iter:0 - Alpha:0.05 - Batch 171/249 - Min Loss:21.91 - Loss:21.912473814264104\n",
      " Iter:0 - Alpha:0.05 - Batch 172/249 - Min Loss:21.87 - Loss:21.87768638895867\n",
      " Iter:0 - Alpha:0.05 - Batch 173/249 - Min Loss:21.83 - Loss:21.838576114100217\n",
      " Iter:0 - Alpha:0.05 - Batch 174/249 - Min Loss:21.80 - Loss:21.80405958495522\n",
      " Iter:0 - Alpha:0.05 - Batch 175/249 - Min Loss:21.78 - Loss:21.784260013079553\n",
      " Iter:0 - Alpha:0.05 - Batch 176/249 - Min Loss:21.74 - Loss:21.74475958739141\n",
      " Iter:0 - Alpha:0.05 - Batch 177/249 - Min Loss:21.69 - Loss:21.698125838389046\n",
      " Iter:0 - Alpha:0.05 - Batch 178/249 - Min Loss:21.65 - Loss:21.65479522037255\n",
      " Iter:0 - Alpha:0.05 - Batch 179/249 - Min Loss:21.63 - Loss:21.634155948532573\n",
      " Iter:0 - Alpha:0.05 - Batch 180/249 - Min Loss:21.58 - Loss:21.58894731588048\n",
      " Iter:0 - Alpha:0.05 - Batch 181/249 - Min Loss:21.54 - Loss:21.540478425168967\n",
      " Iter:0 - Alpha:0.05 - Batch 182/249 - Min Loss:21.49 - Loss:21.491152365361543\n",
      " Iter:0 - Alpha:0.05 - Batch 183/249 - Min Loss:21.43 - Loss:21.43741011009254\n",
      " Iter:0 - Alpha:0.05 - Batch 184/249 - Min Loss:21.39 - Loss:21.396446858152117\n",
      " Iter:0 - Alpha:0.05 - Batch 185/249 - Min Loss:21.34 - Loss:21.34966061495993\n",
      " Iter:0 - Alpha:0.05 - Batch 186/249 - Min Loss:21.31 - Loss:21.313963872379915\n",
      " Iter:0 - Alpha:0.05 - Batch 187/249 - Min Loss:21.28 - Loss:21.28614534920907\n",
      " Iter:0 - Alpha:0.05 - Batch 188/249 - Min Loss:21.28 - Loss:21.284110100069643\n",
      " Iter:0 - Alpha:0.05 - Batch 189/249 - Min Loss:21.24 - Loss:21.247424538870906\n",
      " Iter:0 - Alpha:0.05 - Batch 190/249 - Min Loss:21.21 - Loss:21.21850392420512\n",
      " Iter:0 - Alpha:0.05 - Batch 191/249 - Min Loss:21.18 - Loss:21.18642728173565\n",
      " Iter:0 - Alpha:0.05 - Batch 192/249 - Min Loss:21.15 - Loss:21.157212575033284\n",
      " Iter:0 - Alpha:0.05 - Batch 193/249 - Min Loss:21.09 - Loss:21.097220764526586\n",
      " Iter:0 - Alpha:0.05 - Batch 194/249 - Min Loss:21.08 - Loss:21.080816340076947\n",
      " Iter:0 - Alpha:0.05 - Batch 195/249 - Min Loss:21.07 - Loss:21.079829356129096\n",
      " Iter:0 - Alpha:0.05 - Batch 196/249 - Min Loss:21.03 - Loss:21.037620383476327\n",
      " Iter:0 - Alpha:0.05 - Batch 197/249 - Min Loss:21.00 - Loss:21.005066890799924\n",
      " Iter:0 - Alpha:0.05 - Batch 198/249 - Min Loss:20.97 - Loss:20.972872553829365\n",
      " Iter:0 - Alpha:0.05 - Batch 199/249 - Min Loss:20.92 - Loss:20.928554034353645\n",
      " Iter:0 - Alpha:0.05 - Batch 200/249 - Min Loss:20.87 - Loss:20.878342780181846\n",
      " Iter:0 - Alpha:0.05 - Batch 201/249 - Min Loss:20.84 - Loss:20.84677806761341\n",
      " Iter:0 - Alpha:0.05 - Batch 202/249 - Min Loss:20.81 - Loss:20.818176281014235\n",
      " Iter:0 - Alpha:0.05 - Batch 203/249 - Min Loss:20.78 - Loss:20.78137692133175\n",
      " Iter:0 - Alpha:0.05 - Batch 204/249 - Min Loss:20.74 - Loss:20.743469843744997\n",
      " Iter:0 - Alpha:0.05 - Batch 205/249 - Min Loss:20.70 - Loss:20.709875200168803\n",
      " Iter:0 - Alpha:0.05 - Batch 206/249 - Min Loss:20.66 - Loss:20.666491640344482\n",
      " Iter:0 - Alpha:0.05 - Batch 207/249 - Min Loss:20.62 - Loss:20.6288106523087\n",
      " Iter:0 - Alpha:0.05 - Batch 208/249 - Min Loss:20.59 - Loss:20.590619822093068\n",
      " Iter:0 - Alpha:0.05 - Batch 209/249 - Min Loss:20.54 - Loss:20.542361014300944\n",
      " Iter:0 - Alpha:0.05 - Batch 210/249 - Min Loss:20.49 - Loss:20.496415462230768\n",
      " Iter:0 - Alpha:0.05 - Batch 211/249 - Min Loss:20.44 - Loss:20.441349759417715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Alpha:0.05 - Batch 212/249 - Min Loss:20.39 - Loss:20.39599301502027\n",
      " Iter:0 - Alpha:0.05 - Batch 213/249 - Min Loss:20.36 - Loss:20.360461327645773\n",
      " Iter:0 - Alpha:0.05 - Batch 214/249 - Min Loss:20.32 - Loss:20.325281850977365\n",
      " Iter:0 - Alpha:0.05 - Batch 215/249 - Min Loss:20.27 - Loss:20.278807369732316\n",
      " Iter:0 - Alpha:0.05 - Batch 216/249 - Min Loss:20.23 - Loss:20.237935206033733\n",
      " Iter:0 - Alpha:0.05 - Batch 217/249 - Min Loss:20.20 - Loss:20.209764510470404\n",
      " Iter:0 - Alpha:0.05 - Batch 218/249 - Min Loss:20.16 - Loss:20.165079793908806\n",
      " Iter:0 - Alpha:0.05 - Batch 219/249 - Min Loss:20.11 - Loss:20.11112536761875\n",
      " Iter:0 - Alpha:0.05 - Batch 220/249 - Min Loss:20.08 - Loss:20.086122273176773\n",
      " Iter:0 - Alpha:0.05 - Batch 221/249 - Min Loss:20.07 - Loss:20.076143610731382\n",
      " Iter:0 - Alpha:0.05 - Batch 222/249 - Min Loss:20.04 - Loss:20.043559566257503\n",
      " Iter:0 - Alpha:0.05 - Batch 223/249 - Min Loss:20.00 - Loss:20.00920325402397\n",
      " Iter:0 - Alpha:0.05 - Batch 224/249 - Min Loss:19.97 - Loss:19.977826971367755\n",
      " Iter:0 - Alpha:0.05 - Batch 225/249 - Min Loss:19.94 - Loss:19.94358039750277\n",
      " Iter:0 - Alpha:0.05 - Batch 226/249 - Min Loss:19.90 - Loss:19.90476126041431\n",
      " Iter:0 - Alpha:0.05 - Batch 227/249 - Min Loss:19.87 - Loss:19.87503672084642\n",
      " Iter:0 - Alpha:0.05 - Batch 228/249 - Min Loss:19.85 - Loss:19.858647404719413\n",
      " Iter:0 - Alpha:0.05 - Batch 229/249 - Min Loss:19.82 - Loss:19.827994299416932\n",
      " Iter:0 - Alpha:0.05 - Batch 230/249 - Min Loss:19.80 - Loss:19.80006588635624\n",
      " Iter:0 - Alpha:0.05 - Batch 231/249 - Min Loss:19.78 - Loss:19.781960704051023\n",
      " Iter:0 - Alpha:0.05 - Batch 232/249 - Min Loss:19.74 - Loss:19.748921739597982\n",
      " Iter:0 - Alpha:0.05 - Batch 233/249 - Min Loss:19.72 - Loss:19.72884029362532\n",
      " Iter:0 - Alpha:0.05 - Batch 234/249 - Min Loss:19.69 - Loss:19.695942328410677\n",
      " Iter:0 - Alpha:0.05 - Batch 235/249 - Min Loss:19.65 - Loss:19.659996594521616\n",
      " Iter:0 - Alpha:0.05 - Batch 236/249 - Min Loss:19.62 - Loss:19.620460940863357\n",
      " Iter:0 - Alpha:0.05 - Batch 237/249 - Min Loss:19.59 - Loss:19.593258104924697\n",
      " Iter:0 - Alpha:0.05 - Batch 238/249 - Min Loss:19.57 - Loss:19.570013503957735\n",
      " Iter:0 - Alpha:0.05 - Batch 239/249 - Min Loss:19.55 - Loss:19.551207062410658\n",
      " Iter:0 - Alpha:0.05 - Batch 240/249 - Min Loss:19.53 - Loss:19.533643503343033\n",
      " Iter:0 - Alpha:0.05 - Batch 241/249 - Min Loss:19.49 - Loss:19.494422243564937\n",
      " Iter:0 - Alpha:0.05 - Batch 242/249 - Min Loss:19.46 - Loss:19.462752623744866\n",
      " Iter:0 - Alpha:0.05 - Batch 243/249 - Min Loss:19.42 - Loss:19.42995264847497\n",
      " Iter:0 - Alpha:0.05 - Batch 244/249 - Min Loss:19.42 - Loss:19.428583886630655\n",
      " Iter:0 - Alpha:0.05 - Batch 245/249 - Min Loss:19.40 - Loss:19.406795316738627\n",
      " Iter:0 - Alpha:0.05 - Batch 246/249 - Min Loss:19.38 - Loss:19.385980289884966\n",
      " Iter:0 - Alpha:0.05 - Batch 247/249 - Min Loss:19.36 - Loss:19.36023292018209\n",
      " Iter:0 - Alpha:0.05 - Batch 248/249 - Min Loss:19.33 - Loss:19.331820829175516\n",
      " Iter:0 - Alpha:0.05 - Batch 249/249 - Min Loss:19.30 - Loss:19.30611192566829\n",
      " Iter:1 - Alpha:0.049 - Batch 1/249 - Min Loss:13.11 - Loss:13.111845403358927 - he t tere t tere t tere t tere t tere t tere t tere t tere t tere t te\n",
      " Iter:1 - Alpha:0.049 - Batch 3/249 - Min Loss:13.03 - Loss:13.128515900486498\n",
      " Iter:1 - Alpha:0.049 - Batch 4/249 - Min Loss:13.01 - Loss:13.011190073562062\n",
      " Iter:2 - Alpha:0.049 - Batch 3/249 - Min Loss:12.95 - Loss:13.194871282233168 - hend therer, Thend therend therer, Thend therend therer, Thend therend\n",
      " Iter:2 - Alpha:0.049 - Batch 4/249 - Min Loss:12.86 - Loss:12.860803866942682\n",
      " Iter:2 - Alpha:0.049 - Batch 210/249 - Min Loss:12.57 - Loss:12.588890099728637\n",
      " Iter:2 - Alpha:0.049 - Batch 211/249 - Min Loss:12.57 - Loss:12.57078441158006\n",
      " Iter:2 - Alpha:0.049 - Batch 212/249 - Min Loss:12.56 - Loss:12.562238708695633\n",
      " Iter:2 - Alpha:0.049 - Batch 213/249 - Min Loss:12.55 - Loss:12.556298268271846\n",
      " Iter:2 - Alpha:0.049 - Batch 214/249 - Min Loss:12.55 - Loss:12.553058171573571\n",
      " Iter:2 - Alpha:0.049 - Batch 215/249 - Min Loss:12.54 - Loss:12.542096172749288\n",
      " Iter:2 - Alpha:0.049 - Batch 217/249 - Min Loss:12.53 - Loss:12.53752510165533\n",
      " Iter:2 - Alpha:0.049 - Batch 218/249 - Min Loss:12.52 - Loss:12.529738361198293\n",
      " Iter:2 - Alpha:0.049 - Batch 219/249 - Min Loss:12.51 - Loss:12.515136272320483\n",
      " Iter:2 - Alpha:0.049 - Batch 249/249 - Min Loss:12.51 - Loss:12.569132956057373\n",
      " Iter:3 - Alpha:0.048 - Batch 1/249 - Min Loss:12.13 - Loss:12.130230988187032 - hend theres, and theres, and theseres, and theseres, and theseres, and\n",
      " Iter:3 - Alpha:0.048 - Batch 215/249 - Min Loss:12.10 - Loss:12.112782834942687\n",
      " Iter:3 - Alpha:0.048 - Batch 217/249 - Min Loss:12.10 - Loss:12.105233491605729\n",
      " Iter:3 - Alpha:0.048 - Batch 218/249 - Min Loss:12.09 - Loss:12.095940079469578\n",
      " Iter:3 - Alpha:0.048 - Batch 219/249 - Min Loss:12.07 - Loss:12.079817227021158\n",
      " Iter:3 - Alpha:0.048 - Batch 223/249 - Min Loss:12.07 - Loss:12.077048045556348\n",
      " Iter:3 - Alpha:0.048 - Batch 224/249 - Min Loss:12.07 - Loss:12.073857910392594\n",
      " Iter:3 - Alpha:0.048 - Batch 234/249 - Min Loss:12.07 - Loss:12.075737132578383\n",
      " Iter:3 - Alpha:0.048 - Batch 235/249 - Min Loss:12.06 - Loss:12.067593417228723\n",
      " Iter:3 - Alpha:0.048 - Batch 236/249 - Min Loss:12.05 - Loss:12.059817247895689\n",
      " Iter:4 - Alpha:0.048 - Batch 17/249 - Min Loss:12.05 - Loss:12.068611258622392 hend and eeseres,  Beateseres,  Beateseres,  Beateseres,  Beateseres, \n",
      " Iter:4 - Alpha:0.048 - Batch 18/249 - Min Loss:12.02 - Loss:12.02229953650627\n",
      " Iter:4 - Alpha:0.048 - Batch 20/249 - Min Loss:11.97 - Loss:12.019903657454023\n",
      " Iter:4 - Alpha:0.048 - Batch 23/249 - Min Loss:11.96 - Loss:12.039721607301018\n",
      " Iter:4 - Alpha:0.048 - Batch 24/249 - Min Loss:11.95 - Loss:11.955036435605422\n",
      " Iter:4 - Alpha:0.048 - Batch 25/249 - Min Loss:11.87 - Loss:11.877604310109632\n",
      " Iter:4 - Alpha:0.048 - Batch 56/249 - Min Loss:11.78 - Loss:11.789744537945955\n",
      " Iter:4 - Alpha:0.048 - Batch 99/249 - Min Loss:11.76 - Loss:11.761846373599491\n",
      " Iter:4 - Alpha:0.048 - Batch 100/249 - Min Loss:11.75 - Loss:11.751020735618347\n",
      " Iter:4 - Alpha:0.048 - Batch 101/249 - Min Loss:11.73 - Loss:11.736132817659184\n",
      " Iter:4 - Alpha:0.048 - Batch 102/249 - Min Loss:11.71 - Loss:11.717983956843657\n",
      " Iter:4 - Alpha:0.048 - Batch 103/249 - Min Loss:11.71 - Loss:11.712692258076371\n",
      " Iter:4 - Alpha:0.048 - Batch 104/249 - Min Loss:11.71 - Loss:11.710899106083774\n",
      " Iter:4 - Alpha:0.048 - Batch 105/249 - Min Loss:11.70 - Loss:11.707990121350166\n",
      " Iter:4 - Alpha:0.048 - Batch 107/249 - Min Loss:11.69 - Loss:11.694944306721085\n",
      " Iter:4 - Alpha:0.048 - Batch 128/249 - Min Loss:11.68 - Loss:11.684572291437446\n",
      " Iter:4 - Alpha:0.048 - Batch 129/249 - Min Loss:11.67 - Loss:11.675496665690455\n",
      " Iter:4 - Alpha:0.048 - Batch 130/249 - Min Loss:11.66 - Loss:11.668890274526854\n",
      " Iter:4 - Alpha:0.048 - Batch 131/249 - Min Loss:11.64 - Loss:11.649336759069701\n",
      " Iter:4 - Alpha:0.048 - Batch 133/249 - Min Loss:11.63 - Loss:11.637616759228706\n",
      " Iter:4 - Alpha:0.048 - Batch 135/249 - Min Loss:11.62 - Loss:11.63428682967364\n",
      " Iter:4 - Alpha:0.048 - Batch 137/249 - Min Loss:11.62 - Loss:11.622859300450438\n",
      " Iter:4 - Alpha:0.048 - Batch 144/249 - Min Loss:11.60 - Loss:11.608096795592914\n",
      " Iter:4 - Alpha:0.048 - Batch 152/249 - Min Loss:11.60 - Loss:11.608435087020344\n",
      " Iter:4 - Alpha:0.048 - Batch 153/249 - Min Loss:11.59 - Loss:11.597399785180531\n",
      " Iter:4 - Alpha:0.048 - Batch 201/249 - Min Loss:11.58 - Loss:11.589449667408491\n",
      " Iter:4 - Alpha:0.048 - Batch 202/249 - Min Loss:11.58 - Loss:11.584999807348789\n",
      " Iter:4 - Alpha:0.048 - Batch 203/249 - Min Loss:11.58 - Loss:11.584662738042356\n",
      " Iter:4 - Alpha:0.048 - Batch 205/249 - Min Loss:11.58 - Loss:11.581314617302567\n",
      " Iter:4 - Alpha:0.048 - Batch 206/249 - Min Loss:11.57 - Loss:11.576564369826576\n",
      " Iter:4 - Alpha:0.048 - Batch 207/249 - Min Loss:11.57 - Loss:11.570989374435403\n",
      " Iter:4 - Alpha:0.048 - Batch 208/249 - Min Loss:11.56 - Loss:11.563274731403995\n",
      " Iter:4 - Alpha:0.048 - Batch 209/249 - Min Loss:11.55 - Loss:11.555628002170211\n",
      " Iter:4 - Alpha:0.048 - Batch 210/249 - Min Loss:11.54 - Loss:11.547610477776836\n",
      " Iter:4 - Alpha:0.048 - Batch 211/249 - Min Loss:11.52 - Loss:11.528577994088598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:4 - Alpha:0.048 - Batch 212/249 - Min Loss:11.52 - Loss:11.521334295993437\n",
      " Iter:4 - Alpha:0.048 - Batch 213/249 - Min Loss:11.51 - Loss:11.514544546568438\n",
      " Iter:4 - Alpha:0.048 - Batch 214/249 - Min Loss:11.51 - Loss:11.513433829370246\n",
      " Iter:4 - Alpha:0.048 - Batch 215/249 - Min Loss:11.50 - Loss:11.502637432239196\n",
      " Iter:4 - Alpha:0.048 - Batch 217/249 - Min Loss:11.49 - Loss:11.496531834549412\n",
      " Iter:4 - Alpha:0.048 - Batch 218/249 - Min Loss:11.48 - Loss:11.488019769387577\n",
      " Iter:4 - Alpha:0.048 - Batch 219/249 - Min Loss:11.47 - Loss:11.473366406214671\n",
      " Iter:4 - Alpha:0.048 - Batch 221/249 - Min Loss:11.46 - Loss:11.474413061828578\n",
      " Iter:4 - Alpha:0.048 - Batch 222/249 - Min Loss:11.46 - Loss:11.467548086427962\n",
      " Iter:4 - Alpha:0.048 - Batch 223/249 - Min Loss:11.46 - Loss:11.462718862415915\n",
      " Iter:4 - Alpha:0.048 - Batch 224/249 - Min Loss:11.45 - Loss:11.456995633031061\n",
      " Iter:4 - Alpha:0.048 - Batch 225/249 - Min Loss:11.45 - Loss:11.452679754033158\n",
      " Iter:4 - Alpha:0.048 - Batch 240/249 - Min Loss:11.45 - Loss:11.458924622418877\n",
      " Iter:4 - Alpha:0.048 - Batch 242/249 - Min Loss:11.44 - Loss:11.448867895928165\n",
      " Iter:4 - Alpha:0.048 - Batch 249/249 - Min Loss:11.44 - Loss:11.447779877337853\n",
      " Iter:5 - Alpha:0.047 - Batch 1/249 - Min Loss:11.39 - Loss:11.396887404725819 - hend seend seend seend seend seend seend seend seend seend seend seend\n",
      " Iter:5 - Alpha:0.047 - Batch 4/249 - Min Loss:11.38 - Loss:11.512659347292667\n",
      " Iter:5 - Alpha:0.047 - Batch 56/249 - Min Loss:11.31 - Loss:11.344776745309433\n",
      " Iter:5 - Alpha:0.047 - Batch 105/249 - Min Loss:11.31 - Loss:11.318280215835468\n",
      " Iter:5 - Alpha:0.047 - Batch 107/249 - Min Loss:11.30 - Loss:11.303898915281472\n",
      " Iter:5 - Alpha:0.047 - Batch 129/249 - Min Loss:11.29 - Loss:11.300193602564697\n",
      " Iter:5 - Alpha:0.047 - Batch 130/249 - Min Loss:11.29 - Loss:11.293794585483674\n",
      " Iter:5 - Alpha:0.047 - Batch 131/249 - Min Loss:11.27 - Loss:11.274769446270325\n",
      " Iter:5 - Alpha:0.047 - Batch 133/249 - Min Loss:11.25 - Loss:11.259054848329036\n",
      " Iter:5 - Alpha:0.047 - Batch 135/249 - Min Loss:11.24 - Loss:11.249063213061353\n",
      " Iter:5 - Alpha:0.047 - Batch 137/249 - Min Loss:11.23 - Loss:11.236654207953192\n",
      " Iter:5 - Alpha:0.047 - Batch 138/249 - Min Loss:11.21 - Loss:11.219907203713223\n",
      " Iter:5 - Alpha:0.047 - Batch 142/249 - Min Loss:11.21 - Loss:11.214969241364662\n",
      " Iter:5 - Alpha:0.047 - Batch 143/249 - Min Loss:11.20 - Loss:11.203053436856298\n",
      " Iter:5 - Alpha:0.047 - Batch 144/249 - Min Loss:11.19 - Loss:11.190465947788814\n",
      " Iter:5 - Alpha:0.047 - Batch 152/249 - Min Loss:11.18 - Loss:11.187686940755276\n",
      " Iter:5 - Alpha:0.047 - Batch 153/249 - Min Loss:11.17 - Loss:11.177946284709687\n",
      " Iter:5 - Alpha:0.047 - Batch 207/249 - Min Loss:11.16 - Loss:11.173129977378249\n",
      " Iter:5 - Alpha:0.047 - Batch 208/249 - Min Loss:11.16 - Loss:11.163990691914513\n",
      " Iter:5 - Alpha:0.047 - Batch 209/249 - Min Loss:11.15 - Loss:11.15650719948327\n",
      " Iter:5 - Alpha:0.047 - Batch 210/249 - Min Loss:11.14 - Loss:11.148306226646675\n",
      " Iter:5 - Alpha:0.047 - Batch 211/249 - Min Loss:11.13 - Loss:11.130086404823077\n",
      " Iter:5 - Alpha:0.047 - Batch 212/249 - Min Loss:11.12 - Loss:11.124460426834831\n",
      " Iter:5 - Alpha:0.047 - Batch 213/249 - Min Loss:11.11 - Loss:11.116645745489764\n",
      " Iter:5 - Alpha:0.047 - Batch 214/249 - Min Loss:11.11 - Loss:11.115069959904687\n",
      " Iter:5 - Alpha:0.047 - Batch 215/249 - Min Loss:11.10 - Loss:11.104906833919681\n",
      " Iter:5 - Alpha:0.047 - Batch 217/249 - Min Loss:11.09 - Loss:11.097733793338126\n",
      " Iter:5 - Alpha:0.047 - Batch 218/249 - Min Loss:11.08 - Loss:11.089401621067497\n",
      " Iter:5 - Alpha:0.047 - Batch 219/249 - Min Loss:11.07 - Loss:11.075501952261202\n",
      " Iter:5 - Alpha:0.047 - Batch 221/249 - Min Loss:11.07 - Loss:11.076194851300338\n",
      " Iter:5 - Alpha:0.047 - Batch 222/249 - Min Loss:11.06 - Loss:11.069423475314416\n",
      " Iter:5 - Alpha:0.047 - Batch 223/249 - Min Loss:11.06 - Loss:11.06355350521653\n",
      " Iter:5 - Alpha:0.047 - Batch 224/249 - Min Loss:11.05 - Loss:11.057820632679839\n",
      " Iter:5 - Alpha:0.047 - Batch 225/249 - Min Loss:11.05 - Loss:11.052252483586946\n",
      " Iter:5 - Alpha:0.047 - Batch 226/249 - Min Loss:11.04 - Loss:11.049454032187471\n",
      " Iter:5 - Alpha:0.047 - Batch 234/249 - Min Loss:11.04 - Loss:11.049897989903435\n",
      " Iter:5 - Alpha:0.047 - Batch 235/249 - Min Loss:11.04 - Loss:11.04419348680408\n",
      " Iter:5 - Alpha:0.047 - Batch 236/249 - Min Loss:11.03 - Loss:11.037882564499965\n",
      " Iter:5 - Alpha:0.047 - Batch 237/249 - Min Loss:11.03 - Loss:11.030744261748705\n",
      " Iter:5 - Alpha:0.047 - Batch 240/249 - Min Loss:11.02 - Loss:11.038117826679585\n",
      " Iter:5 - Alpha:0.047 - Batch 242/249 - Min Loss:11.02 - Loss:11.027896389532212\n",
      " Iter:5 - Alpha:0.047 - Batch 249/249 - Min Loss:11.02 - Loss:11.027841630322417\n",
      " Iter:6 - Alpha:0.047 - Batch 1/249 - Min Loss:10.94 - Loss:10.94846578175177 - hend seent seen theeseres, and seent seen theeseres, and seent seen th\n",
      " Iter:6 - Alpha:0.047 - Batch 2/249 - Min Loss:10.74 - Loss:10.747035913082376\n",
      " Iter:6 - Alpha:0.047 - Batch 4/249 - Min Loss:10.71 - Loss:10.782595779564186\n",
      " Iter:7 - Alpha:0.046 - Batch 56/249 - Min Loss:10.69 - Loss:10.696160372611905- her, Thees, and seen, and seen, and seen, and seen, and seen, and seen\n",
      " Iter:7 - Alpha:0.046 - Batch 144/249 - Min Loss:10.67 - Loss:10.684790907350228\n",
      " Iter:7 - Alpha:0.046 - Batch 147/249 - Min Loss:10.67 - Loss:10.672282799205652\n",
      " Iter:7 - Alpha:0.046 - Batch 148/249 - Min Loss:10.66 - Loss:10.669254488866965\n",
      " Iter:7 - Alpha:0.046 - Batch 149/249 - Min Loss:10.66 - Loss:10.661622840591273\n",
      " Iter:7 - Alpha:0.046 - Batch 150/249 - Min Loss:10.65 - Loss:10.659808129639268\n",
      " Iter:7 - Alpha:0.046 - Batch 151/249 - Min Loss:10.65 - Loss:10.657461127618138\n",
      " Iter:7 - Alpha:0.046 - Batch 152/249 - Min Loss:10.65 - Loss:10.65674007582653\n",
      " Iter:7 - Alpha:0.046 - Batch 218/249 - Min Loss:10.65 - Loss:10.664557207183474\n",
      " Iter:7 - Alpha:0.046 - Batch 219/249 - Min Loss:10.65 - Loss:10.652280381090808\n",
      " Iter:7 - Alpha:0.046 - Batch 221/249 - Min Loss:10.65 - Loss:10.654989542995766\n",
      " Iter:7 - Alpha:0.046 - Batch 222/249 - Min Loss:10.65 - Loss:10.650760403043043\n",
      " Iter:7 - Alpha:0.046 - Batch 223/249 - Min Loss:10.64 - Loss:10.646728783759627\n",
      " Iter:7 - Alpha:0.046 - Batch 224/249 - Min Loss:10.64 - Loss:10.640521934308497\n",
      " Iter:7 - Alpha:0.046 - Batch 225/249 - Min Loss:10.63 - Loss:10.636133813466383\n",
      " Iter:7 - Alpha:0.046 - Batch 226/249 - Min Loss:10.63 - Loss:10.63278510958494\n",
      " Iter:7 - Alpha:0.046 - Batch 234/249 - Min Loss:10.62 - Loss:10.633732465601001\n",
      " Iter:7 - Alpha:0.046 - Batch 235/249 - Min Loss:10.62 - Loss:10.6247065278588\n",
      " Iter:7 - Alpha:0.046 - Batch 236/249 - Min Loss:10.61 - Loss:10.617672008235404\n",
      " Iter:8 - Alpha:0.046 - Batch 1/249 - Min Loss:10.61 - Loss:10.828118049238487 - her, and seend seend seend seend seend seend seend seend seend seend s\n",
      " Iter:8 - Alpha:0.046 - Batch 2/249 - Min Loss:10.55 - Loss:10.552413775751592\n",
      " Iter:8 - Alpha:0.046 - Batch 3/249 - Min Loss:10.29 - Loss:10.299977976326558\n",
      " Iter:8 - Alpha:0.046 - Batch 4/249 - Min Loss:10.20 - Loss:10.206841007559493\n",
      " Iter:9 - Alpha:0.045 - Batch 3/249 - Min Loss:10.14 - Loss:10.288110384826338- her theer theer theer theer theer theer theer theer theer theer theer \n",
      " Iter:9 - Alpha:0.045 - Batch 4/249 - Min Loss:10.13 - Loss:10.133361603518697\n",
      " Iter:9 - Alpha:0.045 - Batch 249/249 - Min Loss:10.01 - Loss:10.355797987768489"
     ]
    }
   ],
   "source": [
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And will sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill sin to sill\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 15\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "#         m = (temp_dist > np.random.rand()).argmax() # sample from predictions\n",
    "        m = output.data.argmax() # take the max prediction\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "print(generate_sample(n=500, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[3.45638663]\n",
      "Pred:[0 0 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 1 0 1]\n",
      "9 + 60 = 1\n",
      "------------\n",
      "Error:[3.63389116]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 0 1 1 1 1 1 1]\n",
      "28 + 35 = 255\n",
      "------------\n",
      "Error:[3.91366595]\n",
      "Pred:[0 1 0 0 1 0 0 0]\n",
      "True:[1 0 1 0 0 0 0 0]\n",
      "116 + 44 = 72\n",
      "------------\n",
      "Error:[3.72191702]\n",
      "Pred:[1 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 0 1]\n",
      "4 + 73 = 223\n",
      "------------\n",
      "Error:[3.5852713]\n",
      "Pred:[0 0 0 0 1 0 0 0]\n",
      "True:[0 1 0 1 0 0 1 0]\n",
      "71 + 11 = 8\n",
      "------------\n",
      "Error:[2.53352328]\n",
      "Pred:[1 0 1 0 0 0 1 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "81 + 113 = 162\n",
      "------------\n",
      "Error:[0.57691441]\n",
      "Pred:[0 1 0 1 0 0 0 1]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "81 + 0 = 81\n",
      "------------\n",
      "Error:[1.42589952]\n",
      "Pred:[1 0 0 0 0 0 0 1]\n",
      "True:[1 0 0 0 0 0 0 1]\n",
      "4 + 125 = 129\n",
      "------------\n",
      "Error:[0.47477457]\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "39 + 17 = 56\n",
      "------------\n",
      "Error:[0.21595037]\n",
      "Pred:[0 0 0 0 1 1 1 0]\n",
      "True:[0 0 0 0 1 1 1 0]\n",
      "11 + 3 = 14\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print (\"Error:\" + str(overallError))\n",
    "        print (\"Pred:\" + str(d))\n",
    "        print (\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print (str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print (\"------------\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (temp)",
   "language": "python",
   "name": "temp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
